{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GANs for MNIST"
      ],
      "metadata": {
        "id": "s3ekG60FKt1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we fit vanilla GANs to the MNIST dataset."
      ],
      "metadata": {
        "id": "FezESETeKvpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment"
      ],
      "metadata": {
        "id": "vYXrulR9LE7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's start by installing a couple of dependencies.\n",
        "\n",
        "!pip -q install torch torchvision lightning pandas seaborn git+https://github.com/oelin/valkyrie"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "pjqCGMLNMB0r",
        "outputId": "25385716-8b5b-4ddf-8fc2-c7280e5b7b83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's import everything we need.\n",
        "\n",
        "from typing import Any, Callable, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, Lambda, ToPILImage, PILToTensor, ToTensor, Normalize\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "import valkyrie as vk"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JOz0GwklM7pb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's initailize the MNIST dataset.\n",
        "\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(mean=0.5, std=0.5),\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='.', train=True, transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "IUFkBKOoSwWd",
        "outputId": "d9ae9ff2-3212-4801-9b4c-9e36a8081805"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 31142458.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 39873500.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 39820192.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20752209.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Models"
      ],
      "metadata": {
        "id": "FlNS4i8KS7UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "# Quick error fix\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# On Windows platform, the torch.distributed package only\n",
        "# supports Gloo backend, FileStore and TcpStore.\n",
        "# For FileStore, set init_method parameter in init_process_group\n",
        "# to a local file. Example as follow:\n",
        "# init_method=\"file:///f:/libtmp/some_file\"\n",
        "# dist.init_process_group(\n",
        "#    \"gloo\",\n",
        "#    rank=rank,\n",
        "#    init_method=init_method,\n",
        "#    world_size=world_size)\n",
        "# For TcpStore, same way as on Linux.\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "setup(0, 1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4PYlQGGYyjkq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "\n",
        "#fabric = L.Fabric(accelerator=\"cuda\", devices=8, strategy=\"ddp\")\n",
        "\n",
        "fabric = L.Fabric(accelerator=\"cpu\", devices=10, strategy=\"ddp_notebook\")\n",
        "fabric.launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t7Q-fHK-ylkV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown The generator.\n",
        "\n",
        "latent_dimension = 64\n",
        "input_dimension = 28 * 28\n",
        "\n",
        "generator = nn.Sequential(\n",
        "    nn.Linear(latent_dimension, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, input_dimension),\n",
        "    nn.Tanh(),\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "_ew-eRrQ298g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown The discriminator.\n",
        "\n",
        "discriminator = nn.Sequential(\n",
        "    nn.Linear(input_dimension, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 1),\n",
        "    nn.Sigmoid(),\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "j02XWLmxBUQF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unnormalize_image(x):\n",
        "    return ((x + 1) / 2).clamp(0, 1)"
      ],
      "metadata": {
        "id": "P5rx1-xi-NAZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Train them.\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 300\n",
        "batches = len(train_dataset) // batch_size\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "discriminator_optimizer = Adam(discriminator.parameters(), lr=2e-4)\n",
        "generator_optimizer = Adam(generator.parameters(), lr=2e-4)\n",
        "\n",
        "\n",
        "def generate_fake(batch_size) -> torch.Tensor:\n",
        "    z = torch.randn(batch_size, latent_dimension).cuda()\n",
        "    x = generator(z)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "discriminator_losses = []\n",
        "generator_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (inputs_real, _) in enumerate(train_dataloader):\n",
        "\n",
        "        # Train the discriminator.\n",
        "\n",
        "        if inputs_real.size(0) != batch_size:\n",
        "            continue\n",
        "\n",
        "        inputs_real = inputs_real.view(batch_size, -1).cuda()\n",
        "        inputs_fake = generate_fake(batch_size).detach().cuda()\n",
        "        labels_real = torch.ones(batch_size, 1).cuda()\n",
        "        labels_fake = torch.zeros(batch_size, 1).cuda()\n",
        "\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        discriminator_loss_real = criterion(discriminator(inputs_real), labels_real)\n",
        "        discriminator_loss_fake = criterion(discriminator(inputs_fake), labels_fake)\n",
        "        discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
        "        discriminator_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator.\n",
        "\n",
        "        inputs_fake = generate_fake(batch_size).cuda()\n",
        "\n",
        "        generator_optimizer.zero_grad()\n",
        "        generator_loss = criterion(discriminator(inputs_fake), labels_real)\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # Record losses.\n",
        "\n",
        "        discriminator_losses.append(discriminator_loss.item())\n",
        "        generator_losses.append(generator_loss.item())\n",
        "\n",
        "        if (i % 100) == 0:\n",
        "            mean_discriminator_loss = sum(discriminator_losses) / len(discriminator_losses)\n",
        "            mean_generator_loss = sum(generator_losses) / len(generator_losses)\n",
        "\n",
        "            print(f'Epoch {epoch}/{epochs}, Batch {i}/{batches}, generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}')\n",
        "\n",
        "    # Save results.\n",
        "\n",
        "    print('Saving results...')\n",
        "\n",
        "    images_to_save = inputs_fake.detach().cpu().view(inputs_fake.size(0), 1, 28, 28)\n",
        "    save_image(unnormalize_image(images_to_save.data), os.path.join('samples', f'epoch-{epoch}.png'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpvm762X4lV5",
        "outputId": "3bf6280e-052e-41fd-d878-230f30f8c35c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/300, Batch 0/937, generator loss: 0.7021292448043823, discriminator loss: 1.4290435314178467\n",
            "Epoch 0/300, Batch 100/937, generator loss: 1.5424737511294904, discriminator loss: 0.7002204756925602\n",
            "Epoch 0/300, Batch 200/937, generator loss: 2.4499487085128897, discriminator loss: 0.4083510832166049\n",
            "Epoch 0/300, Batch 300/937, generator loss: 2.780550401670196, discriminator loss: 0.3540844147548426\n",
            "Epoch 0/300, Batch 400/937, generator loss: 3.31599023588875, discriminator loss: 0.33587620733886436\n",
            "Epoch 0/300, Batch 500/937, generator loss: 3.5412213979604954, discriminator loss: 0.34543886923504447\n",
            "Epoch 0/300, Batch 600/937, generator loss: 3.851242894638398, discriminator loss: 0.3241525048734047\n",
            "Epoch 0/300, Batch 700/937, generator loss: 3.9208222960099346, discriminator loss: 0.3076201768355516\n",
            "Epoch 0/300, Batch 800/937, generator loss: 3.948273557923111, discriminator loss: 0.3095491712868735\n",
            "Epoch 0/300, Batch 900/937, generator loss: 3.9190756919381355, discriminator loss: 0.3274981129902118\n",
            "Saving results...\n",
            "Epoch 1/300, Batch 0/937, generator loss: 3.9049999375206066, discriminator loss: 0.3384900661602394\n",
            "Epoch 1/300, Batch 100/937, generator loss: 3.8875442917291827, discriminator loss: 0.3557411942142914\n",
            "Epoch 1/300, Batch 200/937, generator loss: 3.8198037261179336, discriminator loss: 0.3799253703178494\n",
            "Epoch 1/300, Batch 300/937, generator loss: 3.7379460252448315, discriminator loss: 0.40545962666355445\n",
            "Epoch 1/300, Batch 400/937, generator loss: 3.668810379567524, discriminator loss: 0.4242031272634396\n",
            "Epoch 1/300, Batch 500/937, generator loss: 3.6276136770616487, discriminator loss: 0.4220844491285185\n",
            "Epoch 1/300, Batch 600/937, generator loss: 3.574057287489329, discriminator loss: 0.4226407373084242\n",
            "Epoch 1/300, Batch 700/937, generator loss: 3.518694528329649, discriminator loss: 0.43117577306271254\n",
            "Epoch 1/300, Batch 800/937, generator loss: 3.4944864042677732, discriminator loss: 0.4280522981352067\n",
            "Epoch 1/300, Batch 900/937, generator loss: 3.496578907713667, discriminator loss: 0.42315321935541417\n",
            "Saving results...\n",
            "Epoch 2/300, Batch 0/937, generator loss: 3.488588630835215, discriminator loss: 0.4231348902960618\n",
            "Epoch 2/300, Batch 100/937, generator loss: 3.4829501026793372, discriminator loss: 0.4209161956570571\n",
            "Epoch 2/300, Batch 200/937, generator loss: 3.473139339096575, discriminator loss: 0.41477276867233126\n",
            "Epoch 2/300, Batch 300/937, generator loss: 3.448025151093801, discriminator loss: 0.4090533563751599\n",
            "Epoch 2/300, Batch 400/937, generator loss: 3.443293256471445, discriminator loss: 0.4013208312172811\n",
            "Epoch 2/300, Batch 500/937, generator loss: 3.4442723172087417, discriminator loss: 0.39703456836782003\n",
            "Epoch 2/300, Batch 600/937, generator loss: 3.446308720882493, discriminator loss: 0.39206710913717147\n",
            "Epoch 2/300, Batch 700/937, generator loss: 3.458269681444446, discriminator loss: 0.38688221347708146\n",
            "Epoch 2/300, Batch 800/937, generator loss: 3.4821621540773693, discriminator loss: 0.37953444977508527\n",
            "Epoch 2/300, Batch 900/937, generator loss: 3.50132200668524, discriminator loss: 0.37175341211863466\n",
            "Saving results...\n",
            "Epoch 3/300, Batch 0/937, generator loss: 3.5084810265224653, discriminator loss: 0.36838191528678743\n",
            "Epoch 3/300, Batch 100/937, generator loss: 3.517015261043395, discriminator loss: 0.3638428599115686\n",
            "Epoch 3/300, Batch 200/937, generator loss: 3.525345724555917, discriminator loss: 0.3568781469361102\n",
            "Epoch 3/300, Batch 300/937, generator loss: 3.542159060720153, discriminator loss: 0.35189304922786385\n",
            "Epoch 3/300, Batch 400/937, generator loss: 3.5914621986866293, discriminator loss: 0.3442618431990843\n",
            "Epoch 3/300, Batch 500/937, generator loss: 3.6370362088552133, discriminator loss: 0.3383006646279875\n",
            "Epoch 3/300, Batch 600/937, generator loss: 3.6698039534467886, discriminator loss: 0.3336282960321549\n",
            "Epoch 3/300, Batch 700/937, generator loss: 3.68773871386988, discriminator loss: 0.3317202323674307\n",
            "Epoch 3/300, Batch 800/937, generator loss: 3.711498345135982, discriminator loss: 0.33048761400661264\n",
            "Epoch 3/300, Batch 900/937, generator loss: 3.7410549654561125, discriminator loss: 0.3292508705986808\n",
            "Saving results...\n",
            "Epoch 4/300, Batch 0/937, generator loss: 3.7521205917139886, discriminator loss: 0.3279936314801688\n",
            "Epoch 4/300, Batch 100/937, generator loss: 3.7723276052235875, discriminator loss: 0.3273767896013992\n",
            "Epoch 4/300, Batch 200/937, generator loss: 3.785125574980485, discriminator loss: 0.32446107049098827\n",
            "Epoch 4/300, Batch 300/937, generator loss: 3.795543611667927, discriminator loss: 0.32266865665620714\n",
            "Epoch 4/300, Batch 400/937, generator loss: 3.799530959137953, discriminator loss: 0.32239214919469794\n",
            "Epoch 4/300, Batch 500/937, generator loss: 3.8076240987911256, discriminator loss: 0.3188110531028733\n",
            "Epoch 4/300, Batch 600/937, generator loss: 3.807795337779299, discriminator loss: 0.3181819299278341\n",
            "Epoch 4/300, Batch 700/937, generator loss: 3.82306596837062, discriminator loss: 0.31789927094736026\n",
            "Epoch 4/300, Batch 800/937, generator loss: 3.826229871184151, discriminator loss: 0.3196711768202728\n",
            "Epoch 4/300, Batch 900/937, generator loss: 3.8250156209837574, discriminator loss: 0.3190520425489588\n",
            "Saving results...\n",
            "Epoch 5/300, Batch 0/937, generator loss: 3.8242987350958613, discriminator loss: 0.31846575823827555\n",
            "Epoch 5/300, Batch 100/937, generator loss: 3.8192523715763076, discriminator loss: 0.3176658559540762\n",
            "Epoch 5/300, Batch 200/937, generator loss: 3.825975806838511, discriminator loss: 0.31663742162305536\n",
            "Epoch 5/300, Batch 300/937, generator loss: 3.835238278928264, discriminator loss: 0.3148915332932932\n",
            "Epoch 5/300, Batch 400/937, generator loss: 3.841975825992195, discriminator loss: 0.313781257891662\n",
            "Epoch 5/300, Batch 500/937, generator loss: 3.8600562462303754, discriminator loss: 0.3119284716774909\n",
            "Epoch 5/300, Batch 600/937, generator loss: 3.8772461478271767, discriminator loss: 0.3101170366088008\n",
            "Epoch 5/300, Batch 700/937, generator loss: 3.9039062939631286, discriminator loss: 0.307873866878757\n",
            "Epoch 5/300, Batch 800/937, generator loss: 3.919353569234065, discriminator loss: 0.30616039257310534\n",
            "Epoch 5/300, Batch 900/937, generator loss: 3.92958286590274, discriminator loss: 0.30461489122909136\n",
            "Saving results...\n",
            "Epoch 6/300, Batch 0/937, generator loss: 3.9358144844742236, discriminator loss: 0.30467392981420754\n",
            "Epoch 6/300, Batch 100/937, generator loss: 3.9487743899161094, discriminator loss: 0.3044464634153709\n",
            "Epoch 6/300, Batch 200/937, generator loss: 3.9608671127900252, discriminator loss: 0.3027295840207744\n",
            "Epoch 6/300, Batch 300/937, generator loss: 3.974931808927263, discriminator loss: 0.3013169321443973\n",
            "Epoch 6/300, Batch 400/937, generator loss: 3.9885039131506406, discriminator loss: 0.2997681468678198\n",
            "Epoch 6/300, Batch 500/937, generator loss: 3.999088702814341, discriminator loss: 0.29938908078953097\n",
            "Epoch 6/300, Batch 600/937, generator loss: 4.005503305826811, discriminator loss: 0.29858616286167067\n",
            "Epoch 6/300, Batch 700/937, generator loss: 4.0127032295484515, discriminator loss: 0.2982281306536113\n",
            "Epoch 6/300, Batch 800/937, generator loss: 4.008771828238734, discriminator loss: 0.29996253131704226\n",
            "Epoch 6/300, Batch 900/937, generator loss: 4.0111639645409225, discriminator loss: 0.2997154209088342\n",
            "Saving results...\n",
            "Epoch 7/300, Batch 0/937, generator loss: 4.011996989488238, discriminator loss: 0.29955817502298643\n",
            "Epoch 7/300, Batch 100/937, generator loss: 4.014707116957183, discriminator loss: 0.2989849359764754\n",
            "Epoch 7/300, Batch 200/937, generator loss: 4.01553888538709, discriminator loss: 0.297924712829887\n",
            "Epoch 7/300, Batch 300/937, generator loss: 4.016325744725872, discriminator loss: 0.2969477978201024\n",
            "Epoch 7/300, Batch 400/937, generator loss: 4.016579466535784, discriminator loss: 0.2960795427897367\n",
            "Epoch 7/300, Batch 500/937, generator loss: 4.015184435147064, discriminator loss: 0.29529023086414713\n",
            "Epoch 7/300, Batch 600/937, generator loss: 4.014511344144797, discriminator loss: 0.29410514250324843\n",
            "Epoch 7/300, Batch 700/937, generator loss: 4.020669249061382, discriminator loss: 0.29255728878788956\n",
            "Epoch 7/300, Batch 800/937, generator loss: 4.018383276454457, discriminator loss: 0.292403808532222\n",
            "Epoch 7/300, Batch 900/937, generator loss: 4.021187797652173, discriminator loss: 0.29110198576482704\n",
            "Saving results...\n",
            "Epoch 8/300, Batch 0/937, generator loss: 4.024086134289366, discriminator loss: 0.2908084122703817\n",
            "Epoch 8/300, Batch 100/937, generator loss: 4.039860877007485, discriminator loss: 0.2903935860475184\n",
            "Epoch 8/300, Batch 200/937, generator loss: 4.048921050840467, discriminator loss: 0.2915141726220267\n",
            "Epoch 8/300, Batch 300/937, generator loss: 4.060617186151071, discriminator loss: 0.2909449429362458\n",
            "Epoch 8/300, Batch 400/937, generator loss: 4.0739902259806255, discriminator loss: 0.2902572259870159\n",
            "Epoch 8/300, Batch 500/937, generator loss: 4.084800557425667, discriminator loss: 0.2897959551052502\n",
            "Epoch 8/300, Batch 600/937, generator loss: 4.088340016453446, discriminator loss: 0.2889886866098784\n",
            "Epoch 8/300, Batch 700/937, generator loss: 4.0938609003788, discriminator loss: 0.289245290402448\n",
            "Epoch 8/300, Batch 800/937, generator loss: 4.103966140964023, discriminator loss: 0.28994264035143663\n",
            "Epoch 8/300, Batch 900/937, generator loss: 4.112455808386656, discriminator loss: 0.2911520300516667\n",
            "Saving results...\n",
            "Epoch 9/300, Batch 0/937, generator loss: 4.1155575571154595, discriminator loss: 0.2909711187081277\n",
            "Epoch 9/300, Batch 100/937, generator loss: 4.116848760915914, discriminator loss: 0.29197604879466255\n",
            "Epoch 9/300, Batch 200/937, generator loss: 4.120483331604912, discriminator loss: 0.29203762276754397\n",
            "Epoch 9/300, Batch 300/937, generator loss: 4.118741471290534, discriminator loss: 0.2930085704081189\n",
            "Epoch 9/300, Batch 400/937, generator loss: 4.117730684205637, discriminator loss: 0.2928889588703154\n",
            "Epoch 9/300, Batch 500/937, generator loss: 4.115938422815736, discriminator loss: 0.2934735264822604\n",
            "Epoch 9/300, Batch 600/937, generator loss: 4.118091868983451, discriminator loss: 0.29376982092628084\n",
            "Epoch 9/300, Batch 700/937, generator loss: 4.117831488152367, discriminator loss: 0.293422963714522\n",
            "Epoch 9/300, Batch 800/937, generator loss: 4.117111607724412, discriminator loss: 0.2936269421088462\n",
            "Epoch 9/300, Batch 900/937, generator loss: 4.121332255751054, discriminator loss: 0.2936890186290555\n",
            "Saving results...\n",
            "Epoch 10/300, Batch 0/937, generator loss: 4.125906359912771, discriminator loss: 0.2938110169940386\n",
            "Epoch 10/300, Batch 100/937, generator loss: 4.124798026026749, discriminator loss: 0.2946898058385661\n",
            "Epoch 10/300, Batch 200/937, generator loss: 4.124942999170836, discriminator loss: 0.29513988238232425\n",
            "Epoch 10/300, Batch 300/937, generator loss: 4.130045329725689, discriminator loss: 0.2952731405196571\n",
            "Epoch 10/300, Batch 400/937, generator loss: 4.1330647802062535, discriminator loss: 0.2953805552221543\n",
            "Epoch 10/300, Batch 500/937, generator loss: 4.132439351996056, discriminator loss: 0.2957952201393597\n",
            "Epoch 10/300, Batch 600/937, generator loss: 4.132792884129936, discriminator loss: 0.29603787425581546\n",
            "Epoch 10/300, Batch 700/937, generator loss: 4.1323640593858855, discriminator loss: 0.2961010559187451\n",
            "Epoch 10/300, Batch 800/937, generator loss: 4.132085427570081, discriminator loss: 0.2961399065259752\n",
            "Epoch 10/300, Batch 900/937, generator loss: 4.131238073786653, discriminator loss: 0.2959736275204014\n",
            "Saving results...\n",
            "Epoch 11/300, Batch 0/937, generator loss: 4.131218460158271, discriminator loss: 0.2958454619789533\n",
            "Epoch 11/300, Batch 100/937, generator loss: 4.130450515123764, discriminator loss: 0.2959664941963659\n",
            "Epoch 11/300, Batch 200/937, generator loss: 4.127765559785984, discriminator loss: 0.2962299262382286\n",
            "Epoch 11/300, Batch 300/937, generator loss: 4.129089155117805, discriminator loss: 0.29615795640078224\n",
            "Epoch 11/300, Batch 400/937, generator loss: 4.1320682489241864, discriminator loss: 0.2960603161477357\n",
            "Epoch 11/300, Batch 500/937, generator loss: 4.131582601540491, discriminator loss: 0.29642517229391857\n",
            "Epoch 11/300, Batch 600/937, generator loss: 4.13207357692924, discriminator loss: 0.296158630381088\n",
            "Epoch 11/300, Batch 700/937, generator loss: 4.141337811335012, discriminator loss: 0.2958977450609889\n",
            "Epoch 11/300, Batch 800/937, generator loss: 4.140028571622124, discriminator loss: 0.2964039038756757\n",
            "Epoch 11/300, Batch 900/937, generator loss: 4.138646316982482, discriminator loss: 0.2960293835712375\n",
            "Saving results...\n",
            "Epoch 12/300, Batch 0/937, generator loss: 4.137939885156639, discriminator loss: 0.29621628157599017\n",
            "Epoch 12/300, Batch 100/937, generator loss: 4.137998788621574, discriminator loss: 0.29596936326751416\n",
            "Epoch 12/300, Batch 200/937, generator loss: 4.136835265414795, discriminator loss: 0.2959224925057083\n",
            "Epoch 12/300, Batch 300/937, generator loss: 4.135870006586267, discriminator loss: 0.296106771010891\n",
            "Epoch 12/300, Batch 400/937, generator loss: 4.131809724063103, discriminator loss: 0.29667299222656973\n",
            "Epoch 12/300, Batch 500/937, generator loss: 4.1278749055637105, discriminator loss: 0.2967526739330787\n",
            "Epoch 12/300, Batch 600/937, generator loss: 4.127286873547122, discriminator loss: 0.2969115879564358\n",
            "Epoch 12/300, Batch 700/937, generator loss: 4.124946701836017, discriminator loss: 0.29679018350551195\n",
            "Epoch 12/300, Batch 800/937, generator loss: 4.123571607605359, discriminator loss: 0.29669877828627317\n",
            "Epoch 12/300, Batch 900/937, generator loss: 4.122719967257295, discriminator loss: 0.295945453077567\n",
            "Saving results...\n",
            "Epoch 13/300, Batch 0/937, generator loss: 4.122433771343972, discriminator loss: 0.2958383790028201\n",
            "Epoch 13/300, Batch 100/937, generator loss: 4.120385937549221, discriminator loss: 0.29592402471374113\n",
            "Epoch 13/300, Batch 200/937, generator loss: 4.120719942955493, discriminator loss: 0.2957502037023202\n",
            "Epoch 13/300, Batch 300/937, generator loss: 4.122423802439629, discriminator loss: 0.29580276966825586\n",
            "Epoch 13/300, Batch 400/937, generator loss: 4.124542684698878, discriminator loss: 0.2954543442039749\n",
            "Epoch 13/300, Batch 500/937, generator loss: 4.125820355443732, discriminator loss: 0.2954499056767522\n",
            "Epoch 13/300, Batch 600/937, generator loss: 4.125742939129027, discriminator loss: 0.2954300184877121\n",
            "Epoch 13/300, Batch 700/937, generator loss: 4.128290266020663, discriminator loss: 0.2954031894765152\n",
            "Epoch 13/300, Batch 800/937, generator loss: 4.13013852275753, discriminator loss: 0.295158810048671\n",
            "Epoch 13/300, Batch 900/937, generator loss: 4.132370973062559, discriminator loss: 0.2947025201699863\n",
            "Saving results...\n",
            "Epoch 14/300, Batch 0/937, generator loss: 4.131811078428559, discriminator loss: 0.2947403914935115\n",
            "Epoch 14/300, Batch 100/937, generator loss: 4.1296214834221985, discriminator loss: 0.2948105872862142\n",
            "Epoch 14/300, Batch 200/937, generator loss: 4.1274459986717495, discriminator loss: 0.2949000001317281\n",
            "Epoch 14/300, Batch 300/937, generator loss: 4.125740521030886, discriminator loss: 0.2951563397060564\n",
            "Epoch 14/300, Batch 400/937, generator loss: 4.12353605239188, discriminator loss: 0.2956418119289923\n",
            "Epoch 14/300, Batch 500/937, generator loss: 4.119441177532688, discriminator loss: 0.29597925557363436\n",
            "Epoch 14/300, Batch 600/937, generator loss: 4.115066913584681, discriminator loss: 0.2962948289386289\n",
            "Epoch 14/300, Batch 700/937, generator loss: 4.112139293910091, discriminator loss: 0.29632758824175626\n",
            "Epoch 14/300, Batch 800/937, generator loss: 4.110762200754464, discriminator loss: 0.29632559520649376\n",
            "Epoch 14/300, Batch 900/937, generator loss: 4.109335943124325, discriminator loss: 0.29623416782794526\n",
            "Saving results...\n",
            "Epoch 15/300, Batch 0/937, generator loss: 4.107758813174564, discriminator loss: 0.29646520767685325\n",
            "Epoch 15/300, Batch 100/937, generator loss: 4.1048906638052, discriminator loss: 0.2963818403249846\n",
            "Epoch 15/300, Batch 200/937, generator loss: 4.101030531977998, discriminator loss: 0.2965744930903786\n",
            "Epoch 15/300, Batch 300/937, generator loss: 4.097203583251836, discriminator loss: 0.29686916047303946\n",
            "Epoch 15/300, Batch 400/937, generator loss: 4.09440238996062, discriminator loss: 0.29726468530181005\n",
            "Epoch 15/300, Batch 500/937, generator loss: 4.089940374931584, discriminator loss: 0.297825941567722\n",
            "Epoch 15/300, Batch 600/937, generator loss: 4.086286176227879, discriminator loss: 0.29767285392204845\n",
            "Epoch 15/300, Batch 700/937, generator loss: 4.082491550888431, discriminator loss: 0.2983023838101444\n",
            "Epoch 15/300, Batch 800/937, generator loss: 4.080311664187048, discriminator loss: 0.29850791436437\n",
            "Epoch 15/300, Batch 900/937, generator loss: 4.077376128283214, discriminator loss: 0.2990894969109387\n",
            "Saving results...\n",
            "Epoch 16/300, Batch 0/937, generator loss: 4.075960381567888, discriminator loss: 0.29916785657704015\n",
            "Epoch 16/300, Batch 100/937, generator loss: 4.071202874669401, discriminator loss: 0.29983155516540916\n",
            "Epoch 16/300, Batch 200/937, generator loss: 4.068997347868039, discriminator loss: 0.30029015009428967\n",
            "Epoch 16/300, Batch 300/937, generator loss: 4.067044040609247, discriminator loss: 0.30069358849442823\n",
            "Epoch 16/300, Batch 400/937, generator loss: 4.065620837667443, discriminator loss: 0.30101638373607303\n",
            "Epoch 16/300, Batch 500/937, generator loss: 4.066058690253402, discriminator loss: 0.3011554553003544\n",
            "Epoch 16/300, Batch 600/937, generator loss: 4.064028518532328, discriminator loss: 0.30152052934102647\n",
            "Epoch 16/300, Batch 700/937, generator loss: 4.061952744802356, discriminator loss: 0.30190561359698037\n",
            "Epoch 16/300, Batch 800/937, generator loss: 4.061195262474187, discriminator loss: 0.3024237755697131\n",
            "Epoch 16/300, Batch 900/937, generator loss: 4.059895244263825, discriminator loss: 0.3027891750896149\n",
            "Saving results...\n",
            "Epoch 17/300, Batch 0/937, generator loss: 4.058892011182232, discriminator loss: 0.30286993154249175\n",
            "Epoch 17/300, Batch 100/937, generator loss: 4.055567248839552, discriminator loss: 0.303327607273758\n",
            "Epoch 17/300, Batch 200/937, generator loss: 4.05261240995455, discriminator loss: 0.30378132943717573\n",
            "Epoch 17/300, Batch 300/937, generator loss: 4.048217103533442, discriminator loss: 0.30463196491100725\n",
            "Epoch 17/300, Batch 400/937, generator loss: 4.041941584160338, discriminator loss: 0.3054988540321205\n",
            "Epoch 17/300, Batch 500/937, generator loss: 4.036783635453052, discriminator loss: 0.30642384918394133\n",
            "Epoch 17/300, Batch 600/937, generator loss: 4.031727344259232, discriminator loss: 0.307049843179444\n",
            "Epoch 17/300, Batch 700/937, generator loss: 4.0264967906819145, discriminator loss: 0.30785542526789195\n",
            "Epoch 17/300, Batch 800/937, generator loss: 4.021271605781645, discriminator loss: 0.30895238201274744\n",
            "Epoch 17/300, Batch 900/937, generator loss: 4.015277001511392, discriminator loss: 0.3097135859997019\n",
            "Saving results...\n",
            "Epoch 18/300, Batch 0/937, generator loss: 4.012930641415604, discriminator loss: 0.31014212169799593\n",
            "Epoch 18/300, Batch 100/937, generator loss: 4.006894491410476, discriminator loss: 0.31102798610140525\n",
            "Epoch 18/300, Batch 200/937, generator loss: 4.001003353176088, discriminator loss: 0.311937447018948\n",
            "Epoch 18/300, Batch 300/937, generator loss: 3.9965175520688367, discriminator loss: 0.31265821185030473\n",
            "Epoch 18/300, Batch 400/937, generator loss: 3.991810881039371, discriminator loss: 0.3133151927976815\n",
            "Epoch 18/300, Batch 500/937, generator loss: 3.9879433409313934, discriminator loss: 0.3140928633041303\n",
            "Epoch 18/300, Batch 600/937, generator loss: 3.9803888982132767, discriminator loss: 0.3149306008333864\n",
            "Epoch 18/300, Batch 700/937, generator loss: 3.9742466269712318, discriminator loss: 0.31590303892059046\n",
            "Epoch 18/300, Batch 800/937, generator loss: 3.9684474044315223, discriminator loss: 0.31673095595902767\n",
            "Epoch 18/300, Batch 900/937, generator loss: 3.9634088993495213, discriminator loss: 0.3175975967840425\n",
            "Saving results...\n",
            "Epoch 19/300, Batch 0/937, generator loss: 3.9615782997235103, discriminator loss: 0.3179575876154757\n",
            "Epoch 19/300, Batch 100/937, generator loss: 3.9553469425971164, discriminator loss: 0.3188560434614605\n",
            "Epoch 19/300, Batch 200/937, generator loss: 3.9485743141451084, discriminator loss: 0.3199354414558098\n",
            "Epoch 19/300, Batch 300/937, generator loss: 3.941543120559975, discriminator loss: 0.32101640726842556\n",
            "Epoch 19/300, Batch 400/937, generator loss: 3.934693856671914, discriminator loss: 0.3220529660166711\n",
            "Epoch 19/300, Batch 500/937, generator loss: 3.9283458487117677, discriminator loss: 0.3229479007447347\n",
            "Epoch 19/300, Batch 600/937, generator loss: 3.920970171926411, discriminator loss: 0.32398333774474336\n",
            "Epoch 19/300, Batch 700/937, generator loss: 3.914098430555386, discriminator loss: 0.3251795987648929\n",
            "Epoch 19/300, Batch 800/937, generator loss: 3.906118696923539, discriminator loss: 0.32607372935402396\n",
            "Epoch 19/300, Batch 900/937, generator loss: 3.8976519364063766, discriminator loss: 0.3275777164900833\n",
            "Saving results...\n",
            "Epoch 20/300, Batch 0/937, generator loss: 3.8949516933689847, discriminator loss: 0.32791048731698813\n",
            "Epoch 20/300, Batch 100/937, generator loss: 3.887864222291055, discriminator loss: 0.3288911023762416\n",
            "Epoch 20/300, Batch 200/937, generator loss: 3.879372054244364, discriminator loss: 0.3302994066851613\n",
            "Epoch 20/300, Batch 300/937, generator loss: 3.8719109042584474, discriminator loss: 0.3314387964169948\n",
            "Epoch 20/300, Batch 400/937, generator loss: 3.8637893828850705, discriminator loss: 0.3328374716706314\n",
            "Epoch 20/300, Batch 500/937, generator loss: 3.857366497570162, discriminator loss: 0.3337369293530526\n",
            "Epoch 20/300, Batch 600/937, generator loss: 3.851101780827145, discriminator loss: 0.33508389427788743\n",
            "Epoch 20/300, Batch 700/937, generator loss: 3.8454110734649514, discriminator loss: 0.3359016925993295\n",
            "Epoch 20/300, Batch 800/937, generator loss: 3.8387780912023355, discriminator loss: 0.3369447795253749\n",
            "Epoch 20/300, Batch 900/937, generator loss: 3.832443245434663, discriminator loss: 0.3381586571792425\n",
            "Saving results...\n",
            "Epoch 21/300, Batch 0/937, generator loss: 3.8301059278406884, discriminator loss: 0.33836908906089397\n",
            "Epoch 21/300, Batch 100/937, generator loss: 3.824048879206488, discriminator loss: 0.3392070918967888\n",
            "Epoch 21/300, Batch 200/937, generator loss: 3.817620432455423, discriminator loss: 0.34035404097423405\n",
            "Epoch 21/300, Batch 300/937, generator loss: 3.8115683086182885, discriminator loss: 0.341407210819236\n",
            "Epoch 21/300, Batch 400/937, generator loss: 3.805525843363167, discriminator loss: 0.34252938445030734\n",
            "Epoch 21/300, Batch 500/937, generator loss: 3.7997643772934717, discriminator loss: 0.34370255604827943\n",
            "Epoch 21/300, Batch 600/937, generator loss: 3.7928908010719677, discriminator loss: 0.344786391486688\n",
            "Epoch 21/300, Batch 700/937, generator loss: 3.7859029897220564, discriminator loss: 0.3460563426309554\n",
            "Epoch 21/300, Batch 800/937, generator loss: 3.779634210490915, discriminator loss: 0.34714959536210926\n",
            "Epoch 21/300, Batch 900/937, generator loss: 3.7724180699686625, discriminator loss: 0.34831955275263077\n",
            "Saving results...\n",
            "Epoch 22/300, Batch 0/937, generator loss: 3.770183649824685, discriminator loss: 0.34863844846539954\n",
            "Epoch 22/300, Batch 100/937, generator loss: 3.7629086089341386, discriminator loss: 0.3500434744596884\n",
            "Epoch 22/300, Batch 200/937, generator loss: 3.755047284480101, discriminator loss: 0.3513218935295392\n",
            "Epoch 22/300, Batch 300/937, generator loss: 3.7482651141708385, discriminator loss: 0.3526194929791614\n",
            "Epoch 22/300, Batch 400/937, generator loss: 3.7418726057799803, discriminator loss: 0.3539395881981161\n",
            "Epoch 22/300, Batch 500/937, generator loss: 3.735204682854321, discriminator loss: 0.354953373613274\n",
            "Epoch 22/300, Batch 600/937, generator loss: 3.72840920008496, discriminator loss: 0.3561667913076256\n",
            "Epoch 22/300, Batch 700/937, generator loss: 3.7223315137248627, discriminator loss: 0.3573485982717272\n",
            "Epoch 22/300, Batch 800/937, generator loss: 3.716289295914739, discriminator loss: 0.3583815029840338\n",
            "Epoch 22/300, Batch 900/937, generator loss: 3.709599479406123, discriminator loss: 0.35941217538785747\n",
            "Saving results...\n",
            "Epoch 23/300, Batch 0/937, generator loss: 3.707871994829191, discriminator loss: 0.35980429518041385\n",
            "Epoch 23/300, Batch 100/937, generator loss: 3.701573004313302, discriminator loss: 0.3609629308210599\n",
            "Epoch 23/300, Batch 200/937, generator loss: 3.6948199746948736, discriminator loss: 0.3621772197268714\n",
            "Epoch 23/300, Batch 300/937, generator loss: 3.688960158252659, discriminator loss: 0.3632392880641321\n",
            "Epoch 23/300, Batch 400/937, generator loss: 3.683431733011352, discriminator loss: 0.36412416320326724\n",
            "Epoch 23/300, Batch 500/937, generator loss: 3.677048609406702, discriminator loss: 0.36525200444542083\n",
            "Epoch 23/300, Batch 600/937, generator loss: 3.671470913531527, discriminator loss: 0.3663677127205129\n",
            "Epoch 23/300, Batch 700/937, generator loss: 3.6648131892977056, discriminator loss: 0.3673873926607821\n",
            "Epoch 23/300, Batch 800/937, generator loss: 3.6598303214531542, discriminator loss: 0.36828122320921614\n",
            "Epoch 23/300, Batch 900/937, generator loss: 3.654229703286466, discriminator loss: 0.36935066934173894\n",
            "Saving results...\n",
            "Epoch 24/300, Batch 0/937, generator loss: 3.6519877344145932, discriminator loss: 0.3697725943089634\n",
            "Epoch 24/300, Batch 100/937, generator loss: 3.646365952571448, discriminator loss: 0.37064097263570367\n",
            "Epoch 24/300, Batch 200/937, generator loss: 3.6409459208411294, discriminator loss: 0.3716147612246553\n",
            "Epoch 24/300, Batch 300/937, generator loss: 3.6354603263204956, discriminator loss: 0.3725550064665944\n",
            "Epoch 24/300, Batch 400/937, generator loss: 3.629427076820892, discriminator loss: 0.37390180918605714\n",
            "Epoch 24/300, Batch 500/937, generator loss: 3.6236909832871023, discriminator loss: 0.37485124867386294\n",
            "Epoch 24/300, Batch 600/937, generator loss: 3.6176711719230954, discriminator loss: 0.3759139110637344\n",
            "Epoch 24/300, Batch 700/937, generator loss: 3.6114765937390336, discriminator loss: 0.37713054272334673\n",
            "Epoch 24/300, Batch 800/937, generator loss: 3.6063457855681156, discriminator loss: 0.37797812178543494\n",
            "Epoch 24/300, Batch 900/937, generator loss: 3.600368864904071, discriminator loss: 0.3791444185334246\n",
            "Saving results...\n",
            "Epoch 25/300, Batch 0/937, generator loss: 3.597960198827805, discriminator loss: 0.37961254781895265\n",
            "Epoch 25/300, Batch 100/937, generator loss: 3.5917294101960135, discriminator loss: 0.3806739963713343\n",
            "Epoch 25/300, Batch 200/937, generator loss: 3.5857451305551473, discriminator loss: 0.38169827719354554\n",
            "Epoch 25/300, Batch 300/937, generator loss: 3.5791001166715937, discriminator loss: 0.38277328196885685\n",
            "Epoch 25/300, Batch 400/937, generator loss: 3.5730823393778848, discriminator loss: 0.3839502005363245\n",
            "Epoch 25/300, Batch 500/937, generator loss: 3.5665482757138953, discriminator loss: 0.38515203632554473\n",
            "Epoch 25/300, Batch 600/937, generator loss: 3.560024352732283, discriminator loss: 0.38643054691672296\n",
            "Epoch 25/300, Batch 700/937, generator loss: 3.5533673947356306, discriminator loss: 0.3877194501205749\n",
            "Epoch 25/300, Batch 800/937, generator loss: 3.546489704561147, discriminator loss: 0.3890190598069219\n",
            "Epoch 25/300, Batch 900/937, generator loss: 3.53983001614944, discriminator loss: 0.39029790546576915\n",
            "Saving results...\n",
            "Epoch 26/300, Batch 0/937, generator loss: 3.5374282260018997, discriminator loss: 0.3906860625597564\n",
            "Epoch 26/300, Batch 100/937, generator loss: 3.530898658731643, discriminator loss: 0.3917590980610093\n",
            "Epoch 26/300, Batch 200/937, generator loss: 3.525705586549674, discriminator loss: 0.39292435539674864\n",
            "Epoch 26/300, Batch 300/937, generator loss: 3.519493696253428, discriminator loss: 0.39403375604773017\n",
            "Epoch 26/300, Batch 400/937, generator loss: 3.5138051140117263, discriminator loss: 0.3951373862001308\n",
            "Epoch 26/300, Batch 500/937, generator loss: 3.5076149880361065, discriminator loss: 0.39629178061707954\n",
            "Epoch 26/300, Batch 600/937, generator loss: 3.502449758619766, discriminator loss: 0.3973388525471972\n",
            "Epoch 26/300, Batch 700/937, generator loss: 3.4978656126330057, discriminator loss: 0.39838314852496504\n",
            "Epoch 26/300, Batch 800/937, generator loss: 3.4934103932990106, discriminator loss: 0.39913259123465367\n",
            "Epoch 26/300, Batch 900/937, generator loss: 3.4879326390756713, discriminator loss: 0.40018017867094396\n",
            "Saving results...\n",
            "Epoch 27/300, Batch 0/937, generator loss: 3.486330377742236, discriminator loss: 0.40041664296105917\n",
            "Epoch 27/300, Batch 100/937, generator loss: 3.4807639400672725, discriminator loss: 0.40144395487397677\n",
            "Epoch 27/300, Batch 200/937, generator loss: 3.4755996705340406, discriminator loss: 0.4024883278935563\n",
            "Epoch 27/300, Batch 300/937, generator loss: 3.470304184036795, discriminator loss: 0.403549547692528\n",
            "Epoch 27/300, Batch 400/937, generator loss: 3.4647432045969055, discriminator loss: 0.40453044378455977\n",
            "Epoch 27/300, Batch 500/937, generator loss: 3.4592398666480717, discriminator loss: 0.4055670836429263\n",
            "Epoch 27/300, Batch 600/937, generator loss: 3.453742050068259, discriminator loss: 0.406664407591562\n",
            "Epoch 27/300, Batch 700/937, generator loss: 3.4482641918361185, discriminator loss: 0.4075260644692641\n",
            "Epoch 27/300, Batch 800/937, generator loss: 3.442931420490203, discriminator loss: 0.40854614036750064\n",
            "Epoch 27/300, Batch 900/937, generator loss: 3.437199950857472, discriminator loss: 0.4095267914201467\n",
            "Saving results...\n",
            "Epoch 28/300, Batch 0/937, generator loss: 3.435375849764108, discriminator loss: 0.4099153364893276\n",
            "Epoch 28/300, Batch 100/937, generator loss: 3.430404949906557, discriminator loss: 0.41075572702129415\n",
            "Epoch 28/300, Batch 200/937, generator loss: 3.425735012611616, discriminator loss: 0.4114585054875791\n",
            "Epoch 28/300, Batch 300/937, generator loss: 3.4206908256097392, discriminator loss: 0.4124600154498592\n",
            "Epoch 28/300, Batch 400/937, generator loss: 3.415555130539434, discriminator loss: 0.41337663642918065\n",
            "Epoch 28/300, Batch 500/937, generator loss: 3.4101717210468716, discriminator loss: 0.4143978475969259\n",
            "Epoch 28/300, Batch 600/937, generator loss: 3.404885425656733, discriminator loss: 0.41548346580640655\n",
            "Epoch 28/300, Batch 700/937, generator loss: 3.400353325948058, discriminator loss: 0.41635400821673435\n",
            "Epoch 28/300, Batch 800/937, generator loss: 3.394965143474367, discriminator loss: 0.4174520805272521\n",
            "Epoch 28/300, Batch 900/937, generator loss: 3.3905937994559223, discriminator loss: 0.41840385255678475\n",
            "Saving results...\n",
            "Epoch 29/300, Batch 0/937, generator loss: 3.3883975893072606, discriminator loss: 0.4188220370469034\n",
            "Epoch 29/300, Batch 100/937, generator loss: 3.383388765225656, discriminator loss: 0.41972728049120456\n",
            "Epoch 29/300, Batch 200/937, generator loss: 3.379319738954591, discriminator loss: 0.4204869245110205\n",
            "Epoch 29/300, Batch 300/937, generator loss: 3.37454109949438, discriminator loss: 0.42146181717570314\n",
            "Epoch 29/300, Batch 400/937, generator loss: 3.3697512053640097, discriminator loss: 0.42233572395447766\n",
            "Epoch 29/300, Batch 500/937, generator loss: 3.3648170467387772, discriminator loss: 0.4233805568906418\n",
            "Epoch 29/300, Batch 600/937, generator loss: 3.3601657891085503, discriminator loss: 0.42422486416651617\n",
            "Epoch 29/300, Batch 700/937, generator loss: 3.355210829443592, discriminator loss: 0.42522408343814677\n",
            "Epoch 29/300, Batch 800/937, generator loss: 3.351196248896376, discriminator loss: 0.4260575459991589\n",
            "Epoch 29/300, Batch 900/937, generator loss: 3.3466219146948744, discriminator loss: 0.4268628375486252\n",
            "Saving results...\n",
            "Epoch 30/300, Batch 0/937, generator loss: 3.3447335711865054, discriminator loss: 0.4273090522315823\n",
            "Epoch 30/300, Batch 100/937, generator loss: 3.3398540997229675, discriminator loss: 0.4282417664803066\n",
            "Epoch 30/300, Batch 200/937, generator loss: 3.3354030856665617, discriminator loss: 0.42903139558184056\n",
            "Epoch 30/300, Batch 300/937, generator loss: 3.3309241506992198, discriminator loss: 0.4299876229318099\n",
            "Epoch 30/300, Batch 400/937, generator loss: 3.3264771936998545, discriminator loss: 0.4310163416707701\n",
            "Epoch 30/300, Batch 500/937, generator loss: 3.3218974770250833, discriminator loss: 0.43183477204638704\n",
            "Epoch 30/300, Batch 600/937, generator loss: 3.3177002497877117, discriminator loss: 0.4327891920629803\n",
            "Epoch 30/300, Batch 700/937, generator loss: 3.3131853361772086, discriminator loss: 0.4336163621694641\n",
            "Epoch 30/300, Batch 800/937, generator loss: 3.3085212540009676, discriminator loss: 0.43462930327115584\n",
            "Epoch 30/300, Batch 900/937, generator loss: 3.303621899183384, discriminator loss: 0.43572228435773785\n",
            "Saving results...\n",
            "Epoch 31/300, Batch 0/937, generator loss: 3.3020743003453115, discriminator loss: 0.43598729268093966\n",
            "Epoch 31/300, Batch 100/937, generator loss: 3.2973029871581265, discriminator loss: 0.43709516012306254\n",
            "Epoch 31/300, Batch 200/937, generator loss: 3.292976059592217, discriminator loss: 0.43808036896500013\n",
            "Epoch 31/300, Batch 300/937, generator loss: 3.2885341848459624, discriminator loss: 0.43893693167149117\n",
            "Epoch 31/300, Batch 400/937, generator loss: 3.2846520671525328, discriminator loss: 0.4398925407862767\n",
            "Epoch 31/300, Batch 500/937, generator loss: 3.280334054203322, discriminator loss: 0.44076043550649774\n",
            "Epoch 31/300, Batch 600/937, generator loss: 3.275710167054688, discriminator loss: 0.4417248786569317\n",
            "Epoch 31/300, Batch 700/937, generator loss: 3.2714710596513568, discriminator loss: 0.4425863313323486\n",
            "Epoch 31/300, Batch 800/937, generator loss: 3.266767969723922, discriminator loss: 0.44356844122804895\n",
            "Epoch 31/300, Batch 900/937, generator loss: 3.262390449304789, discriminator loss: 0.44451582380543725\n",
            "Saving results...\n",
            "Epoch 32/300, Batch 0/937, generator loss: 3.2608797344959477, discriminator loss: 0.4447687726866032\n",
            "Epoch 32/300, Batch 100/937, generator loss: 3.2559891074742313, discriminator loss: 0.4458589151667582\n",
            "Epoch 32/300, Batch 200/937, generator loss: 3.251030036424473, discriminator loss: 0.4469286329771522\n",
            "Epoch 32/300, Batch 300/937, generator loss: 3.246501323500541, discriminator loss: 0.44788595366068595\n",
            "Epoch 32/300, Batch 400/937, generator loss: 3.2419530281389703, discriminator loss: 0.448720358696858\n",
            "Epoch 32/300, Batch 500/937, generator loss: 3.23769358044983, discriminator loss: 0.44969835189183816\n",
            "Epoch 32/300, Batch 600/937, generator loss: 3.233514485139755, discriminator loss: 0.4506498009458523\n",
            "Epoch 32/300, Batch 700/937, generator loss: 3.2295608655917287, discriminator loss: 0.4514670283976607\n",
            "Epoch 32/300, Batch 800/937, generator loss: 3.2250170458189866, discriminator loss: 0.452428893555444\n",
            "Epoch 32/300, Batch 900/937, generator loss: 3.2203311681573745, discriminator loss: 0.45352723028390796\n",
            "Saving results...\n",
            "Epoch 33/300, Batch 0/937, generator loss: 3.2188822505109544, discriminator loss: 0.45381513501646203\n",
            "Epoch 33/300, Batch 100/937, generator loss: 3.2145056652593147, discriminator loss: 0.45483059870648007\n",
            "Epoch 33/300, Batch 200/937, generator loss: 3.2102291346588383, discriminator loss: 0.4558038742401615\n",
            "Epoch 33/300, Batch 300/937, generator loss: 3.206173292898779, discriminator loss: 0.4565688883562991\n",
            "Epoch 33/300, Batch 400/937, generator loss: 3.2017465293004013, discriminator loss: 0.45758573362295385\n",
            "Epoch 33/300, Batch 500/937, generator loss: 3.1972423667125733, discriminator loss: 0.45854706220965696\n",
            "Epoch 33/300, Batch 600/937, generator loss: 3.193340905696621, discriminator loss: 0.4594586503412297\n",
            "Epoch 33/300, Batch 700/937, generator loss: 3.189039974466054, discriminator loss: 0.46028733483532347\n",
            "Epoch 33/300, Batch 800/937, generator loss: 3.184864697587662, discriminator loss: 0.4613633633747835\n",
            "Epoch 33/300, Batch 900/937, generator loss: 3.1804123557507, discriminator loss: 0.4623971179995858\n",
            "Saving results...\n",
            "Epoch 34/300, Batch 0/937, generator loss: 3.179037651758925, discriminator loss: 0.46260419086741095\n",
            "Epoch 34/300, Batch 100/937, generator loss: 3.1749599237007686, discriminator loss: 0.4635428268115173\n",
            "Epoch 34/300, Batch 200/937, generator loss: 3.170760259548647, discriminator loss: 0.4646799989657511\n",
            "Epoch 34/300, Batch 300/937, generator loss: 3.166965787709658, discriminator loss: 0.4655307860462155\n",
            "Epoch 34/300, Batch 400/937, generator loss: 3.162917358645274, discriminator loss: 0.4664490833500203\n",
            "Epoch 34/300, Batch 500/937, generator loss: 3.1589149266095506, discriminator loss: 0.4673865320265575\n",
            "Epoch 34/300, Batch 600/937, generator loss: 3.154927776884785, discriminator loss: 0.46835743521009\n",
            "Epoch 34/300, Batch 700/937, generator loss: 3.1509873537936137, discriminator loss: 0.4693097878177591\n",
            "Epoch 34/300, Batch 800/937, generator loss: 3.1467558188977756, discriminator loss: 0.470235089279111\n",
            "Epoch 34/300, Batch 900/937, generator loss: 3.1426892721822925, discriminator loss: 0.471230110896078\n",
            "Saving results...\n",
            "Epoch 35/300, Batch 0/937, generator loss: 3.1416261763630997, discriminator loss: 0.47137509510130077\n",
            "Epoch 35/300, Batch 100/937, generator loss: 3.1373804589889307, discriminator loss: 0.4724569902266104\n",
            "Epoch 35/300, Batch 200/937, generator loss: 3.133739927276682, discriminator loss: 0.47331266192063837\n",
            "Epoch 35/300, Batch 300/937, generator loss: 3.129833697089473, discriminator loss: 0.4742196319466747\n",
            "Epoch 35/300, Batch 400/937, generator loss: 3.1260744836610086, discriminator loss: 0.4750741970659035\n",
            "Epoch 35/300, Batch 500/937, generator loss: 3.1220788931145598, discriminator loss: 0.47590344712936583\n",
            "Epoch 35/300, Batch 600/937, generator loss: 3.118208069837526, discriminator loss: 0.47694628968958025\n",
            "Epoch 35/300, Batch 700/937, generator loss: 3.1143350771970644, discriminator loss: 0.4778023852924686\n",
            "Epoch 35/300, Batch 800/937, generator loss: 3.1104353595144025, discriminator loss: 0.4786365839802079\n",
            "Epoch 35/300, Batch 900/937, generator loss: 3.1069244414635167, discriminator loss: 0.47963790303929327\n",
            "Saving results...\n",
            "Epoch 36/300, Batch 0/937, generator loss: 3.1056672748226504, discriminator loss: 0.47991962515007\n",
            "Epoch 36/300, Batch 100/937, generator loss: 3.1020445762836864, discriminator loss: 0.4806979162436234\n",
            "Epoch 36/300, Batch 200/937, generator loss: 3.0982515423356127, discriminator loss: 0.48162744149191994\n",
            "Epoch 36/300, Batch 300/937, generator loss: 3.095054762331489, discriminator loss: 0.4824883803347383\n",
            "Epoch 36/300, Batch 400/937, generator loss: 3.091704303045932, discriminator loss: 0.48315390492290267\n",
            "Epoch 36/300, Batch 500/937, generator loss: 3.088648617810846, discriminator loss: 0.4839576364740552\n",
            "Epoch 36/300, Batch 600/937, generator loss: 3.0849996208068227, discriminator loss: 0.48479014079889443\n",
            "Epoch 36/300, Batch 700/937, generator loss: 3.0818242283237596, discriminator loss: 0.4854808936866401\n",
            "Epoch 36/300, Batch 800/937, generator loss: 3.07800656303013, discriminator loss: 0.48651714201868695\n",
            "Epoch 36/300, Batch 900/937, generator loss: 3.0747747803937036, discriminator loss: 0.487329359585746\n",
            "Saving results...\n",
            "Epoch 37/300, Batch 0/937, generator loss: 3.073454800573934, discriminator loss: 0.487641605548292\n",
            "Epoch 37/300, Batch 100/937, generator loss: 3.069909526366148, discriminator loss: 0.48843508801105073\n",
            "Epoch 37/300, Batch 200/937, generator loss: 3.0663523472599836, discriminator loss: 0.4892607838206008\n",
            "Epoch 37/300, Batch 300/937, generator loss: 3.0631338985214445, discriminator loss: 0.4899969381785986\n",
            "Epoch 37/300, Batch 400/937, generator loss: 3.059674582899144, discriminator loss: 0.4907624135635706\n",
            "Epoch 37/300, Batch 500/937, generator loss: 3.056290604456738, discriminator loss: 0.4915857839752132\n",
            "Epoch 37/300, Batch 600/937, generator loss: 3.0528004855314665, discriminator loss: 0.4924986631489354\n",
            "Epoch 37/300, Batch 700/937, generator loss: 3.0493056388133186, discriminator loss: 0.493254634358804\n",
            "Epoch 37/300, Batch 800/937, generator loss: 3.0456357216848464, discriminator loss: 0.4941302189372915\n",
            "Epoch 37/300, Batch 900/937, generator loss: 3.042258293128664, discriminator loss: 0.49486782742930757\n",
            "Saving results...\n",
            "Epoch 38/300, Batch 0/937, generator loss: 3.041046685774351, discriminator loss: 0.49518784900725626\n",
            "Epoch 38/300, Batch 100/937, generator loss: 3.0376457824293586, discriminator loss: 0.4960538304330828\n",
            "Epoch 38/300, Batch 200/937, generator loss: 3.034094740252668, discriminator loss: 0.49683621199667616\n",
            "Epoch 38/300, Batch 300/937, generator loss: 3.031193522036907, discriminator loss: 0.4975273303892372\n",
            "Epoch 38/300, Batch 400/937, generator loss: 3.027701165555143, discriminator loss: 0.4984043845019424\n",
            "Epoch 38/300, Batch 500/937, generator loss: 3.0247121176565286, discriminator loss: 0.49913785852279113\n",
            "Epoch 38/300, Batch 600/937, generator loss: 3.021096979866836, discriminator loss: 0.49998978218153994\n",
            "Epoch 38/300, Batch 700/937, generator loss: 3.017954369615326, discriminator loss: 0.5007668169612456\n",
            "Epoch 38/300, Batch 800/937, generator loss: 3.014470458328667, discriminator loss: 0.5016786552374006\n",
            "Epoch 38/300, Batch 900/937, generator loss: 3.0111252319438235, discriminator loss: 0.5026028429380716\n",
            "Saving results...\n",
            "Epoch 39/300, Batch 0/937, generator loss: 3.0098636415208024, discriminator loss: 0.5028900488095452\n",
            "Epoch 39/300, Batch 100/937, generator loss: 3.0067144955704923, discriminator loss: 0.503664208586046\n",
            "Epoch 39/300, Batch 200/937, generator loss: 3.003564233253549, discriminator loss: 0.5044382744741068\n",
            "Epoch 39/300, Batch 300/937, generator loss: 3.000208535721527, discriminator loss: 0.5053603965289597\n",
            "Epoch 39/300, Batch 400/937, generator loss: 2.9966891327684912, discriminator loss: 0.5061395006603612\n",
            "Epoch 39/300, Batch 500/937, generator loss: 2.9931921066462692, discriminator loss: 0.5070888652670281\n",
            "Epoch 39/300, Batch 600/937, generator loss: 2.9897836813145147, discriminator loss: 0.5079420367626891\n",
            "Epoch 39/300, Batch 700/937, generator loss: 2.9863297467475776, discriminator loss: 0.5087391587536197\n",
            "Epoch 39/300, Batch 800/937, generator loss: 2.9829569292041667, discriminator loss: 0.5095696831489143\n",
            "Epoch 39/300, Batch 900/937, generator loss: 2.9792936307225597, discriminator loss: 0.5104334550442992\n",
            "Saving results...\n",
            "Epoch 40/300, Batch 0/937, generator loss: 2.978128866726852, discriminator loss: 0.5107551000588719\n",
            "Epoch 40/300, Batch 100/937, generator loss: 2.974489741256735, discriminator loss: 0.5116888319790032\n",
            "Epoch 40/300, Batch 200/937, generator loss: 2.9710338550819806, discriminator loss: 0.5125276878915495\n",
            "Epoch 40/300, Batch 300/937, generator loss: 2.9678437410909537, discriminator loss: 0.5132526630265897\n",
            "Epoch 40/300, Batch 400/937, generator loss: 2.964628528243923, discriminator loss: 0.5141851467769472\n",
            "Epoch 40/300, Batch 500/937, generator loss: 2.961554388977843, discriminator loss: 0.5149337113066805\n",
            "Epoch 40/300, Batch 600/937, generator loss: 2.9584136780080716, discriminator loss: 0.5157521495286984\n",
            "Epoch 40/300, Batch 700/937, generator loss: 2.955449557264418, discriminator loss: 0.5164415429423758\n",
            "Epoch 40/300, Batch 800/937, generator loss: 2.952080468281207, discriminator loss: 0.5173325938207939\n",
            "Epoch 40/300, Batch 900/937, generator loss: 2.949141054419187, discriminator loss: 0.5181489942561828\n",
            "Saving results...\n",
            "Epoch 41/300, Batch 0/937, generator loss: 2.947905790480627, discriminator loss: 0.5184214417036981\n",
            "Epoch 41/300, Batch 100/937, generator loss: 2.944493973182118, discriminator loss: 0.5192926269293673\n",
            "Epoch 41/300, Batch 200/937, generator loss: 2.941296665679847, discriminator loss: 0.5201110243341986\n",
            "Epoch 41/300, Batch 300/937, generator loss: 2.937942053699144, discriminator loss: 0.5209471076154236\n",
            "Epoch 41/300, Batch 400/937, generator loss: 2.9348883520132536, discriminator loss: 0.5216643174356775\n",
            "Epoch 41/300, Batch 500/937, generator loss: 2.9319474820187943, discriminator loss: 0.5223079994531457\n",
            "Epoch 41/300, Batch 600/937, generator loss: 2.9290416914188877, discriminator loss: 0.523138227211903\n",
            "Epoch 41/300, Batch 700/937, generator loss: 2.9259012468973244, discriminator loss: 0.5239999193860634\n",
            "Epoch 41/300, Batch 800/937, generator loss: 2.922982201980994, discriminator loss: 0.5246492617177637\n",
            "Epoch 41/300, Batch 900/937, generator loss: 2.9197391585577503, discriminator loss: 0.5254931955304719\n",
            "Saving results...\n",
            "Epoch 42/300, Batch 0/937, generator loss: 2.9186025089660848, discriminator loss: 0.5257613566010838\n",
            "Epoch 42/300, Batch 100/937, generator loss: 2.9154578993792106, discriminator loss: 0.5265715728323053\n",
            "Epoch 42/300, Batch 200/937, generator loss: 2.912344406697169, discriminator loss: 0.5273693928185432\n",
            "Epoch 42/300, Batch 300/937, generator loss: 2.90923392465445, discriminator loss: 0.5281701234787564\n",
            "Epoch 42/300, Batch 400/937, generator loss: 2.9061231984895457, discriminator loss: 0.5289563950450596\n",
            "Epoch 42/300, Batch 500/937, generator loss: 2.9029188230246765, discriminator loss: 0.5298246857393433\n",
            "Epoch 42/300, Batch 600/937, generator loss: 2.8999905189604385, discriminator loss: 0.5305006787455018\n",
            "Epoch 42/300, Batch 700/937, generator loss: 2.8970363031238997, discriminator loss: 0.531223254519416\n",
            "Epoch 42/300, Batch 800/937, generator loss: 2.8943212471066486, discriminator loss: 0.5319801630437664\n",
            "Epoch 42/300, Batch 900/937, generator loss: 2.8912847762255325, discriminator loss: 0.5327277554308993\n",
            "Saving results...\n",
            "Epoch 43/300, Batch 0/937, generator loss: 2.8903086292202844, discriminator loss: 0.5329802643693003\n",
            "Epoch 43/300, Batch 100/937, generator loss: 2.88718020519034, discriminator loss: 0.5337816768071868\n",
            "Epoch 43/300, Batch 200/937, generator loss: 2.884238278351788, discriminator loss: 0.5344692722682742\n",
            "Epoch 43/300, Batch 300/937, generator loss: 2.8811264845315376, discriminator loss: 0.5352908657099442\n",
            "Epoch 43/300, Batch 400/937, generator loss: 2.878141528087931, discriminator loss: 0.5360186162420187\n",
            "Epoch 43/300, Batch 500/937, generator loss: 2.8752906731932413, discriminator loss: 0.536712296394758\n",
            "Epoch 43/300, Batch 600/937, generator loss: 2.872305645295392, discriminator loss: 0.5375413886636458\n",
            "Epoch 43/300, Batch 700/937, generator loss: 2.8693278686264874, discriminator loss: 0.5382964030245521\n",
            "Epoch 43/300, Batch 800/937, generator loss: 2.866193996732033, discriminator loss: 0.5390419209567989\n",
            "Epoch 43/300, Batch 900/937, generator loss: 2.863427399942182, discriminator loss: 0.5398161532051657\n",
            "Saving results...\n",
            "Epoch 44/300, Batch 0/937, generator loss: 2.8623339124586833, discriminator loss: 0.5400656559748417\n",
            "Epoch 44/300, Batch 100/937, generator loss: 2.859343532262198, discriminator loss: 0.5407694804379698\n",
            "Epoch 44/300, Batch 200/937, generator loss: 2.8564144863528487, discriminator loss: 0.541502148880517\n",
            "Epoch 44/300, Batch 300/937, generator loss: 2.8534907821173237, discriminator loss: 0.5422290891682336\n",
            "Epoch 44/300, Batch 400/937, generator loss: 2.8507705105625636, discriminator loss: 0.5429246762675753\n",
            "Epoch 44/300, Batch 500/937, generator loss: 2.847922884032986, discriminator loss: 0.5436238547678023\n",
            "Epoch 44/300, Batch 600/937, generator loss: 2.8454919616461125, discriminator loss: 0.5442619310934329\n",
            "Epoch 44/300, Batch 700/937, generator loss: 2.8427501630398124, discriminator loss: 0.544986370427003\n",
            "Epoch 44/300, Batch 800/937, generator loss: 2.840386486505299, discriminator loss: 0.545577170694038\n",
            "Epoch 44/300, Batch 900/937, generator loss: 2.8378432415512966, discriminator loss: 0.5461947071031342\n",
            "Saving results...\n",
            "Epoch 45/300, Batch 0/937, generator loss: 2.8368144691476718, discriminator loss: 0.5464767869546892\n",
            "Epoch 45/300, Batch 100/937, generator loss: 2.834186028173179, discriminator loss: 0.547202078828368\n",
            "Epoch 45/300, Batch 200/937, generator loss: 2.8316939811525224, discriminator loss: 0.5478158535994422\n",
            "Epoch 45/300, Batch 300/937, generator loss: 2.8291006639683314, discriminator loss: 0.5485212007440198\n",
            "Epoch 45/300, Batch 400/937, generator loss: 2.8262623488085277, discriminator loss: 0.5492846719906282\n",
            "Epoch 45/300, Batch 500/937, generator loss: 2.8236190647366, discriminator loss: 0.5499537753891477\n",
            "Epoch 45/300, Batch 600/937, generator loss: 2.821048648347189, discriminator loss: 0.5505802527732768\n",
            "Epoch 45/300, Batch 700/937, generator loss: 2.818485977883791, discriminator loss: 0.5512959966084496\n",
            "Epoch 45/300, Batch 800/937, generator loss: 2.815991579444472, discriminator loss: 0.5519246220129503\n",
            "Epoch 45/300, Batch 900/937, generator loss: 2.813370004986242, discriminator loss: 0.5525599923089506\n",
            "Saving results...\n",
            "Epoch 46/300, Batch 0/937, generator loss: 2.8123542432264754, discriminator loss: 0.5528513467631572\n",
            "Epoch 46/300, Batch 100/937, generator loss: 2.8098001266823784, discriminator loss: 0.5535066099073661\n",
            "Epoch 46/300, Batch 200/937, generator loss: 2.807192888040591, discriminator loss: 0.5541365222101126\n",
            "Epoch 46/300, Batch 300/937, generator loss: 2.804621655794123, discriminator loss: 0.5548015673977712\n",
            "Epoch 46/300, Batch 400/937, generator loss: 2.8021193707663117, discriminator loss: 0.555461053408812\n",
            "Epoch 46/300, Batch 500/937, generator loss: 2.7995752432194716, discriminator loss: 0.5561113802846128\n",
            "Epoch 46/300, Batch 600/937, generator loss: 2.796950347798123, discriminator loss: 0.5567791056272119\n",
            "Epoch 46/300, Batch 700/937, generator loss: 2.7940594887465666, discriminator loss: 0.5575479589870601\n",
            "Epoch 46/300, Batch 800/937, generator loss: 2.7914180752967828, discriminator loss: 0.5582347126273776\n",
            "Epoch 46/300, Batch 900/937, generator loss: 2.788740182253154, discriminator loss: 0.5589523957375355\n",
            "Saving results...\n",
            "Epoch 47/300, Batch 0/937, generator loss: 2.787752881265845, discriminator loss: 0.5591701690774143\n",
            "Epoch 47/300, Batch 100/937, generator loss: 2.784924046117956, discriminator loss: 0.5599475506782585\n",
            "Epoch 47/300, Batch 200/937, generator loss: 2.7821945809097337, discriminator loss: 0.5606942944046763\n",
            "Epoch 47/300, Batch 300/937, generator loss: 2.77944132601376, discriminator loss: 0.5613895357239875\n",
            "Epoch 47/300, Batch 400/937, generator loss: 2.776453688695128, discriminator loss: 0.5622334001953005\n",
            "Epoch 47/300, Batch 500/937, generator loss: 2.773686933263193, discriminator loss: 0.5628793097186046\n",
            "Epoch 47/300, Batch 600/937, generator loss: 2.7710908274656982, discriminator loss: 0.5636039408003931\n",
            "Epoch 47/300, Batch 700/937, generator loss: 2.768610121173202, discriminator loss: 0.5642471889532673\n",
            "Epoch 47/300, Batch 800/937, generator loss: 2.7660912452586306, discriminator loss: 0.5649240204544338\n",
            "Epoch 47/300, Batch 900/937, generator loss: 2.7636542838841054, discriminator loss: 0.5656644673727065\n",
            "Saving results...\n",
            "Epoch 48/300, Batch 0/937, generator loss: 2.7628865213326526, discriminator loss: 0.565818193628627\n",
            "Epoch 48/300, Batch 100/937, generator loss: 2.7603763367820475, discriminator loss: 0.5665385924610427\n",
            "Epoch 48/300, Batch 200/937, generator loss: 2.7577706950065015, discriminator loss: 0.5671641825884202\n",
            "Epoch 48/300, Batch 300/937, generator loss: 2.7553755826747026, discriminator loss: 0.5677928946314788\n",
            "Epoch 48/300, Batch 400/937, generator loss: 2.7528468842419924, discriminator loss: 0.5684500380092471\n",
            "Epoch 48/300, Batch 500/937, generator loss: 2.7504540288065886, discriminator loss: 0.5690798803701599\n",
            "Epoch 48/300, Batch 600/937, generator loss: 2.74806021763123, discriminator loss: 0.5697064693714503\n",
            "Epoch 48/300, Batch 700/937, generator loss: 2.7452477153458257, discriminator loss: 0.5705205575184649\n",
            "Epoch 48/300, Batch 800/937, generator loss: 2.742727959617228, discriminator loss: 0.5711949905354768\n",
            "Epoch 48/300, Batch 900/937, generator loss: 2.7401259685764727, discriminator loss: 0.5718308073695526\n",
            "Saving results...\n",
            "Epoch 49/300, Batch 0/937, generator loss: 2.739203438381547, discriminator loss: 0.572083987619199\n",
            "Epoch 49/300, Batch 100/937, generator loss: 2.7367317321849014, discriminator loss: 0.5727153295750578\n",
            "Epoch 49/300, Batch 200/937, generator loss: 2.7340027435275602, discriminator loss: 0.5735252741810692\n",
            "Epoch 49/300, Batch 300/937, generator loss: 2.73162860228735, discriminator loss: 0.5741585658930657\n",
            "Epoch 49/300, Batch 400/937, generator loss: 2.7292817813691097, discriminator loss: 0.5747505866337632\n",
            "Epoch 49/300, Batch 500/937, generator loss: 2.7268060901546467, discriminator loss: 0.5754309475823859\n",
            "Epoch 49/300, Batch 600/937, generator loss: 2.724539179852583, discriminator loss: 0.5760152146778272\n",
            "Epoch 49/300, Batch 700/937, generator loss: 2.722192672242446, discriminator loss: 0.5766707007104828\n",
            "Epoch 49/300, Batch 800/937, generator loss: 2.7199291667719994, discriminator loss: 0.5772268898271864\n",
            "Epoch 49/300, Batch 900/937, generator loss: 2.7175552220561907, discriminator loss: 0.5778744032226665\n",
            "Saving results...\n",
            "Epoch 50/300, Batch 0/937, generator loss: 2.716639193498028, discriminator loss: 0.5781725723580188\n",
            "Epoch 50/300, Batch 100/937, generator loss: 2.714272129015431, discriminator loss: 0.5788118945946975\n",
            "Epoch 50/300, Batch 200/937, generator loss: 2.7121042424399975, discriminator loss: 0.5794290898311192\n",
            "Epoch 50/300, Batch 300/937, generator loss: 2.709777027753471, discriminator loss: 0.5801022854793798\n",
            "Epoch 50/300, Batch 400/937, generator loss: 2.707493147061178, discriminator loss: 0.580701402087244\n",
            "Epoch 50/300, Batch 500/937, generator loss: 2.705254877666715, discriminator loss: 0.5813164719262447\n",
            "Epoch 50/300, Batch 600/937, generator loss: 2.7027872704532068, discriminator loss: 0.5819618428381012\n",
            "Epoch 50/300, Batch 700/937, generator loss: 2.700453676217708, discriminator loss: 0.582574490883411\n",
            "Epoch 50/300, Batch 800/937, generator loss: 2.6981949565485714, discriminator loss: 0.5831298503020695\n",
            "Epoch 50/300, Batch 900/937, generator loss: 2.695734129585183, discriminator loss: 0.5838059741359354\n",
            "Saving results...\n",
            "Epoch 51/300, Batch 0/937, generator loss: 2.694965902362008, discriminator loss: 0.5840231212713664\n",
            "Epoch 51/300, Batch 100/937, generator loss: 2.69271204441741, discriminator loss: 0.5846499260950608\n",
            "Epoch 51/300, Batch 200/937, generator loss: 2.6906477487814113, discriminator loss: 0.5852658561800156\n",
            "Epoch 51/300, Batch 300/937, generator loss: 2.68837971386542, discriminator loss: 0.5858917443527644\n",
            "Epoch 51/300, Batch 400/937, generator loss: 2.6862345067056728, discriminator loss: 0.5864620063989808\n",
            "Epoch 51/300, Batch 500/937, generator loss: 2.684045441416595, discriminator loss: 0.5870409785937135\n",
            "Epoch 51/300, Batch 600/937, generator loss: 2.682000446141707, discriminator loss: 0.5876072445702502\n",
            "Epoch 51/300, Batch 700/937, generator loss: 2.679906509962929, discriminator loss: 0.5881607030856247\n",
            "Epoch 51/300, Batch 800/937, generator loss: 2.6773924437228316, discriminator loss: 0.5888718223726591\n",
            "Epoch 51/300, Batch 900/937, generator loss: 2.6753024870589144, discriminator loss: 0.5893741464465758\n",
            "Saving results...\n",
            "Epoch 52/300, Batch 0/937, generator loss: 2.6744553103307505, discriminator loss: 0.5895936699638983\n",
            "Epoch 52/300, Batch 100/937, generator loss: 2.672193094491348, discriminator loss: 0.5901599021239947\n",
            "Epoch 52/300, Batch 200/937, generator loss: 2.6702144786784037, discriminator loss: 0.5907992153607526\n",
            "Epoch 52/300, Batch 300/937, generator loss: 2.6679425966587678, discriminator loss: 0.5913525831796635\n",
            "Epoch 52/300, Batch 400/937, generator loss: 2.6659208099229343, discriminator loss: 0.5919916079917937\n",
            "Epoch 52/300, Batch 500/937, generator loss: 2.6636600930536987, discriminator loss: 0.592585665808528\n",
            "Epoch 52/300, Batch 600/937, generator loss: 2.661352557841135, discriminator loss: 0.5931965219527408\n",
            "Epoch 52/300, Batch 700/937, generator loss: 2.658978863741692, discriminator loss: 0.5938792972035281\n",
            "Epoch 52/300, Batch 800/937, generator loss: 2.6566050350395254, discriminator loss: 0.5945329526390227\n",
            "Epoch 52/300, Batch 900/937, generator loss: 2.6543544852931795, discriminator loss: 0.5951435622009883\n",
            "Saving results...\n",
            "Epoch 53/300, Batch 0/937, generator loss: 2.6534949584179324, discriminator loss: 0.595450140044647\n",
            "Epoch 53/300, Batch 100/937, generator loss: 2.6512944463661863, discriminator loss: 0.5960618468939283\n",
            "Epoch 53/300, Batch 200/937, generator loss: 2.6490594071365714, discriminator loss: 0.5966519552531893\n",
            "Epoch 53/300, Batch 300/937, generator loss: 2.6467891294138, discriminator loss: 0.5973439869035412\n",
            "Epoch 53/300, Batch 400/937, generator loss: 2.6445663945520876, discriminator loss: 0.5979590077194421\n",
            "Epoch 53/300, Batch 500/937, generator loss: 2.6422347979517835, discriminator loss: 0.5986195461668802\n",
            "Epoch 53/300, Batch 600/937, generator loss: 2.6400228153411254, discriminator loss: 0.5992168739866752\n",
            "Epoch 53/300, Batch 700/937, generator loss: 2.6377034765208083, discriminator loss: 0.5998571477180951\n",
            "Epoch 53/300, Batch 800/937, generator loss: 2.635286525976766, discriminator loss: 0.6005530440450862\n",
            "Epoch 53/300, Batch 900/937, generator loss: 2.6331392404899288, discriminator loss: 0.6011211023328189\n",
            "Saving results...\n",
            "Epoch 54/300, Batch 0/937, generator loss: 2.632208804940138, discriminator loss: 0.6014009781772128\n",
            "Epoch 54/300, Batch 100/937, generator loss: 2.630063134513312, discriminator loss: 0.6019977861318239\n",
            "Epoch 54/300, Batch 200/937, generator loss: 2.627757874095357, discriminator loss: 0.6026073778539678\n",
            "Epoch 54/300, Batch 300/937, generator loss: 2.6256759510979175, discriminator loss: 0.6032165870128716\n",
            "Epoch 54/300, Batch 400/937, generator loss: 2.6235101582917557, discriminator loss: 0.6038698194201043\n",
            "Epoch 54/300, Batch 500/937, generator loss: 2.6214900798052385, discriminator loss: 0.6044884546312485\n",
            "Epoch 54/300, Batch 600/937, generator loss: 2.6194306126739666, discriminator loss: 0.6050787138773358\n",
            "Epoch 54/300, Batch 700/937, generator loss: 2.617348200048038, discriminator loss: 0.6056554307267016\n",
            "Epoch 54/300, Batch 800/937, generator loss: 2.615191532945389, discriminator loss: 0.606257282352723\n",
            "Epoch 54/300, Batch 900/937, generator loss: 2.6132572762500743, discriminator loss: 0.6068194818311753\n",
            "Saving results...\n",
            "Epoch 55/300, Batch 0/937, generator loss: 2.6126146722990917, discriminator loss: 0.6070077974587512\n",
            "Epoch 55/300, Batch 100/937, generator loss: 2.6105720444328235, discriminator loss: 0.6075821684456849\n",
            "Epoch 55/300, Batch 200/937, generator loss: 2.6083621864195714, discriminator loss: 0.608276927556531\n",
            "Epoch 55/300, Batch 300/937, generator loss: 2.6062963801554777, discriminator loss: 0.6087975124092242\n",
            "Epoch 55/300, Batch 400/937, generator loss: 2.604218190799864, discriminator loss: 0.6094133239107474\n",
            "Epoch 55/300, Batch 500/937, generator loss: 2.6020799439931066, discriminator loss: 0.6100442155643534\n",
            "Epoch 55/300, Batch 600/937, generator loss: 2.5999750717742707, discriminator loss: 0.6106393916073095\n",
            "Epoch 55/300, Batch 700/937, generator loss: 2.597906076856307, discriminator loss: 0.6112561568763877\n",
            "Epoch 55/300, Batch 800/937, generator loss: 2.595906325236979, discriminator loss: 0.611830306655973\n",
            "Epoch 55/300, Batch 900/937, generator loss: 2.5939044081682345, discriminator loss: 0.6123916341006206\n",
            "Saving results...\n",
            "Epoch 56/300, Batch 0/937, generator loss: 2.5931703776551083, discriminator loss: 0.6125408499042045\n",
            "Epoch 56/300, Batch 100/937, generator loss: 2.5913245466038766, discriminator loss: 0.6131659211374446\n",
            "Epoch 56/300, Batch 200/937, generator loss: 2.589328238525383, discriminator loss: 0.6137333480287657\n",
            "Epoch 56/300, Batch 300/937, generator loss: 2.587362959123427, discriminator loss: 0.6142615888585343\n",
            "Epoch 56/300, Batch 400/937, generator loss: 2.585497781724798, discriminator loss: 0.6147809046408917\n",
            "Epoch 56/300, Batch 500/937, generator loss: 2.58329540044179, discriminator loss: 0.6154341554394407\n",
            "Epoch 56/300, Batch 600/937, generator loss: 2.5813438944800597, discriminator loss: 0.616014927152844\n",
            "Epoch 56/300, Batch 700/937, generator loss: 2.5790749104478135, discriminator loss: 0.6166347761810652\n",
            "Epoch 56/300, Batch 800/937, generator loss: 2.5770800520604737, discriminator loss: 0.6172164039345037\n",
            "Epoch 56/300, Batch 900/937, generator loss: 2.5749848369129493, discriminator loss: 0.6178134275578712\n",
            "Saving results...\n",
            "Epoch 57/300, Batch 0/937, generator loss: 2.574294829904263, discriminator loss: 0.6180299659973671\n",
            "Epoch 57/300, Batch 100/937, generator loss: 2.572338436695153, discriminator loss: 0.6185846972573563\n",
            "Epoch 57/300, Batch 200/937, generator loss: 2.5703710059046947, discriminator loss: 0.6192219859099259\n",
            "Epoch 57/300, Batch 300/937, generator loss: 2.5681986518710533, discriminator loss: 0.6198592385978722\n",
            "Epoch 57/300, Batch 400/937, generator loss: 2.566141318369922, discriminator loss: 0.6204325640413136\n",
            "Epoch 57/300, Batch 500/937, generator loss: 2.5641017072089327, discriminator loss: 0.6210361334244233\n",
            "Epoch 57/300, Batch 600/937, generator loss: 2.56200813782853, discriminator loss: 0.621607265998712\n",
            "Epoch 57/300, Batch 700/937, generator loss: 2.5601652233057774, discriminator loss: 0.6221579177595291\n",
            "Epoch 57/300, Batch 800/937, generator loss: 2.5581966586657017, discriminator loss: 0.6226756648628325\n",
            "Epoch 57/300, Batch 900/937, generator loss: 2.5562172883651333, discriminator loss: 0.6232470848885848\n",
            "Saving results...\n",
            "Epoch 58/300, Batch 0/937, generator loss: 2.5555048273123293, discriminator loss: 0.6235074300895362\n",
            "Epoch 58/300, Batch 100/937, generator loss: 2.5536043604869634, discriminator loss: 0.6240205430555583\n",
            "Epoch 58/300, Batch 200/937, generator loss: 2.551897940335052, discriminator loss: 0.624533010852508\n",
            "Epoch 58/300, Batch 300/937, generator loss: 2.5501752192065257, discriminator loss: 0.6251295712227934\n",
            "Epoch 58/300, Batch 400/937, generator loss: 2.5485141678303, discriminator loss: 0.6256364375548287\n",
            "Epoch 58/300, Batch 500/937, generator loss: 2.5466307582328285, discriminator loss: 0.6261756997006349\n",
            "Epoch 58/300, Batch 600/937, generator loss: 2.544867594468769, discriminator loss: 0.6267119317979447\n",
            "Epoch 58/300, Batch 700/937, generator loss: 2.5428541467449963, discriminator loss: 0.627316258424205\n",
            "Epoch 58/300, Batch 800/937, generator loss: 2.541021210504386, discriminator loss: 0.6278532294038611\n",
            "Epoch 58/300, Batch 900/937, generator loss: 2.539059141960602, discriminator loss: 0.6283868088778028\n",
            "Saving results...\n",
            "Epoch 59/300, Batch 0/937, generator loss: 2.5383746770772753, discriminator loss: 0.6286083670951215\n",
            "Epoch 59/300, Batch 100/937, generator loss: 2.536374767619923, discriminator loss: 0.6291609411082799\n",
            "Epoch 59/300, Batch 200/937, generator loss: 2.534474852165297, discriminator loss: 0.6297069576633156\n",
            "Epoch 59/300, Batch 300/937, generator loss: 2.532609435660517, discriminator loss: 0.6302504202803499\n",
            "Epoch 59/300, Batch 400/937, generator loss: 2.5307451463627255, discriminator loss: 0.6308230821664893\n",
            "Epoch 59/300, Batch 500/937, generator loss: 2.52887346149538, discriminator loss: 0.6314155667390908\n",
            "Epoch 59/300, Batch 600/937, generator loss: 2.527014172355129, discriminator loss: 0.6319748467096671\n",
            "Epoch 59/300, Batch 700/937, generator loss: 2.525223009212719, discriminator loss: 0.632532128085768\n",
            "Epoch 59/300, Batch 800/937, generator loss: 2.523408340125857, discriminator loss: 0.6330414783257653\n",
            "Epoch 59/300, Batch 900/937, generator loss: 2.521490242413378, discriminator loss: 0.633633485777089\n",
            "Saving results...\n",
            "Epoch 60/300, Batch 0/937, generator loss: 2.5207955198647496, discriminator loss: 0.6338418758766189\n",
            "Epoch 60/300, Batch 100/937, generator loss: 2.5189768261833794, discriminator loss: 0.6343721780380368\n",
            "Epoch 60/300, Batch 200/937, generator loss: 2.5171319897050894, discriminator loss: 0.6348919157816352\n",
            "Epoch 60/300, Batch 300/937, generator loss: 2.515281838732594, discriminator loss: 0.6353794817139191\n",
            "Epoch 60/300, Batch 400/937, generator loss: 2.5135957452793214, discriminator loss: 0.6359376359184018\n",
            "Epoch 60/300, Batch 500/937, generator loss: 2.5117864615195784, discriminator loss: 0.6364304653670891\n",
            "Epoch 60/300, Batch 600/937, generator loss: 2.5099065329775474, discriminator loss: 0.636987838995553\n",
            "Epoch 60/300, Batch 700/937, generator loss: 2.508100457336476, discriminator loss: 0.6375792556423193\n",
            "Epoch 60/300, Batch 800/937, generator loss: 2.506410240570909, discriminator loss: 0.6381010275173208\n",
            "Epoch 60/300, Batch 900/937, generator loss: 2.5045691295818853, discriminator loss: 0.6386087714281761\n",
            "Saving results...\n",
            "Epoch 61/300, Batch 0/937, generator loss: 2.5039068089893313, discriminator loss: 0.6387881812921385\n",
            "Epoch 61/300, Batch 100/937, generator loss: 2.5022818437311, discriminator loss: 0.6392960507214359\n",
            "Epoch 61/300, Batch 200/937, generator loss: 2.5004829831912123, discriminator loss: 0.6397708466340111\n",
            "Epoch 61/300, Batch 300/937, generator loss: 2.4987348841738677, discriminator loss: 0.6402820591310756\n",
            "Epoch 61/300, Batch 400/937, generator loss: 2.496883274682174, discriminator loss: 0.6408050488770565\n",
            "Epoch 61/300, Batch 500/937, generator loss: 2.4951991739318227, discriminator loss: 0.6413142735492534\n",
            "Epoch 61/300, Batch 600/937, generator loss: 2.49338092775968, discriminator loss: 0.641808613082177\n",
            "Epoch 61/300, Batch 700/937, generator loss: 2.49159919632314, discriminator loss: 0.6423099413866977\n",
            "Epoch 61/300, Batch 800/937, generator loss: 2.4897777695832133, discriminator loss: 0.6428385674578091\n",
            "Epoch 61/300, Batch 900/937, generator loss: 2.4880827895285993, discriminator loss: 0.6433174855140359\n",
            "Saving results...\n",
            "Epoch 62/300, Batch 0/937, generator loss: 2.4874622330658203, discriminator loss: 0.643490267360628\n",
            "Epoch 62/300, Batch 100/937, generator loss: 2.485689411392551, discriminator loss: 0.6440172972712118\n",
            "Epoch 62/300, Batch 200/937, generator loss: 2.483900950297308, discriminator loss: 0.6445538215100821\n",
            "Epoch 62/300, Batch 300/937, generator loss: 2.4823493845465454, discriminator loss: 0.6450115918822256\n",
            "Epoch 62/300, Batch 400/937, generator loss: 2.480616239358527, discriminator loss: 0.6455485732257442\n",
            "Epoch 62/300, Batch 500/937, generator loss: 2.478923762765229, discriminator loss: 0.6460521834525692\n",
            "Epoch 62/300, Batch 600/937, generator loss: 2.477126622636676, discriminator loss: 0.6466073314912458\n",
            "Epoch 62/300, Batch 700/937, generator loss: 2.4753920016417306, discriminator loss: 0.64710816424896\n",
            "Epoch 62/300, Batch 800/937, generator loss: 2.473617578406965, discriminator loss: 0.647670732051835\n",
            "Epoch 62/300, Batch 900/937, generator loss: 2.471936077695346, discriminator loss: 0.6481666648228276\n",
            "Saving results...\n",
            "Epoch 63/300, Batch 0/937, generator loss: 2.4713381117058084, discriminator loss: 0.6483534765761103\n",
            "Epoch 63/300, Batch 100/937, generator loss: 2.469597289736487, discriminator loss: 0.6488734359690368\n",
            "Epoch 63/300, Batch 200/937, generator loss: 2.468042427423369, discriminator loss: 0.6493501941583395\n",
            "Epoch 63/300, Batch 300/937, generator loss: 2.4663251849558305, discriminator loss: 0.6498494894221613\n",
            "Epoch 63/300, Batch 400/937, generator loss: 2.4646168151354626, discriminator loss: 0.6503507706704869\n",
            "Epoch 63/300, Batch 500/937, generator loss: 2.462922200453439, discriminator loss: 0.6508718046622476\n",
            "Epoch 63/300, Batch 600/937, generator loss: 2.4611853011190363, discriminator loss: 0.6513610866454537\n",
            "Epoch 63/300, Batch 700/937, generator loss: 2.4594478324811058, discriminator loss: 0.6519126905196838\n",
            "Epoch 63/300, Batch 800/937, generator loss: 2.4576988780567444, discriminator loss: 0.6523957201880533\n",
            "Epoch 63/300, Batch 900/937, generator loss: 2.4558850720515837, discriminator loss: 0.652864684707144\n",
            "Saving results...\n",
            "Epoch 64/300, Batch 0/937, generator loss: 2.455231696355834, discriminator loss: 0.6530819711055788\n",
            "Epoch 64/300, Batch 100/937, generator loss: 2.453406618713656, discriminator loss: 0.6535925307909751\n",
            "Epoch 64/300, Batch 200/937, generator loss: 2.4517070741181035, discriminator loss: 0.654103163708958\n",
            "Epoch 64/300, Batch 300/937, generator loss: 2.45007683714697, discriminator loss: 0.6545733769798324\n",
            "Epoch 64/300, Batch 400/937, generator loss: 2.448440667372202, discriminator loss: 0.655066944122508\n",
            "Epoch 64/300, Batch 500/937, generator loss: 2.44673516052594, discriminator loss: 0.6556010116375615\n",
            "Epoch 64/300, Batch 600/937, generator loss: 2.445139336989274, discriminator loss: 0.6560535712989357\n",
            "Epoch 64/300, Batch 700/937, generator loss: 2.4436133083245717, discriminator loss: 0.6564972476270212\n",
            "Epoch 64/300, Batch 800/937, generator loss: 2.441958112978873, discriminator loss: 0.6570467175477371\n",
            "Epoch 64/300, Batch 900/937, generator loss: 2.4404370387731427, discriminator loss: 0.6574762023264551\n",
            "Saving results...\n",
            "Epoch 65/300, Batch 0/937, generator loss: 2.4398224635274604, discriminator loss: 0.657669848238792\n",
            "Epoch 65/300, Batch 100/937, generator loss: 2.4381910046116233, discriminator loss: 0.6581914268029313\n",
            "Epoch 65/300, Batch 200/937, generator loss: 2.4364962005710438, discriminator loss: 0.6586472234203941\n",
            "Epoch 65/300, Batch 300/937, generator loss: 2.4349649658593595, discriminator loss: 0.6591373554074569\n",
            "Epoch 65/300, Batch 400/937, generator loss: 2.4332948675314077, discriminator loss: 0.659598967026965\n",
            "Epoch 65/300, Batch 500/937, generator loss: 2.431760188878268, discriminator loss: 0.6600579988799009\n",
            "Epoch 65/300, Batch 600/937, generator loss: 2.4301358678969036, discriminator loss: 0.6605662417861267\n",
            "Epoch 65/300, Batch 700/937, generator loss: 2.428595320022567, discriminator loss: 0.660973515746497\n",
            "Epoch 65/300, Batch 800/937, generator loss: 2.4270360138662954, discriminator loss: 0.661480906512217\n",
            "Epoch 65/300, Batch 900/937, generator loss: 2.425368292542587, discriminator loss: 0.6619953616493532\n",
            "Saving results...\n",
            "Epoch 66/300, Batch 0/937, generator loss: 2.4248375446734385, discriminator loss: 0.6621256509757745\n",
            "Epoch 66/300, Batch 100/937, generator loss: 2.4232432564481963, discriminator loss: 0.6625898169507595\n",
            "Epoch 66/300, Batch 200/937, generator loss: 2.4216623642144373, discriminator loss: 0.6630878233611598\n",
            "Epoch 66/300, Batch 300/937, generator loss: 2.42006881200141, discriminator loss: 0.6635534543205773\n",
            "Epoch 66/300, Batch 400/937, generator loss: 2.418522594833777, discriminator loss: 0.6640244732952734\n",
            "Epoch 66/300, Batch 500/937, generator loss: 2.417021821368493, discriminator loss: 0.6644649526137405\n",
            "Epoch 66/300, Batch 600/937, generator loss: 2.4155061529571697, discriminator loss: 0.6649466203861095\n",
            "Epoch 66/300, Batch 700/937, generator loss: 2.4139969505774337, discriminator loss: 0.6653897624687457\n",
            "Epoch 66/300, Batch 800/937, generator loss: 2.412402094518986, discriminator loss: 0.6658736495444892\n",
            "Epoch 66/300, Batch 900/937, generator loss: 2.410814709826636, discriminator loss: 0.6663538879685917\n",
            "Saving results...\n",
            "Epoch 67/300, Batch 0/937, generator loss: 2.4102649123017748, discriminator loss: 0.6665141751310293\n",
            "Epoch 67/300, Batch 100/937, generator loss: 2.4087080091730937, discriminator loss: 0.666973223969928\n",
            "Epoch 67/300, Batch 200/937, generator loss: 2.407190550484707, discriminator loss: 0.6674322339793961\n",
            "Epoch 67/300, Batch 300/937, generator loss: 2.405656476824361, discriminator loss: 0.6679170591753159\n",
            "Epoch 67/300, Batch 400/937, generator loss: 2.4039976672305077, discriminator loss: 0.6684211985503205\n",
            "Epoch 67/300, Batch 500/937, generator loss: 2.40243102477489, discriminator loss: 0.6688799194139152\n",
            "Epoch 67/300, Batch 600/937, generator loss: 2.4009170185413673, discriminator loss: 0.6692870648929884\n",
            "Epoch 67/300, Batch 700/937, generator loss: 2.3994161639280414, discriminator loss: 0.6697496681612057\n",
            "Epoch 67/300, Batch 800/937, generator loss: 2.3978760554060017, discriminator loss: 0.6701847482478518\n",
            "Epoch 67/300, Batch 900/937, generator loss: 2.396256193997872, discriminator loss: 0.6706798746128889\n",
            "Saving results...\n",
            "Epoch 68/300, Batch 0/937, generator loss: 2.3957047123490436, discriminator loss: 0.6708326353879852\n",
            "Epoch 68/300, Batch 100/937, generator loss: 2.3941620454125148, discriminator loss: 0.671303317441845\n",
            "Epoch 68/300, Batch 200/937, generator loss: 2.392622635592145, discriminator loss: 0.6717760048479465\n",
            "Epoch 68/300, Batch 300/937, generator loss: 2.3911419785406154, discriminator loss: 0.672202127031726\n",
            "Epoch 68/300, Batch 400/937, generator loss: 2.3897160367194332, discriminator loss: 0.6726606036110294\n",
            "Epoch 68/300, Batch 500/937, generator loss: 2.388196511829011, discriminator loss: 0.6731219121046591\n",
            "Epoch 68/300, Batch 600/937, generator loss: 2.386662980579612, discriminator loss: 0.6735826906265158\n",
            "Epoch 68/300, Batch 700/937, generator loss: 2.3851892655554257, discriminator loss: 0.6739867643896613\n",
            "Epoch 68/300, Batch 800/937, generator loss: 2.3837088044065147, discriminator loss: 0.6744241588504674\n",
            "Epoch 68/300, Batch 900/937, generator loss: 2.382263280550344, discriminator loss: 0.6748665371935653\n",
            "Saving results...\n",
            "Epoch 69/300, Batch 0/937, generator loss: 2.381721502629708, discriminator loss: 0.6750197889082843\n",
            "Epoch 69/300, Batch 100/937, generator loss: 2.3802618140139993, discriminator loss: 0.6754363686579029\n",
            "Epoch 69/300, Batch 200/937, generator loss: 2.3789036787838884, discriminator loss: 0.6758909300923197\n",
            "Epoch 69/300, Batch 300/937, generator loss: 2.3775122833027313, discriminator loss: 0.6763067612132625\n",
            "Epoch 69/300, Batch 400/937, generator loss: 2.376178233317556, discriminator loss: 0.6767025181545657\n",
            "Epoch 69/300, Batch 500/937, generator loss: 2.3747803976084456, discriminator loss: 0.677129621038096\n",
            "Epoch 69/300, Batch 600/937, generator loss: 2.3735285955592413, discriminator loss: 0.6775908711019454\n",
            "Epoch 69/300, Batch 700/937, generator loss: 2.3721017242909848, discriminator loss: 0.6780217238381387\n",
            "Epoch 69/300, Batch 800/937, generator loss: 2.3706257046498367, discriminator loss: 0.6784870326805862\n",
            "Epoch 69/300, Batch 900/937, generator loss: 2.3692023769013306, discriminator loss: 0.6789020224712394\n",
            "Saving results...\n",
            "Epoch 70/300, Batch 0/937, generator loss: 2.3686619248574634, discriminator loss: 0.6790593697303362\n",
            "Epoch 70/300, Batch 100/937, generator loss: 2.3673405534802883, discriminator loss: 0.6794447320370959\n",
            "Epoch 70/300, Batch 200/937, generator loss: 2.3659343510910107, discriminator loss: 0.6798959677234538\n",
            "Epoch 70/300, Batch 300/937, generator loss: 2.3645414999488006, discriminator loss: 0.6803470078276643\n",
            "Epoch 70/300, Batch 400/937, generator loss: 2.3631030800145982, discriminator loss: 0.6807853352066834\n",
            "Epoch 70/300, Batch 500/937, generator loss: 2.3616860807812277, discriminator loss: 0.6811660563008199\n",
            "Epoch 70/300, Batch 600/937, generator loss: 2.3602820847916974, discriminator loss: 0.6816147676147774\n",
            "Epoch 70/300, Batch 700/937, generator loss: 2.358887418100016, discriminator loss: 0.6820252276254023\n",
            "Epoch 70/300, Batch 800/937, generator loss: 2.3575366880952924, discriminator loss: 0.6824006017711777\n",
            "Epoch 70/300, Batch 900/937, generator loss: 2.3560387128524396, discriminator loss: 0.6828701760603014\n",
            "Saving results...\n",
            "Epoch 71/300, Batch 0/937, generator loss: 2.3554889085187884, discriminator loss: 0.6830738630524767\n",
            "Epoch 71/300, Batch 100/937, generator loss: 2.3541144399165734, discriminator loss: 0.6834504446207658\n",
            "Epoch 71/300, Batch 200/937, generator loss: 2.3526295858215707, discriminator loss: 0.6839060647554491\n",
            "Epoch 71/300, Batch 300/937, generator loss: 2.3512151071643133, discriminator loss: 0.6843173613487165\n",
            "Epoch 71/300, Batch 400/937, generator loss: 2.349819167479499, discriminator loss: 0.6847185767450538\n",
            "Epoch 71/300, Batch 500/937, generator loss: 2.348412604447075, discriminator loss: 0.6851759490543621\n",
            "Epoch 71/300, Batch 600/937, generator loss: 2.3469629470508013, discriminator loss: 0.6856033774546745\n",
            "Epoch 71/300, Batch 700/937, generator loss: 2.3456579825479102, discriminator loss: 0.6859607329775969\n",
            "Epoch 71/300, Batch 800/937, generator loss: 2.3442506527620237, discriminator loss: 0.6863842234489094\n",
            "Epoch 71/300, Batch 900/937, generator loss: 2.342887509874781, discriminator loss: 0.6867850225834495\n",
            "Saving results...\n",
            "Epoch 72/300, Batch 0/937, generator loss: 2.3423747223283895, discriminator loss: 0.6869246331767934\n",
            "Epoch 72/300, Batch 100/937, generator loss: 2.3411200157800134, discriminator loss: 0.6873177699167423\n",
            "Epoch 72/300, Batch 200/937, generator loss: 2.339779605307354, discriminator loss: 0.6877239067560816\n",
            "Epoch 72/300, Batch 300/937, generator loss: 2.33835312789774, discriminator loss: 0.6881114723184913\n",
            "Epoch 72/300, Batch 400/937, generator loss: 2.337084414110006, discriminator loss: 0.6884505926131113\n",
            "Epoch 72/300, Batch 500/937, generator loss: 2.335742937177606, discriminator loss: 0.6888650488000143\n",
            "Epoch 72/300, Batch 600/937, generator loss: 2.3344247964060227, discriminator loss: 0.6892804059175428\n",
            "Epoch 72/300, Batch 700/937, generator loss: 2.3331494220938467, discriminator loss: 0.6896430939118983\n",
            "Epoch 72/300, Batch 800/937, generator loss: 2.3318459083385576, discriminator loss: 0.6900101561660008\n",
            "Epoch 72/300, Batch 900/937, generator loss: 2.330546203456835, discriminator loss: 0.6904223208388893\n",
            "Saving results...\n",
            "Epoch 73/300, Batch 0/937, generator loss: 2.3300238712318935, discriminator loss: 0.6905806357919088\n",
            "Epoch 73/300, Batch 100/937, generator loss: 2.328663386516107, discriminator loss: 0.6909469081219581\n",
            "Epoch 73/300, Batch 200/937, generator loss: 2.3273755814246417, discriminator loss: 0.6913451680393943\n",
            "Epoch 73/300, Batch 300/937, generator loss: 2.325968231748054, discriminator loss: 0.6917775155934826\n",
            "Epoch 73/300, Batch 400/937, generator loss: 2.3246964131124885, discriminator loss: 0.692163231790764\n",
            "Epoch 73/300, Batch 500/937, generator loss: 2.323341696043533, discriminator loss: 0.6925710443829989\n",
            "Epoch 73/300, Batch 600/937, generator loss: 2.322010334301802, discriminator loss: 0.6929868772793935\n",
            "Epoch 73/300, Batch 700/937, generator loss: 2.320689039705997, discriminator loss: 0.6933572100291759\n",
            "Epoch 73/300, Batch 800/937, generator loss: 2.319402113086154, discriminator loss: 0.693728692423284\n",
            "Epoch 73/300, Batch 900/937, generator loss: 2.3180105772478674, discriminator loss: 0.6941707162900094\n",
            "Saving results...\n",
            "Epoch 74/300, Batch 0/937, generator loss: 2.3175486025602696, discriminator loss: 0.6943245125444584\n",
            "Epoch 74/300, Batch 100/937, generator loss: 2.316248539758873, discriminator loss: 0.694711300377098\n",
            "Epoch 74/300, Batch 200/937, generator loss: 2.3149261571303317, discriminator loss: 0.6950684924240974\n",
            "Epoch 74/300, Batch 300/937, generator loss: 2.313603876774181, discriminator loss: 0.6954741751322236\n",
            "Epoch 74/300, Batch 400/937, generator loss: 2.312284955464525, discriminator loss: 0.695863067096548\n",
            "Epoch 74/300, Batch 500/937, generator loss: 2.310958003990634, discriminator loss: 0.6962888251478165\n",
            "Epoch 74/300, Batch 600/937, generator loss: 2.3096327968095696, discriminator loss: 0.6966835539094233\n",
            "Epoch 74/300, Batch 700/937, generator loss: 2.3083759928507384, discriminator loss: 0.6970661697867296\n",
            "Epoch 74/300, Batch 800/937, generator loss: 2.3070437935995303, discriminator loss: 0.6974553512289426\n",
            "Epoch 74/300, Batch 900/937, generator loss: 2.305870905046075, discriminator loss: 0.6978264751239522\n",
            "Saving results...\n",
            "Epoch 75/300, Batch 0/937, generator loss: 2.305353558400562, discriminator loss: 0.6979708278549471\n",
            "Epoch 75/300, Batch 100/937, generator loss: 2.304063976736716, discriminator loss: 0.6983450134536231\n",
            "Epoch 75/300, Batch 200/937, generator loss: 2.302833864920472, discriminator loss: 0.6986867667070799\n",
            "Epoch 75/300, Batch 300/937, generator loss: 2.3015905133999817, discriminator loss: 0.699059227983989\n",
            "Epoch 75/300, Batch 400/937, generator loss: 2.3003773204394045, discriminator loss: 0.6994379512993931\n",
            "Epoch 75/300, Batch 500/937, generator loss: 2.2992330238625627, discriminator loss: 0.6997805613318251\n",
            "Epoch 75/300, Batch 600/937, generator loss: 2.298003702843638, discriminator loss: 0.7001687195277139\n",
            "Epoch 75/300, Batch 700/937, generator loss: 2.296842558491669, discriminator loss: 0.7005484593260405\n",
            "Epoch 75/300, Batch 800/937, generator loss: 2.295545049142949, discriminator loss: 0.7009333687930305\n",
            "Epoch 75/300, Batch 900/937, generator loss: 2.2943740088894176, discriminator loss: 0.7012829359290444\n",
            "Saving results...\n",
            "Epoch 76/300, Batch 0/937, generator loss: 2.293858684675461, discriminator loss: 0.7014478548141105\n",
            "Epoch 76/300, Batch 100/937, generator loss: 2.2925873130333, discriminator loss: 0.701810940274124\n",
            "Epoch 76/300, Batch 200/937, generator loss: 2.291421295980958, discriminator loss: 0.7021644327001795\n",
            "Epoch 76/300, Batch 300/937, generator loss: 2.290162324659519, discriminator loss: 0.7025233869790954\n",
            "Epoch 76/300, Batch 400/937, generator loss: 2.288988046016521, discriminator loss: 0.7028909922748937\n",
            "Epoch 76/300, Batch 500/937, generator loss: 2.2875913340625513, discriminator loss: 0.7033413121116749\n",
            "Epoch 76/300, Batch 600/937, generator loss: 2.286446976285361, discriminator loss: 0.7036560829455202\n",
            "Epoch 76/300, Batch 700/937, generator loss: 2.285189097786292, discriminator loss: 0.7040626829848255\n",
            "Epoch 76/300, Batch 800/937, generator loss: 2.2839694758868494, discriminator loss: 0.7044652456043744\n",
            "Epoch 76/300, Batch 900/937, generator loss: 2.2827829384409344, discriminator loss: 0.7048649014616192\n",
            "Saving results...\n",
            "Epoch 77/300, Batch 0/937, generator loss: 2.2823202961761755, discriminator loss: 0.7050084154847919\n",
            "Epoch 77/300, Batch 100/937, generator loss: 2.2811651589952953, discriminator loss: 0.7053752276282822\n",
            "Epoch 77/300, Batch 200/937, generator loss: 2.2799809147558627, discriminator loss: 0.7056969599350618\n",
            "Epoch 77/300, Batch 300/937, generator loss: 2.278814468238994, discriminator loss: 0.706063737559516\n",
            "Epoch 77/300, Batch 400/937, generator loss: 2.277633102211272, discriminator loss: 0.7064396398503726\n",
            "Epoch 77/300, Batch 500/937, generator loss: 2.276398377735206, discriminator loss: 0.7067996698068243\n",
            "Epoch 77/300, Batch 600/937, generator loss: 2.275138701338129, discriminator loss: 0.7071553115185184\n",
            "Epoch 77/300, Batch 700/937, generator loss: 2.273986147702681, discriminator loss: 0.707482563827082\n",
            "Epoch 77/300, Batch 800/937, generator loss: 2.2727468424247013, discriminator loss: 0.7078869856649919\n",
            "Epoch 77/300, Batch 900/937, generator loss: 2.2715701987619354, discriminator loss: 0.7082635392492393\n",
            "Saving results...\n",
            "Epoch 78/300, Batch 0/937, generator loss: 2.271076839601274, discriminator loss: 0.708417640582453\n",
            "Epoch 78/300, Batch 100/937, generator loss: 2.26985820570709, discriminator loss: 0.7087622622313878\n",
            "Epoch 78/300, Batch 200/937, generator loss: 2.2686753546418816, discriminator loss: 0.7090948485565408\n",
            "Epoch 78/300, Batch 300/937, generator loss: 2.2674841192892496, discriminator loss: 0.7094500396587343\n",
            "Epoch 78/300, Batch 400/937, generator loss: 2.266257041003063, discriminator loss: 0.7098257762796402\n",
            "Epoch 78/300, Batch 500/937, generator loss: 2.2650635872341836, discriminator loss: 0.7101948353376961\n",
            "Epoch 78/300, Batch 600/937, generator loss: 2.2638669563827816, discriminator loss: 0.7105460401691124\n",
            "Epoch 78/300, Batch 700/937, generator loss: 2.262630409969475, discriminator loss: 0.7109241721572324\n",
            "Epoch 78/300, Batch 800/937, generator loss: 2.2614541479524046, discriminator loss: 0.711297951202173\n",
            "Epoch 78/300, Batch 900/937, generator loss: 2.260269643227228, discriminator loss: 0.7116463696696261\n",
            "Saving results...\n",
            "Epoch 79/300, Batch 0/937, generator loss: 2.259815814011995, discriminator loss: 0.7117779000552457\n",
            "Epoch 79/300, Batch 100/937, generator loss: 2.258705350627825, discriminator loss: 0.7121421684901055\n",
            "Epoch 79/300, Batch 200/937, generator loss: 2.2575102122253603, discriminator loss: 0.7124948989468245\n",
            "Epoch 79/300, Batch 300/937, generator loss: 2.2563852474039705, discriminator loss: 0.7128387300861895\n",
            "Epoch 79/300, Batch 400/937, generator loss: 2.2552724555126766, discriminator loss: 0.7131930354597792\n",
            "Epoch 79/300, Batch 500/937, generator loss: 2.2540340714217013, discriminator loss: 0.7135624279439094\n",
            "Epoch 79/300, Batch 600/937, generator loss: 2.252880544837015, discriminator loss: 0.7139082847400304\n",
            "Epoch 79/300, Batch 700/937, generator loss: 2.251671660743946, discriminator loss: 0.7142508363963532\n",
            "Epoch 79/300, Batch 800/937, generator loss: 2.2504713268488357, discriminator loss: 0.7146413424105571\n",
            "Epoch 79/300, Batch 900/937, generator loss: 2.249281594686568, discriminator loss: 0.7150010880321307\n",
            "Saving results...\n",
            "Epoch 80/300, Batch 0/937, generator loss: 2.2488790454535756, discriminator loss: 0.7151167764726004\n",
            "Epoch 80/300, Batch 100/937, generator loss: 2.2477355543372903, discriminator loss: 0.7154803188089555\n",
            "Epoch 80/300, Batch 200/937, generator loss: 2.2466340074193614, discriminator loss: 0.7158042860151509\n",
            "Epoch 80/300, Batch 300/937, generator loss: 2.2454717062816907, discriminator loss: 0.716162222060891\n",
            "Epoch 80/300, Batch 400/937, generator loss: 2.244378047158635, discriminator loss: 0.716514474386366\n",
            "Epoch 80/300, Batch 500/937, generator loss: 2.2432590747776557, discriminator loss: 0.7168574128878287\n",
            "Epoch 80/300, Batch 600/937, generator loss: 2.2420727537921223, discriminator loss: 0.7172114141752307\n",
            "Epoch 80/300, Batch 700/937, generator loss: 2.240947278763278, discriminator loss: 0.717548280311528\n",
            "Epoch 80/300, Batch 800/937, generator loss: 2.239817856621197, discriminator loss: 0.7178727924694734\n",
            "Epoch 80/300, Batch 900/937, generator loss: 2.238682215232727, discriminator loss: 0.7182240408371647\n",
            "Saving results...\n",
            "Epoch 81/300, Batch 0/937, generator loss: 2.238238729982402, discriminator loss: 0.7183424735502617\n",
            "Epoch 81/300, Batch 100/937, generator loss: 2.2371570731894637, discriminator loss: 0.7187199214612432\n",
            "Epoch 81/300, Batch 200/937, generator loss: 2.2360508245984314, discriminator loss: 0.7190463741726122\n",
            "Epoch 81/300, Batch 300/937, generator loss: 2.2350409762474923, discriminator loss: 0.7193804125879977\n",
            "Epoch 81/300, Batch 400/937, generator loss: 2.2339246570274973, discriminator loss: 0.7197431708638229\n",
            "Epoch 81/300, Batch 500/937, generator loss: 2.232852257346011, discriminator loss: 0.720047287853801\n",
            "Epoch 81/300, Batch 600/937, generator loss: 2.231768069691077, discriminator loss: 0.7203946215715616\n",
            "Epoch 81/300, Batch 700/937, generator loss: 2.2306943345889745, discriminator loss: 0.7207560735918834\n",
            "Epoch 81/300, Batch 800/937, generator loss: 2.229554194273043, discriminator loss: 0.7210715538837779\n",
            "Epoch 81/300, Batch 900/937, generator loss: 2.2284061484392548, discriminator loss: 0.7214240124654564\n",
            "Saving results...\n",
            "Epoch 82/300, Batch 0/937, generator loss: 2.2280208125183947, discriminator loss: 0.7215403140622962\n",
            "Epoch 82/300, Batch 100/937, generator loss: 2.2269189386438453, discriminator loss: 0.7218870619715884\n",
            "Epoch 82/300, Batch 200/937, generator loss: 2.2258267601452295, discriminator loss: 0.7222287668111439\n",
            "Epoch 82/300, Batch 300/937, generator loss: 2.2247753124177714, discriminator loss: 0.7225911025787857\n",
            "Epoch 82/300, Batch 400/937, generator loss: 2.2236824123560126, discriminator loss: 0.7229315573882692\n",
            "Epoch 82/300, Batch 500/937, generator loss: 2.2225822799398616, discriminator loss: 0.7232613033400391\n",
            "Epoch 82/300, Batch 600/937, generator loss: 2.2214489977578533, discriminator loss: 0.7235771320219028\n",
            "Epoch 82/300, Batch 700/937, generator loss: 2.2203405748662384, discriminator loss: 0.7239479234876336\n",
            "Epoch 82/300, Batch 800/937, generator loss: 2.2192286753881736, discriminator loss: 0.7242743588468177\n",
            "Epoch 82/300, Batch 900/937, generator loss: 2.2181071785958224, discriminator loss: 0.7246144096648309\n",
            "Saving results...\n",
            "Epoch 83/300, Batch 0/937, generator loss: 2.217686081430932, discriminator loss: 0.7247385323129643\n",
            "Epoch 83/300, Batch 100/937, generator loss: 2.2165245145041887, discriminator loss: 0.7250882634356884\n",
            "Epoch 83/300, Batch 200/937, generator loss: 2.2155121562074918, discriminator loss: 0.7254373536915855\n",
            "Epoch 83/300, Batch 300/937, generator loss: 2.2144580214699587, discriminator loss: 0.7257594571028293\n",
            "Epoch 83/300, Batch 400/937, generator loss: 2.2133438879539087, discriminator loss: 0.7261133587512726\n",
            "Epoch 83/300, Batch 500/937, generator loss: 2.212258905262206, discriminator loss: 0.7264264132793252\n",
            "Epoch 83/300, Batch 600/937, generator loss: 2.211140395096078, discriminator loss: 0.7267520166676856\n",
            "Epoch 83/300, Batch 700/937, generator loss: 2.210006269105732, discriminator loss: 0.7271055405658888\n",
            "Epoch 83/300, Batch 800/937, generator loss: 2.2089119725199016, discriminator loss: 0.7274645261107306\n",
            "Epoch 83/300, Batch 900/937, generator loss: 2.2077678960935723, discriminator loss: 0.7278268484954448\n",
            "Saving results...\n",
            "Epoch 84/300, Batch 0/937, generator loss: 2.2074075610778006, discriminator loss: 0.7279185285380766\n",
            "Epoch 84/300, Batch 100/937, generator loss: 2.2062910434024694, discriminator loss: 0.7282495555278954\n",
            "Epoch 84/300, Batch 200/937, generator loss: 2.2052779524234682, discriminator loss: 0.7285457623880526\n",
            "Epoch 84/300, Batch 300/937, generator loss: 2.204227867256705, discriminator loss: 0.7289033609646255\n",
            "Epoch 84/300, Batch 400/937, generator loss: 2.2031699284726747, discriminator loss: 0.7292025423136272\n",
            "Epoch 84/300, Batch 500/937, generator loss: 2.202127066708335, discriminator loss: 0.7295136250296186\n",
            "Epoch 84/300, Batch 600/937, generator loss: 2.2011061058510513, discriminator loss: 0.7298266495667813\n",
            "Epoch 84/300, Batch 700/937, generator loss: 2.200088002639128, discriminator loss: 0.7301450993357992\n",
            "Epoch 84/300, Batch 800/937, generator loss: 2.1989886501944342, discriminator loss: 0.7304763832519021\n",
            "Epoch 84/300, Batch 900/937, generator loss: 2.1979263125124335, discriminator loss: 0.7308115106367112\n",
            "Saving results...\n",
            "Epoch 85/300, Batch 0/937, generator loss: 2.197558497721309, discriminator loss: 0.7309075920143961\n",
            "Epoch 85/300, Batch 100/937, generator loss: 2.196528677711894, discriminator loss: 0.7312484267585491\n",
            "Epoch 85/300, Batch 200/937, generator loss: 2.195569968879293, discriminator loss: 0.7315331537195232\n",
            "Epoch 85/300, Batch 300/937, generator loss: 2.1946097420339297, discriminator loss: 0.7318412505025559\n",
            "Epoch 85/300, Batch 400/937, generator loss: 2.1935461176894364, discriminator loss: 0.7321676376556328\n",
            "Epoch 85/300, Batch 500/937, generator loss: 2.192525613746885, discriminator loss: 0.7324935655827186\n",
            "Epoch 85/300, Batch 600/937, generator loss: 2.191525704992515, discriminator loss: 0.7328195855509108\n",
            "Epoch 85/300, Batch 700/937, generator loss: 2.1904880392565667, discriminator loss: 0.7331402042856868\n",
            "Epoch 85/300, Batch 800/937, generator loss: 2.1895292884035387, discriminator loss: 0.733491954465115\n",
            "Epoch 85/300, Batch 900/937, generator loss: 2.1885563865290303, discriminator loss: 0.733815414676427\n",
            "Saving results...\n",
            "Epoch 86/300, Batch 0/937, generator loss: 2.1881706297908723, discriminator loss: 0.7339507127668992\n",
            "Epoch 86/300, Batch 100/937, generator loss: 2.1871635676118815, discriminator loss: 0.7342709098367933\n",
            "Epoch 86/300, Batch 200/937, generator loss: 2.186186995396703, discriminator loss: 0.7345879300195686\n",
            "Epoch 86/300, Batch 300/937, generator loss: 2.1851518650064836, discriminator loss: 0.7349031129888438\n",
            "Epoch 86/300, Batch 400/937, generator loss: 2.1840999465523465, discriminator loss: 0.7352378907762074\n",
            "Epoch 86/300, Batch 500/937, generator loss: 2.1831018691441773, discriminator loss: 0.7355456296009262\n",
            "Epoch 86/300, Batch 600/937, generator loss: 2.182116817960647, discriminator loss: 0.7358553610479523\n",
            "Epoch 86/300, Batch 700/937, generator loss: 2.1810807150401343, discriminator loss: 0.7361702160868089\n",
            "Epoch 86/300, Batch 800/937, generator loss: 2.18009817419275, discriminator loss: 0.7365063550181996\n",
            "Epoch 86/300, Batch 900/937, generator loss: 2.179130953292047, discriminator loss: 0.7368057976593968\n",
            "Saving results...\n",
            "Epoch 87/300, Batch 0/937, generator loss: 2.178792424048859, discriminator loss: 0.7369067786767771\n",
            "Epoch 87/300, Batch 100/937, generator loss: 2.1777569313116207, discriminator loss: 0.7372280532440786\n",
            "Epoch 87/300, Batch 200/937, generator loss: 2.1767274378599692, discriminator loss: 0.7375601834899733\n",
            "Epoch 87/300, Batch 300/937, generator loss: 2.1757122137623175, discriminator loss: 0.7378493782496196\n",
            "Epoch 87/300, Batch 400/937, generator loss: 2.1746971380525792, discriminator loss: 0.7381686313721729\n",
            "Epoch 87/300, Batch 500/937, generator loss: 2.1737213577818095, discriminator loss: 0.7385009987659932\n",
            "Epoch 87/300, Batch 600/937, generator loss: 2.172656793681895, discriminator loss: 0.73882157330252\n",
            "Epoch 87/300, Batch 700/937, generator loss: 2.1717126804361144, discriminator loss: 0.7391764847608385\n",
            "Epoch 87/300, Batch 800/937, generator loss: 2.1707154503984576, discriminator loss: 0.7394829153938561\n",
            "Epoch 87/300, Batch 900/937, generator loss: 2.169769995136187, discriminator loss: 0.7397930296079622\n",
            "Saving results...\n",
            "Epoch 88/300, Batch 0/937, generator loss: 2.1693431307013262, discriminator loss: 0.739926483335789\n",
            "Epoch 88/300, Batch 100/937, generator loss: 2.168398497220364, discriminator loss: 0.7402193415071379\n",
            "Epoch 88/300, Batch 200/937, generator loss: 2.1673859782667577, discriminator loss: 0.7405023946773133\n",
            "Epoch 88/300, Batch 300/937, generator loss: 2.166400032849431, discriminator loss: 0.7408078278248451\n",
            "Epoch 88/300, Batch 400/937, generator loss: 2.165448308142095, discriminator loss: 0.7410669492542515\n",
            "Epoch 88/300, Batch 500/937, generator loss: 2.1644497182535147, discriminator loss: 0.7413714343562099\n",
            "Epoch 88/300, Batch 600/937, generator loss: 2.1634606253379287, discriminator loss: 0.7416775438643518\n",
            "Epoch 88/300, Batch 700/937, generator loss: 2.1625257182325126, discriminator loss: 0.7419687735639021\n",
            "Epoch 88/300, Batch 800/937, generator loss: 2.1615516466320495, discriminator loss: 0.7422868880483243\n",
            "Epoch 88/300, Batch 900/937, generator loss: 2.16061583482663, discriminator loss: 0.7425668461084644\n",
            "Saving results...\n",
            "Epoch 89/300, Batch 0/937, generator loss: 2.1602623231989, discriminator loss: 0.7427026954598943\n",
            "Epoch 89/300, Batch 100/937, generator loss: 2.1593399407789544, discriminator loss: 0.7429758306652222\n",
            "Epoch 89/300, Batch 200/937, generator loss: 2.1584381412372555, discriminator loss: 0.7432501483578047\n",
            "Epoch 89/300, Batch 300/937, generator loss: 2.1574789443463183, discriminator loss: 0.74353586127538\n",
            "Epoch 89/300, Batch 400/937, generator loss: 2.1565454155117765, discriminator loss: 0.743866667637285\n",
            "Epoch 89/300, Batch 500/937, generator loss: 2.1556562035050293, discriminator loss: 0.7441555230432714\n",
            "Epoch 89/300, Batch 600/937, generator loss: 2.154708917654392, discriminator loss: 0.7444409334989838\n",
            "Epoch 89/300, Batch 700/937, generator loss: 2.1537541003931144, discriminator loss: 0.7447463585722587\n",
            "Epoch 89/300, Batch 800/937, generator loss: 2.1528110552998534, discriminator loss: 0.7450432656052549\n",
            "Epoch 89/300, Batch 900/937, generator loss: 2.1518831242515906, discriminator loss: 0.7453275013368637\n",
            "Saving results...\n",
            "Epoch 90/300, Batch 0/937, generator loss: 2.1515210653744896, discriminator loss: 0.7454581843789736\n",
            "Epoch 90/300, Batch 100/937, generator loss: 2.1505880519927576, discriminator loss: 0.7457294951203922\n",
            "Epoch 90/300, Batch 200/937, generator loss: 2.1496561951738298, discriminator loss: 0.7459984294360378\n",
            "Epoch 90/300, Batch 300/937, generator loss: 2.1487375624301177, discriminator loss: 0.7463155851623624\n",
            "Epoch 90/300, Batch 400/937, generator loss: 2.1477936302275764, discriminator loss: 0.7466082861730875\n",
            "Epoch 90/300, Batch 500/937, generator loss: 2.1468069222135275, discriminator loss: 0.7468957666699095\n",
            "Epoch 90/300, Batch 600/937, generator loss: 2.1458606172895918, discriminator loss: 0.7471745292816123\n",
            "Epoch 90/300, Batch 700/937, generator loss: 2.1448949506223896, discriminator loss: 0.7474882582296769\n",
            "Epoch 90/300, Batch 800/937, generator loss: 2.1439515688560187, discriminator loss: 0.7477877959422616\n",
            "Epoch 90/300, Batch 900/937, generator loss: 2.1429415009995814, discriminator loss: 0.7480968627404938\n",
            "Saving results...\n",
            "Epoch 91/300, Batch 0/937, generator loss: 2.1426027998941257, discriminator loss: 0.7482033491036081\n",
            "Epoch 91/300, Batch 100/937, generator loss: 2.141626321422373, discriminator loss: 0.7484962589816746\n",
            "Epoch 91/300, Batch 200/937, generator loss: 2.1407012153799916, discriminator loss: 0.7487714356909432\n",
            "Epoch 91/300, Batch 300/937, generator loss: 2.1397604049352696, discriminator loss: 0.7490430887542643\n",
            "Epoch 91/300, Batch 400/937, generator loss: 2.1388010180094734, discriminator loss: 0.749345325059462\n",
            "Epoch 91/300, Batch 500/937, generator loss: 2.1378481541474903, discriminator loss: 0.7496255107526987\n",
            "Epoch 91/300, Batch 600/937, generator loss: 2.136974569797244, discriminator loss: 0.7499018217406036\n",
            "Epoch 91/300, Batch 700/937, generator loss: 2.1360692783847672, discriminator loss: 0.7501861127081301\n",
            "Epoch 91/300, Batch 800/937, generator loss: 2.135195912964593, discriminator loss: 0.7504471861483573\n",
            "Epoch 91/300, Batch 900/937, generator loss: 2.1342985819983356, discriminator loss: 0.7507469944653128\n",
            "Saving results...\n",
            "Epoch 92/300, Batch 0/937, generator loss: 2.1339941681195396, discriminator loss: 0.7508553483893289\n",
            "Epoch 92/300, Batch 100/937, generator loss: 2.133070236040801, discriminator loss: 0.7511396325285123\n",
            "Epoch 92/300, Batch 200/937, generator loss: 2.13221328771471, discriminator loss: 0.7514182811130095\n",
            "Epoch 92/300, Batch 300/937, generator loss: 2.1313211653564825, discriminator loss: 0.7516833343700028\n",
            "Epoch 92/300, Batch 400/937, generator loss: 2.1304729057978444, discriminator loss: 0.751944137145724\n",
            "Epoch 92/300, Batch 500/937, generator loss: 2.129591130197038, discriminator loss: 0.7522269750860253\n",
            "Epoch 92/300, Batch 600/937, generator loss: 2.128712429130182, discriminator loss: 0.7524944434815662\n",
            "Epoch 92/300, Batch 700/937, generator loss: 2.127859948566323, discriminator loss: 0.7527828410825949\n",
            "Epoch 92/300, Batch 800/937, generator loss: 2.126986660942813, discriminator loss: 0.7530375491390351\n",
            "Epoch 92/300, Batch 900/937, generator loss: 2.126172941638821, discriminator loss: 0.7532804773096909\n",
            "Saving results...\n",
            "Epoch 93/300, Batch 0/937, generator loss: 2.1258263026570052, discriminator loss: 0.7533846103483081\n",
            "Epoch 93/300, Batch 100/937, generator loss: 2.1249828703047493, discriminator loss: 0.7536418163597534\n",
            "Epoch 93/300, Batch 200/937, generator loss: 2.124119765943425, discriminator loss: 0.7539296115918914\n",
            "Epoch 93/300, Batch 300/937, generator loss: 2.123195876533776, discriminator loss: 0.7542308017470744\n",
            "Epoch 93/300, Batch 400/937, generator loss: 2.1223401459773674, discriminator loss: 0.7545235465919856\n",
            "Epoch 93/300, Batch 500/937, generator loss: 2.1214375264475254, discriminator loss: 0.7548000892009498\n",
            "Epoch 93/300, Batch 600/937, generator loss: 2.1205081262037657, discriminator loss: 0.7550708057931568\n",
            "Epoch 93/300, Batch 700/937, generator loss: 2.11963890806573, discriminator loss: 0.7553416993454156\n",
            "Epoch 93/300, Batch 800/937, generator loss: 2.118713769803246, discriminator loss: 0.7556106517119903\n",
            "Epoch 93/300, Batch 900/937, generator loss: 2.1178522548127763, discriminator loss: 0.7558843970751682\n",
            "Saving results...\n",
            "Epoch 94/300, Batch 0/937, generator loss: 2.1175332764826225, discriminator loss: 0.7559854121234528\n",
            "Epoch 94/300, Batch 100/937, generator loss: 2.116712833189951, discriminator loss: 0.7562302616352458\n",
            "Epoch 94/300, Batch 200/937, generator loss: 2.1158477000597697, discriminator loss: 0.7564800315698276\n",
            "Epoch 94/300, Batch 300/937, generator loss: 2.1149889898337397, discriminator loss: 0.756748660837938\n",
            "Epoch 94/300, Batch 400/937, generator loss: 2.114169350671898, discriminator loss: 0.7570168541806854\n",
            "Epoch 94/300, Batch 500/937, generator loss: 2.1132925413977497, discriminator loss: 0.7573147275038347\n",
            "Epoch 94/300, Batch 600/937, generator loss: 2.112417946119488, discriminator loss: 0.7575755263669015\n",
            "Epoch 94/300, Batch 700/937, generator loss: 2.111575744573402, discriminator loss: 0.7578288917885708\n",
            "Epoch 94/300, Batch 800/937, generator loss: 2.110660007200916, discriminator loss: 0.7581159674709811\n",
            "Epoch 94/300, Batch 900/937, generator loss: 2.109795344643024, discriminator loss: 0.7584095386983914\n",
            "Saving results...\n",
            "Epoch 95/300, Batch 0/937, generator loss: 2.109504585663665, discriminator loss: 0.7584937398183069\n",
            "Epoch 95/300, Batch 100/937, generator loss: 2.1086250514319085, discriminator loss: 0.7587694392187456\n",
            "Epoch 95/300, Batch 200/937, generator loss: 2.1077694516078473, discriminator loss: 0.7590171292753572\n",
            "Epoch 95/300, Batch 300/937, generator loss: 2.1069371649528557, discriminator loss: 0.7592832251123284\n",
            "Epoch 95/300, Batch 400/937, generator loss: 2.106060989503705, discriminator loss: 0.7595528687932143\n",
            "Epoch 95/300, Batch 500/937, generator loss: 2.105234065029719, discriminator loss: 0.7598076851814526\n",
            "Epoch 95/300, Batch 600/937, generator loss: 2.1043766355655795, discriminator loss: 0.7600507165743127\n",
            "Epoch 95/300, Batch 700/937, generator loss: 2.103517714930211, discriminator loss: 0.760326680023029\n",
            "Epoch 95/300, Batch 800/937, generator loss: 2.1027230405321453, discriminator loss: 0.760579914968503\n",
            "Epoch 95/300, Batch 900/937, generator loss: 2.1018790995514136, discriminator loss: 0.760878545610836\n",
            "Saving results...\n",
            "Epoch 96/300, Batch 0/937, generator loss: 2.101590956188927, discriminator loss: 0.7609547859838288\n",
            "Epoch 96/300, Batch 100/937, generator loss: 2.1007690797430283, discriminator loss: 0.7612274758277892\n",
            "Epoch 96/300, Batch 200/937, generator loss: 2.099951248636352, discriminator loss: 0.7614925580513415\n",
            "Epoch 96/300, Batch 300/937, generator loss: 2.099143496390154, discriminator loss: 0.7617486975022583\n",
            "Epoch 96/300, Batch 400/937, generator loss: 2.0982527962162707, discriminator loss: 0.7620090311961399\n",
            "Epoch 96/300, Batch 500/937, generator loss: 2.0973936079681916, discriminator loss: 0.7622667519730304\n",
            "Epoch 96/300, Batch 600/937, generator loss: 2.0965592408451954, discriminator loss: 0.7625505091943173\n",
            "Epoch 96/300, Batch 700/937, generator loss: 2.0956843368825875, discriminator loss: 0.762834113330274\n",
            "Epoch 96/300, Batch 800/937, generator loss: 2.0949275196679005, discriminator loss: 0.7630690326365883\n",
            "Epoch 96/300, Batch 900/937, generator loss: 2.0940730242984102, discriminator loss: 0.7633232213489111\n",
            "Saving results...\n",
            "Epoch 97/300, Batch 0/937, generator loss: 2.0937611092184882, discriminator loss: 0.7634222275625953\n",
            "Epoch 97/300, Batch 100/937, generator loss: 2.0929410882134505, discriminator loss: 0.7636708485034026\n",
            "Epoch 97/300, Batch 200/937, generator loss: 2.092151403427124, discriminator loss: 0.763926019162702\n",
            "Epoch 97/300, Batch 300/937, generator loss: 2.0912842512758223, discriminator loss: 0.764195539279326\n",
            "Epoch 97/300, Batch 400/937, generator loss: 2.0904785883813193, discriminator loss: 0.7644356020339309\n",
            "Epoch 97/300, Batch 500/937, generator loss: 2.0896551568074564, discriminator loss: 0.7647061215828774\n",
            "Epoch 97/300, Batch 600/937, generator loss: 2.0888060111812665, discriminator loss: 0.7649650049337938\n",
            "Epoch 97/300, Batch 700/937, generator loss: 2.087964559419406, discriminator loss: 0.7652234920310407\n",
            "Epoch 97/300, Batch 800/937, generator loss: 2.087171181950266, discriminator loss: 0.7654790884669805\n",
            "Epoch 97/300, Batch 900/937, generator loss: 2.0863166123476016, discriminator loss: 0.7657456236167312\n",
            "Saving results...\n",
            "Epoch 98/300, Batch 0/937, generator loss: 2.086025493760619, discriminator loss: 0.7658185682577344\n",
            "Epoch 98/300, Batch 100/937, generator loss: 2.085191632147209, discriminator loss: 0.7660684664464271\n",
            "Epoch 98/300, Batch 200/937, generator loss: 2.0844159352512155, discriminator loss: 0.7663300722056662\n",
            "Epoch 98/300, Batch 300/937, generator loss: 2.083587463860355, discriminator loss: 0.7665795900836385\n",
            "Epoch 98/300, Batch 400/937, generator loss: 2.0827851107907245, discriminator loss: 0.7668378485209232\n",
            "Epoch 98/300, Batch 500/937, generator loss: 2.081952714359808, discriminator loss: 0.7670993341000312\n",
            "Epoch 98/300, Batch 600/937, generator loss: 2.081216897759482, discriminator loss: 0.767337940040246\n",
            "Epoch 98/300, Batch 700/937, generator loss: 2.0803995417936862, discriminator loss: 0.7675858038471188\n",
            "Epoch 98/300, Batch 800/937, generator loss: 2.0796054038949263, discriminator loss: 0.7678372940645394\n",
            "Epoch 98/300, Batch 900/937, generator loss: 2.078832894023404, discriminator loss: 0.7680745024582332\n",
            "Saving results...\n",
            "Epoch 99/300, Batch 0/937, generator loss: 2.078521899083192, discriminator loss: 0.7681814114912188\n",
            "Epoch 99/300, Batch 100/937, generator loss: 2.0777175715642446, discriminator loss: 0.768426987518724\n",
            "Epoch 99/300, Batch 200/937, generator loss: 2.0769439161953542, discriminator loss: 0.7686535862340307\n",
            "Epoch 99/300, Batch 300/937, generator loss: 2.076158888479183, discriminator loss: 0.7688967368844825\n",
            "Epoch 99/300, Batch 400/937, generator loss: 2.0753471522456173, discriminator loss: 0.7691494667556547\n",
            "Epoch 99/300, Batch 500/937, generator loss: 2.0746272093512412, discriminator loss: 0.7693653274180524\n",
            "Epoch 99/300, Batch 600/937, generator loss: 2.073844024195784, discriminator loss: 0.7695991857303971\n",
            "Epoch 99/300, Batch 700/937, generator loss: 2.0731058368848516, discriminator loss: 0.7698608648440415\n",
            "Epoch 99/300, Batch 800/937, generator loss: 2.0722431122643843, discriminator loss: 0.7701379884590059\n",
            "Epoch 99/300, Batch 900/937, generator loss: 2.07143649050656, discriminator loss: 0.7703996588217319\n",
            "Saving results...\n",
            "Epoch 100/300, Batch 0/937, generator loss: 2.071130142589283, discriminator loss: 0.7704806827459809\n",
            "Epoch 100/300, Batch 100/937, generator loss: 2.0703471157191435, discriminator loss: 0.7707099929855287\n",
            "Epoch 100/300, Batch 200/937, generator loss: 2.0695572100855304, discriminator loss: 0.7709522571394595\n",
            "Epoch 100/300, Batch 300/937, generator loss: 2.0687544562972833, discriminator loss: 0.771202212434755\n",
            "Epoch 100/300, Batch 400/937, generator loss: 2.0680127388312317, discriminator loss: 0.7714488574386693\n",
            "Epoch 100/300, Batch 500/937, generator loss: 2.067256732557907, discriminator loss: 0.7716962951010901\n",
            "Epoch 100/300, Batch 600/937, generator loss: 2.0664525490836336, discriminator loss: 0.7719433478031117\n",
            "Epoch 100/300, Batch 700/937, generator loss: 2.0656560524674457, discriminator loss: 0.7721842939310716\n",
            "Epoch 100/300, Batch 800/937, generator loss: 2.0649040541091703, discriminator loss: 0.7724327912707274\n",
            "Epoch 100/300, Batch 900/937, generator loss: 2.064146527651028, discriminator loss: 0.7726507582887759\n",
            "Saving results...\n",
            "Epoch 101/300, Batch 0/937, generator loss: 2.063839914792741, discriminator loss: 0.7727585370077099\n",
            "Epoch 101/300, Batch 100/937, generator loss: 2.0630996818567042, discriminator loss: 0.7730024393990247\n",
            "Epoch 101/300, Batch 200/937, generator loss: 2.062301764042571, discriminator loss: 0.7732435981750403\n",
            "Epoch 101/300, Batch 300/937, generator loss: 2.0615576635759334, discriminator loss: 0.7734873894408297\n",
            "Epoch 101/300, Batch 400/937, generator loss: 2.060833445789512, discriminator loss: 0.7737231909129306\n",
            "Epoch 101/300, Batch 500/937, generator loss: 2.0601000208438576, discriminator loss: 0.7739513465519241\n",
            "Epoch 101/300, Batch 600/937, generator loss: 2.0593614462990644, discriminator loss: 0.774195008217849\n",
            "Epoch 101/300, Batch 700/937, generator loss: 2.058653216862254, discriminator loss: 0.7744187246772466\n",
            "Epoch 101/300, Batch 800/937, generator loss: 2.0578684119875796, discriminator loss: 0.7746881157685667\n",
            "Epoch 101/300, Batch 900/937, generator loss: 2.057126333052888, discriminator loss: 0.7749182017613926\n",
            "Saving results...\n",
            "Epoch 102/300, Batch 0/937, generator loss: 2.0568440633970124, discriminator loss: 0.7749957544539194\n",
            "Epoch 102/300, Batch 100/937, generator loss: 2.056089945226355, discriminator loss: 0.7752364014286518\n",
            "Epoch 102/300, Batch 200/937, generator loss: 2.055325904600118, discriminator loss: 0.7754669102123218\n",
            "Epoch 102/300, Batch 300/937, generator loss: 2.0545635974161627, discriminator loss: 0.7756993925248162\n",
            "Epoch 102/300, Batch 400/937, generator loss: 2.0538413321055855, discriminator loss: 0.7759131489463944\n",
            "Epoch 102/300, Batch 500/937, generator loss: 2.0530974983418324, discriminator loss: 0.7761355304020071\n",
            "Epoch 102/300, Batch 600/937, generator loss: 2.052368451833787, discriminator loss: 0.7763533902295323\n",
            "Epoch 102/300, Batch 700/937, generator loss: 2.051591587647374, discriminator loss: 0.7765969882420644\n",
            "Epoch 102/300, Batch 800/937, generator loss: 2.050858383066769, discriminator loss: 0.7768245920507206\n",
            "Epoch 102/300, Batch 900/937, generator loss: 2.050066895829053, discriminator loss: 0.7770683441291235\n",
            "Saving results...\n",
            "Epoch 103/300, Batch 0/937, generator loss: 2.0498008150292377, discriminator loss: 0.7771526501079853\n",
            "Epoch 103/300, Batch 100/937, generator loss: 2.049097099683203, discriminator loss: 0.7773830389720962\n",
            "Epoch 103/300, Batch 200/937, generator loss: 2.048385124616907, discriminator loss: 0.7776143814724678\n",
            "Epoch 103/300, Batch 300/937, generator loss: 2.0476950119483748, discriminator loss: 0.7778451680027024\n",
            "Epoch 103/300, Batch 400/937, generator loss: 2.046996767170923, discriminator loss: 0.7780627769743877\n",
            "Epoch 103/300, Batch 500/937, generator loss: 2.0462710592165063, discriminator loss: 0.7782848875304401\n",
            "Epoch 103/300, Batch 600/937, generator loss: 2.045581484986978, discriminator loss: 0.7784926540209317\n",
            "Epoch 103/300, Batch 700/937, generator loss: 2.0448131101920333, discriminator loss: 0.7787262765334373\n",
            "Epoch 103/300, Batch 800/937, generator loss: 2.0441495495958915, discriminator loss: 0.7789430767533562\n",
            "Epoch 103/300, Batch 900/937, generator loss: 2.0433980174849413, discriminator loss: 0.7791802796906492\n",
            "Saving results...\n",
            "Epoch 104/300, Batch 0/937, generator loss: 2.0431423372646864, discriminator loss: 0.7792763478099725\n",
            "Epoch 104/300, Batch 100/937, generator loss: 2.0424489867142315, discriminator loss: 0.7794819935887576\n",
            "Epoch 104/300, Batch 200/937, generator loss: 2.041706625822606, discriminator loss: 0.7797177002413945\n",
            "Epoch 104/300, Batch 300/937, generator loss: 2.040988448460116, discriminator loss: 0.7799116794747205\n",
            "Epoch 104/300, Batch 400/937, generator loss: 2.0403235537541486, discriminator loss: 0.7801281266905602\n",
            "Epoch 104/300, Batch 500/937, generator loss: 2.0396029221164373, discriminator loss: 0.7803578933003565\n",
            "Epoch 104/300, Batch 600/937, generator loss: 2.038868189726731, discriminator loss: 0.7806076910356815\n",
            "Epoch 104/300, Batch 700/937, generator loss: 2.038123995417005, discriminator loss: 0.7808425224247362\n",
            "Epoch 104/300, Batch 800/937, generator loss: 2.037449137547101, discriminator loss: 0.7810690132140177\n",
            "Epoch 104/300, Batch 900/937, generator loss: 2.0367319411616376, discriminator loss: 0.7813019292681416\n",
            "Saving results...\n",
            "Epoch 105/300, Batch 0/937, generator loss: 2.03645857868416, discriminator loss: 0.7813971698719737\n",
            "Epoch 105/300, Batch 100/937, generator loss: 2.035764555432108, discriminator loss: 0.7816104829578269\n",
            "Epoch 105/300, Batch 200/937, generator loss: 2.0350667053712947, discriminator loss: 0.7818292353034026\n",
            "Epoch 105/300, Batch 300/937, generator loss: 2.034309095723733, discriminator loss: 0.7820558362070753\n",
            "Epoch 105/300, Batch 400/937, generator loss: 2.033622878916127, discriminator loss: 0.7822672643187225\n",
            "Epoch 105/300, Batch 500/937, generator loss: 2.0329228716858565, discriminator loss: 0.7824925598018323\n",
            "Epoch 105/300, Batch 600/937, generator loss: 2.0321950179873047, discriminator loss: 0.7827213329056776\n",
            "Epoch 105/300, Batch 700/937, generator loss: 2.0314666574806854, discriminator loss: 0.7829541116381429\n",
            "Epoch 105/300, Batch 800/937, generator loss: 2.0307541164385463, discriminator loss: 0.7831823062900214\n",
            "Epoch 105/300, Batch 900/937, generator loss: 2.0300941888838864, discriminator loss: 0.7833807810324862\n",
            "Saving results...\n",
            "Epoch 106/300, Batch 0/937, generator loss: 2.0298563797383675, discriminator loss: 0.7834634237895995\n",
            "Epoch 106/300, Batch 100/937, generator loss: 2.0291969001892087, discriminator loss: 0.7836503484285853\n",
            "Epoch 106/300, Batch 200/937, generator loss: 2.0285272502825387, discriminator loss: 0.7838779439010233\n",
            "Epoch 106/300, Batch 300/937, generator loss: 2.02785185910307, discriminator loss: 0.784080554981402\n",
            "Epoch 106/300, Batch 400/937, generator loss: 2.027178612702657, discriminator loss: 0.7843051394252964\n",
            "Epoch 106/300, Batch 500/937, generator loss: 2.0264625383094783, discriminator loss: 0.7845137383299461\n",
            "Epoch 106/300, Batch 600/937, generator loss: 2.0257954986072786, discriminator loss: 0.7847282056112445\n",
            "Epoch 106/300, Batch 700/937, generator loss: 2.0251292071681375, discriminator loss: 0.7849411593835353\n",
            "Epoch 106/300, Batch 800/937, generator loss: 2.024457781978006, discriminator loss: 0.7851840345385038\n",
            "Epoch 106/300, Batch 900/937, generator loss: 2.0237680075572375, discriminator loss: 0.7853815776603253\n",
            "Saving results...\n",
            "Epoch 107/300, Batch 0/937, generator loss: 2.0234805375605127, discriminator loss: 0.7854707692287388\n",
            "Epoch 107/300, Batch 100/937, generator loss: 2.022786656596975, discriminator loss: 0.7856815798084139\n",
            "Epoch 107/300, Batch 200/937, generator loss: 2.022101192839374, discriminator loss: 0.7858676430285155\n",
            "Epoch 107/300, Batch 300/937, generator loss: 2.0214171253091395, discriminator loss: 0.7860789384106107\n",
            "Epoch 107/300, Batch 400/937, generator loss: 2.0207072546876215, discriminator loss: 0.7863104246786212\n",
            "Epoch 107/300, Batch 500/937, generator loss: 2.020035838827627, discriminator loss: 0.786500563561242\n",
            "Epoch 107/300, Batch 600/937, generator loss: 2.0193824836901912, discriminator loss: 0.7867084960831244\n",
            "Epoch 107/300, Batch 700/937, generator loss: 2.018722457109276, discriminator loss: 0.7869218736754143\n",
            "Epoch 107/300, Batch 800/937, generator loss: 2.018051624819474, discriminator loss: 0.7871257211589091\n",
            "Epoch 107/300, Batch 900/937, generator loss: 2.0173917510508077, discriminator loss: 0.7873446291325096\n",
            "Saving results...\n",
            "Epoch 108/300, Batch 0/937, generator loss: 2.017131541776404, discriminator loss: 0.7874313496241772\n",
            "Epoch 108/300, Batch 100/937, generator loss: 2.016471737349425, discriminator loss: 0.7876465865044346\n",
            "Epoch 108/300, Batch 200/937, generator loss: 2.01581318631135, discriminator loss: 0.7878382995180784\n",
            "Epoch 108/300, Batch 300/937, generator loss: 2.0151539890908516, discriminator loss: 0.7880454361885065\n",
            "Epoch 108/300, Batch 400/937, generator loss: 2.0144588375021857, discriminator loss: 0.7882464629757896\n",
            "Epoch 108/300, Batch 500/937, generator loss: 2.0137929247313147, discriminator loss: 0.788434573045582\n",
            "Epoch 108/300, Batch 600/937, generator loss: 2.013135916814554, discriminator loss: 0.7886316772958638\n",
            "Epoch 108/300, Batch 700/937, generator loss: 2.0125029729575674, discriminator loss: 0.7888277038114818\n",
            "Epoch 108/300, Batch 800/937, generator loss: 2.011844333393211, discriminator loss: 0.7890285117017364\n",
            "Epoch 108/300, Batch 900/937, generator loss: 2.0112010146830905, discriminator loss: 0.7892304968116195\n",
            "Saving results...\n",
            "Epoch 109/300, Batch 0/937, generator loss: 2.0109757288266517, discriminator loss: 0.7893040478757674\n",
            "Epoch 109/300, Batch 100/937, generator loss: 2.010312851544993, discriminator loss: 0.7895113481917235\n",
            "Epoch 109/300, Batch 200/937, generator loss: 2.0096783153714934, discriminator loss: 0.789710463568413\n",
            "Epoch 109/300, Batch 300/937, generator loss: 2.0090213293716603, discriminator loss: 0.7898952984502292\n",
            "Epoch 109/300, Batch 400/937, generator loss: 2.008407305959147, discriminator loss: 0.7900810740773454\n",
            "Epoch 109/300, Batch 500/937, generator loss: 2.0077540473572393, discriminator loss: 0.790281780236153\n",
            "Epoch 109/300, Batch 600/937, generator loss: 2.0071481287451554, discriminator loss: 0.7904702681934729\n",
            "Epoch 109/300, Batch 700/937, generator loss: 2.0064757119906322, discriminator loss: 0.7906773769100486\n",
            "Epoch 109/300, Batch 800/937, generator loss: 2.0058707213620446, discriminator loss: 0.7908770015536447\n",
            "Epoch 109/300, Batch 900/937, generator loss: 2.0051992773248406, discriminator loss: 0.7910828806822475\n",
            "Saving results...\n",
            "Epoch 110/300, Batch 0/937, generator loss: 2.0050108963868785, discriminator loss: 0.7911533815087509\n",
            "Epoch 110/300, Batch 100/937, generator loss: 2.0043766030942103, discriminator loss: 0.7913525067395096\n",
            "Epoch 110/300, Batch 200/937, generator loss: 2.0037299412003367, discriminator loss: 0.7915631259181226\n",
            "Epoch 110/300, Batch 300/937, generator loss: 2.0030983714458808, discriminator loss: 0.7917496708529075\n",
            "Epoch 110/300, Batch 400/937, generator loss: 2.002476507403476, discriminator loss: 0.7919419163710956\n",
            "Epoch 110/300, Batch 500/937, generator loss: 2.001870368381757, discriminator loss: 0.792137442080761\n",
            "Epoch 110/300, Batch 600/937, generator loss: 2.0012284327672867, discriminator loss: 0.7923579884845943\n",
            "Epoch 110/300, Batch 700/937, generator loss: 2.0006166804877727, discriminator loss: 0.7925315574828913\n",
            "Epoch 110/300, Batch 800/937, generator loss: 1.9999888042472134, discriminator loss: 0.792751319292491\n",
            "Epoch 110/300, Batch 900/937, generator loss: 1.999373380353209, discriminator loss: 0.7929398745793987\n",
            "Saving results...\n",
            "Epoch 111/300, Batch 0/937, generator loss: 1.9991307591884908, discriminator loss: 0.7930005896630498\n",
            "Epoch 111/300, Batch 100/937, generator loss: 1.9985078899510087, discriminator loss: 0.7931934518924723\n",
            "Epoch 111/300, Batch 200/937, generator loss: 1.9979086119161669, discriminator loss: 0.7933741612959028\n",
            "Epoch 111/300, Batch 300/937, generator loss: 1.9973197632921058, discriminator loss: 0.7935511606541171\n",
            "Epoch 111/300, Batch 400/937, generator loss: 1.996685507433203, discriminator loss: 0.7937372884051676\n",
            "Epoch 111/300, Batch 500/937, generator loss: 1.9960678784501726, discriminator loss: 0.7939248200272562\n",
            "Epoch 111/300, Batch 600/937, generator loss: 1.9954530192916866, discriminator loss: 0.7941159247715595\n",
            "Epoch 111/300, Batch 700/937, generator loss: 1.9949005087485, discriminator loss: 0.7942936586948072\n",
            "Epoch 111/300, Batch 800/937, generator loss: 1.994274335509652, discriminator loss: 0.7944957205687199\n",
            "Epoch 111/300, Batch 900/937, generator loss: 1.9936463355503313, discriminator loss: 0.7946923957270404\n",
            "Saving results...\n",
            "Epoch 112/300, Batch 0/937, generator loss: 1.993435344600514, discriminator loss: 0.7947512841834092\n",
            "Epoch 112/300, Batch 100/937, generator loss: 1.9928081239198672, discriminator loss: 0.7949421410062073\n",
            "Epoch 112/300, Batch 200/937, generator loss: 1.9922724173488378, discriminator loss: 0.7951012553876304\n",
            "Epoch 112/300, Batch 300/937, generator loss: 1.9916449377228571, discriminator loss: 0.7952889574976328\n",
            "Epoch 112/300, Batch 400/937, generator loss: 1.991039853083787, discriminator loss: 0.7954717249123778\n",
            "Epoch 112/300, Batch 500/937, generator loss: 1.990451045682756, discriminator loss: 0.7956614933245563\n",
            "Epoch 112/300, Batch 600/937, generator loss: 1.989835894941928, discriminator loss: 0.7958470130806492\n",
            "Epoch 112/300, Batch 700/937, generator loss: 1.9892044053029627, discriminator loss: 0.7960428815074976\n",
            "Epoch 112/300, Batch 800/937, generator loss: 1.9886228462895454, discriminator loss: 0.7962155382226469\n",
            "Epoch 112/300, Batch 900/937, generator loss: 1.9879984642157473, discriminator loss: 0.7964148269486792\n",
            "Saving results...\n",
            "Epoch 113/300, Batch 0/937, generator loss: 1.9878113788617724, discriminator loss: 0.796463865665408\n",
            "Epoch 113/300, Batch 100/937, generator loss: 1.9872031799902097, discriminator loss: 0.7966584359010053\n",
            "Epoch 113/300, Batch 200/937, generator loss: 1.9866388147030927, discriminator loss: 0.7968336237293075\n",
            "Epoch 113/300, Batch 300/937, generator loss: 1.9860390218170585, discriminator loss: 0.7970191255284524\n",
            "Epoch 113/300, Batch 400/937, generator loss: 1.9854322443352135, discriminator loss: 0.7971870792221986\n",
            "Epoch 113/300, Batch 500/937, generator loss: 1.9848335268171782, discriminator loss: 0.7973950672057435\n",
            "Epoch 113/300, Batch 600/937, generator loss: 1.9842535992302217, discriminator loss: 0.7975578060575633\n",
            "Epoch 113/300, Batch 700/937, generator loss: 1.9836488027503152, discriminator loss: 0.7977492123197998\n",
            "Epoch 113/300, Batch 800/937, generator loss: 1.9830371310405357, discriminator loss: 0.7979332868012808\n",
            "Epoch 113/300, Batch 900/937, generator loss: 1.9824640544818006, discriminator loss: 0.7981159516216559\n",
            "Saving results...\n",
            "Epoch 114/300, Batch 0/937, generator loss: 1.9822597219524631, discriminator loss: 0.7981799513330078\n",
            "Epoch 114/300, Batch 100/937, generator loss: 1.9816899361295015, discriminator loss: 0.7983655220482611\n",
            "Epoch 114/300, Batch 200/937, generator loss: 1.9811101139200518, discriminator loss: 0.7985399986113981\n",
            "Epoch 114/300, Batch 300/937, generator loss: 1.9805325230114388, discriminator loss: 0.7987435448230248\n",
            "Epoch 114/300, Batch 400/937, generator loss: 1.9799732046238325, discriminator loss: 0.7989271896554612\n",
            "Epoch 114/300, Batch 500/937, generator loss: 1.9794234611457575, discriminator loss: 0.7990887827522064\n",
            "Epoch 114/300, Batch 600/937, generator loss: 1.9787940798035373, discriminator loss: 0.7992857348694299\n",
            "Epoch 114/300, Batch 700/937, generator loss: 1.9781892282788351, discriminator loss: 0.7994588834310474\n",
            "Epoch 114/300, Batch 800/937, generator loss: 1.9775999236746458, discriminator loss: 0.7996514134719569\n",
            "Epoch 114/300, Batch 900/937, generator loss: 1.9769980085007972, discriminator loss: 0.7998319318267434\n",
            "Saving results...\n",
            "Epoch 115/300, Batch 0/937, generator loss: 1.97680431932732, discriminator loss: 0.7998793369866505\n",
            "Epoch 115/300, Batch 100/937, generator loss: 1.9762403245480915, discriminator loss: 0.8000531656077103\n",
            "Epoch 115/300, Batch 200/937, generator loss: 1.9756978075096794, discriminator loss: 0.8002119549244069\n",
            "Epoch 115/300, Batch 300/937, generator loss: 1.9751485108881317, discriminator loss: 0.8003995530296261\n",
            "Epoch 115/300, Batch 400/937, generator loss: 1.9746260115449485, discriminator loss: 0.8005596546012514\n",
            "Epoch 115/300, Batch 500/937, generator loss: 1.9740119318590956, discriminator loss: 0.8007371967703549\n",
            "Epoch 115/300, Batch 600/937, generator loss: 1.9734452667140483, discriminator loss: 0.8009223659303177\n",
            "Epoch 115/300, Batch 700/937, generator loss: 1.9728965951254684, discriminator loss: 0.8010882006266625\n",
            "Epoch 115/300, Batch 800/937, generator loss: 1.97234478194908, discriminator loss: 0.8012596652484962\n",
            "Epoch 115/300, Batch 900/937, generator loss: 1.9717618232337035, discriminator loss: 0.8014328955927625\n",
            "Saving results...\n",
            "Epoch 116/300, Batch 0/937, generator loss: 1.9715623199042036, discriminator loss: 0.8014942638520988\n",
            "Epoch 116/300, Batch 100/937, generator loss: 1.9709845119606422, discriminator loss: 0.8016729628150677\n",
            "Epoch 116/300, Batch 200/937, generator loss: 1.9704220935976144, discriminator loss: 0.8018429292100246\n",
            "Epoch 116/300, Batch 300/937, generator loss: 1.9698690233195986, discriminator loss: 0.8020033971027942\n",
            "Epoch 116/300, Batch 400/937, generator loss: 1.9693085505826151, discriminator loss: 0.8021952523951708\n",
            "Epoch 116/300, Batch 500/937, generator loss: 1.9687162761905792, discriminator loss: 0.8023804317705283\n",
            "Epoch 116/300, Batch 600/937, generator loss: 1.968160935539977, discriminator loss: 0.8025385262293873\n",
            "Epoch 116/300, Batch 700/937, generator loss: 1.9676239706418064, discriminator loss: 0.8026972464523021\n",
            "Epoch 116/300, Batch 800/937, generator loss: 1.967088032410937, discriminator loss: 0.8028692859032958\n",
            "Epoch 116/300, Batch 900/937, generator loss: 1.9665407438781954, discriminator loss: 0.8030303888515172\n",
            "Saving results...\n",
            "Epoch 117/300, Batch 0/937, generator loss: 1.9663248346630027, discriminator loss: 0.8031084734808188\n",
            "Epoch 117/300, Batch 100/937, generator loss: 1.9657935244599483, discriminator loss: 0.8032579844001541\n",
            "Epoch 117/300, Batch 200/937, generator loss: 1.965233863774169, discriminator loss: 0.8034279040294213\n",
            "Epoch 117/300, Batch 300/937, generator loss: 1.9646827171456767, discriminator loss: 0.8035870284975296\n",
            "Epoch 117/300, Batch 400/937, generator loss: 1.9641030763188654, discriminator loss: 0.8037682150265179\n",
            "Epoch 117/300, Batch 500/937, generator loss: 1.9635379229201984, discriminator loss: 0.8039365104211293\n",
            "Epoch 117/300, Batch 600/937, generator loss: 1.9629797568881857, discriminator loss: 0.8041085641199519\n",
            "Epoch 117/300, Batch 700/937, generator loss: 1.9624434497991088, discriminator loss: 0.8042649035307176\n",
            "Epoch 117/300, Batch 800/937, generator loss: 1.9618818437606218, discriminator loss: 0.80444379510935\n",
            "Epoch 117/300, Batch 900/937, generator loss: 1.9613195574694517, discriminator loss: 0.804617691863762\n",
            "Saving results...\n",
            "Epoch 118/300, Batch 0/937, generator loss: 1.9611255392467657, discriminator loss: 0.8046774030164633\n",
            "Epoch 118/300, Batch 100/937, generator loss: 1.9605776945978284, discriminator loss: 0.8048388699099124\n",
            "Epoch 118/300, Batch 200/937, generator loss: 1.960018759623283, discriminator loss: 0.8050087224720188\n",
            "Epoch 118/300, Batch 300/937, generator loss: 1.959480035529427, discriminator loss: 0.8051681506923215\n",
            "Epoch 118/300, Batch 400/937, generator loss: 1.95896583680761, discriminator loss: 0.8053319479210339\n",
            "Epoch 118/300, Batch 500/937, generator loss: 1.9584226513417256, discriminator loss: 0.805500173443524\n",
            "Epoch 118/300, Batch 600/937, generator loss: 1.9578641577610774, discriminator loss: 0.8056555215161054\n",
            "Epoch 118/300, Batch 700/937, generator loss: 1.9573278381006451, discriminator loss: 0.8058387959755129\n",
            "Epoch 118/300, Batch 800/937, generator loss: 1.9568285750424568, discriminator loss: 0.8060071676680782\n",
            "Epoch 118/300, Batch 900/937, generator loss: 1.9562945587684641, discriminator loss: 0.8061647789067758\n",
            "Saving results...\n",
            "Epoch 119/300, Batch 0/937, generator loss: 1.9560872703894367, discriminator loss: 0.8062242420860921\n",
            "Epoch 119/300, Batch 100/937, generator loss: 1.9555743366494827, discriminator loss: 0.806377930645778\n",
            "Epoch 119/300, Batch 200/937, generator loss: 1.9550296488535965, discriminator loss: 0.8065512955030274\n",
            "Epoch 119/300, Batch 300/937, generator loss: 1.9545140387265483, discriminator loss: 0.8067070642709924\n",
            "Epoch 119/300, Batch 400/937, generator loss: 1.9539771149558682, discriminator loss: 0.8068738635392391\n",
            "Epoch 119/300, Batch 500/937, generator loss: 1.9534514946476238, discriminator loss: 0.8070358871833043\n",
            "Epoch 119/300, Batch 600/937, generator loss: 1.9529344438698362, discriminator loss: 0.807197500679562\n",
            "Epoch 119/300, Batch 700/937, generator loss: 1.9523853293738345, discriminator loss: 0.8073620546316596\n",
            "Epoch 119/300, Batch 800/937, generator loss: 1.9518598740009403, discriminator loss: 0.8075181669164845\n",
            "Epoch 119/300, Batch 900/937, generator loss: 1.9513460648650143, discriminator loss: 0.8076723530880073\n",
            "Saving results...\n",
            "Epoch 120/300, Batch 0/937, generator loss: 1.951174569272964, discriminator loss: 0.8077159837328162\n",
            "Epoch 120/300, Batch 100/937, generator loss: 1.9506470075731182, discriminator loss: 0.8078574825089603\n",
            "Epoch 120/300, Batch 200/937, generator loss: 1.9501519808498002, discriminator loss: 0.8080169232975174\n",
            "Epoch 120/300, Batch 300/937, generator loss: 1.9496425807451196, discriminator loss: 0.8081558498928118\n",
            "Epoch 120/300, Batch 400/937, generator loss: 1.949154642490296, discriminator loss: 0.8083057971491217\n",
            "Epoch 120/300, Batch 500/937, generator loss: 1.9486565439833714, discriminator loss: 0.8084663032285836\n",
            "Epoch 120/300, Batch 600/937, generator loss: 1.9481529832996751, discriminator loss: 0.8086262070470182\n",
            "Epoch 120/300, Batch 700/937, generator loss: 1.94762165440253, discriminator loss: 0.8087855349644063\n",
            "Epoch 120/300, Batch 800/937, generator loss: 1.9470902813354565, discriminator loss: 0.8089590819233423\n",
            "Epoch 120/300, Batch 900/937, generator loss: 1.9465686434564218, discriminator loss: 0.8091201503261511\n",
            "Saving results...\n",
            "Epoch 121/300, Batch 0/937, generator loss: 1.9463808000908907, discriminator loss: 0.8091778750022827\n",
            "Epoch 121/300, Batch 100/937, generator loss: 1.9458891308553046, discriminator loss: 0.8093409070807123\n",
            "Epoch 121/300, Batch 200/937, generator loss: 1.9453646729298688, discriminator loss: 0.8094702246964695\n",
            "Epoch 121/300, Batch 300/937, generator loss: 1.94487111918734, discriminator loss: 0.8096212317051942\n",
            "Epoch 121/300, Batch 400/937, generator loss: 1.9443946807146948, discriminator loss: 0.8097744292564154\n",
            "Epoch 121/300, Batch 500/937, generator loss: 1.9439089472590227, discriminator loss: 0.8099274015644478\n",
            "Epoch 121/300, Batch 600/937, generator loss: 1.9433833849472129, discriminator loss: 0.8101224411493306\n",
            "Epoch 121/300, Batch 700/937, generator loss: 1.942915084565216, discriminator loss: 0.8102720669767094\n",
            "Epoch 121/300, Batch 800/937, generator loss: 1.9423841062676301, discriminator loss: 0.8104264228616235\n",
            "Epoch 121/300, Batch 900/937, generator loss: 1.941886321545486, discriminator loss: 0.8105741965749128\n",
            "Saving results...\n",
            "Epoch 122/300, Batch 0/937, generator loss: 1.9416996886686082, discriminator loss: 0.8106327867863012\n",
            "Epoch 122/300, Batch 100/937, generator loss: 1.9412550942248588, discriminator loss: 0.8107560362725941\n",
            "Epoch 122/300, Batch 200/937, generator loss: 1.9407415829828898, discriminator loss: 0.8109299623583132\n",
            "Epoch 122/300, Batch 300/937, generator loss: 1.9402780886918094, discriminator loss: 0.8110796172594528\n",
            "Epoch 122/300, Batch 400/937, generator loss: 1.939797848339656, discriminator loss: 0.811218356444749\n",
            "Epoch 122/300, Batch 500/937, generator loss: 1.939260508979036, discriminator loss: 0.8113721617673564\n",
            "Epoch 122/300, Batch 600/937, generator loss: 1.9387631287080151, discriminator loss: 0.8115345336620342\n",
            "Epoch 122/300, Batch 700/937, generator loss: 1.9382577974138242, discriminator loss: 0.8116936623749564\n",
            "Epoch 122/300, Batch 800/937, generator loss: 1.9377708395121955, discriminator loss: 0.8118410434204806\n",
            "Epoch 122/300, Batch 900/937, generator loss: 1.937234517088037, discriminator loss: 0.8120204636856939\n",
            "Saving results...\n",
            "Epoch 123/300, Batch 0/937, generator loss: 1.9370368600141972, discriminator loss: 0.8120794911288576\n",
            "Epoch 123/300, Batch 100/937, generator loss: 1.9365328414637832, discriminator loss: 0.8122066400446035\n",
            "Epoch 123/300, Batch 200/937, generator loss: 1.9360494516454079, discriminator loss: 0.812353617854462\n",
            "Epoch 123/300, Batch 300/937, generator loss: 1.9355462430552064, discriminator loss: 0.8125100202919832\n",
            "Epoch 123/300, Batch 400/937, generator loss: 1.9350784617120458, discriminator loss: 0.8126414735232893\n",
            "Epoch 123/300, Batch 500/937, generator loss: 1.934586836817731, discriminator loss: 0.8128085135180307\n",
            "Epoch 123/300, Batch 600/937, generator loss: 1.9341221308365952, discriminator loss: 0.8129479319165372\n",
            "Epoch 123/300, Batch 700/937, generator loss: 1.9336409481383405, discriminator loss: 0.8130875879113977\n",
            "Epoch 123/300, Batch 800/937, generator loss: 1.9331586903599094, discriminator loss: 0.8132361657610622\n",
            "Epoch 123/300, Batch 900/937, generator loss: 1.9326768255376834, discriminator loss: 0.813384108978936\n",
            "Saving results...\n",
            "Epoch 124/300, Batch 0/937, generator loss: 1.9325103437167266, discriminator loss: 0.8134403241305115\n",
            "Epoch 124/300, Batch 100/937, generator loss: 1.9320577029885053, discriminator loss: 0.8135644111126464\n",
            "Epoch 124/300, Batch 200/937, generator loss: 1.9315833065461228, discriminator loss: 0.8137004662245167\n",
            "Epoch 124/300, Batch 300/937, generator loss: 1.9310555028634393, discriminator loss: 0.813872747618352\n",
            "Epoch 124/300, Batch 400/937, generator loss: 1.9305884350889995, discriminator loss: 0.8140050397803875\n",
            "Epoch 124/300, Batch 500/937, generator loss: 1.9301092676290779, discriminator loss: 0.8141646423188668\n",
            "Epoch 124/300, Batch 600/937, generator loss: 1.9296459757157585, discriminator loss: 0.8142950226926935\n",
            "Epoch 124/300, Batch 700/937, generator loss: 1.9291901476689837, discriminator loss: 0.8144527485377897\n",
            "Epoch 124/300, Batch 800/937, generator loss: 1.9287423771381202, discriminator loss: 0.8145865158681693\n",
            "Epoch 124/300, Batch 900/937, generator loss: 1.928256038871075, discriminator loss: 0.8147508923287043\n",
            "Saving results...\n",
            "Epoch 125/300, Batch 0/937, generator loss: 1.9280820006778725, discriminator loss: 0.814800524176304\n",
            "Epoch 125/300, Batch 100/937, generator loss: 1.9276210491601287, discriminator loss: 0.8149397068472204\n",
            "Epoch 125/300, Batch 200/937, generator loss: 1.9271442949225246, discriminator loss: 0.8150744058891963\n",
            "Epoch 125/300, Batch 300/937, generator loss: 1.9266632123326084, discriminator loss: 0.8152061161597002\n",
            "Epoch 125/300, Batch 400/937, generator loss: 1.9262113899361244, discriminator loss: 0.815350360270075\n",
            "Epoch 125/300, Batch 500/937, generator loss: 1.9257195601301083, discriminator loss: 0.8155070463238484\n",
            "Epoch 125/300, Batch 600/937, generator loss: 1.9252956934649044, discriminator loss: 0.8156465117260479\n",
            "Epoch 125/300, Batch 700/937, generator loss: 1.9248037417597494, discriminator loss: 0.8157981522274441\n",
            "Epoch 125/300, Batch 800/937, generator loss: 1.9243719026454176, discriminator loss: 0.8159217834950299\n",
            "Epoch 125/300, Batch 900/937, generator loss: 1.9238685905289994, discriminator loss: 0.8160723117190506\n",
            "Saving results...\n",
            "Epoch 126/300, Batch 0/937, generator loss: 1.923704581721795, discriminator loss: 0.816114329314501\n",
            "Epoch 126/300, Batch 100/937, generator loss: 1.9232547789067806, discriminator loss: 0.8162591421035567\n",
            "Epoch 126/300, Batch 200/937, generator loss: 1.922803505127574, discriminator loss: 0.8163967870508668\n",
            "Epoch 126/300, Batch 300/937, generator loss: 1.9223303761780999, discriminator loss: 0.8165372752671863\n",
            "Epoch 126/300, Batch 400/937, generator loss: 1.9219037162879533, discriminator loss: 0.8166680606655436\n",
            "Epoch 126/300, Batch 500/937, generator loss: 1.921448618978714, discriminator loss: 0.8168090727973386\n",
            "Epoch 126/300, Batch 600/937, generator loss: 1.9209706847102082, discriminator loss: 0.8169580921796316\n",
            "Epoch 126/300, Batch 700/937, generator loss: 1.9205341684127983, discriminator loss: 0.8170891048964685\n",
            "Epoch 126/300, Batch 800/937, generator loss: 1.9200484258083355, discriminator loss: 0.8172380121144496\n",
            "Epoch 126/300, Batch 900/937, generator loss: 1.9195817708082283, discriminator loss: 0.8173811607291477\n",
            "Saving results...\n",
            "Epoch 127/300, Batch 0/937, generator loss: 1.919431778703918, discriminator loss: 0.8174191161687133\n",
            "Epoch 127/300, Batch 100/937, generator loss: 1.9190098434901859, discriminator loss: 0.8175445534232582\n",
            "Epoch 127/300, Batch 200/937, generator loss: 1.9185561745238784, discriminator loss: 0.8176877392210916\n",
            "Epoch 127/300, Batch 300/937, generator loss: 1.918081437610121, discriminator loss: 0.8178290415462185\n",
            "Epoch 127/300, Batch 400/937, generator loss: 1.9176470443841, discriminator loss: 0.8179507012700216\n",
            "Epoch 127/300, Batch 500/937, generator loss: 1.9171751934185188, discriminator loss: 0.8180761748066009\n",
            "Epoch 127/300, Batch 600/937, generator loss: 1.9167338752666843, discriminator loss: 0.8182179449944692\n",
            "Epoch 127/300, Batch 700/937, generator loss: 1.9162331585965757, discriminator loss: 0.8183699611392037\n",
            "Epoch 127/300, Batch 800/937, generator loss: 1.9157879893329983, discriminator loss: 0.818486849772363\n",
            "Epoch 127/300, Batch 900/937, generator loss: 1.9153725544266944, discriminator loss: 0.8186243787900024\n",
            "Saving results...\n",
            "Epoch 128/300, Batch 0/937, generator loss: 1.9151837837473733, discriminator loss: 0.8186966706778078\n",
            "Epoch 128/300, Batch 100/937, generator loss: 1.9147329860563538, discriminator loss: 0.8188351496373948\n",
            "Epoch 128/300, Batch 200/937, generator loss: 1.914289228196985, discriminator loss: 0.8189638623968557\n",
            "Epoch 128/300, Batch 300/937, generator loss: 1.9138172060419536, discriminator loss: 0.8190976199214169\n",
            "Epoch 128/300, Batch 400/937, generator loss: 1.9133989413810453, discriminator loss: 0.8192165141853597\n",
            "Epoch 128/300, Batch 500/937, generator loss: 1.912928771621077, discriminator loss: 0.8193568778674767\n",
            "Epoch 128/300, Batch 600/937, generator loss: 1.9125010936546654, discriminator loss: 0.8194772535555443\n",
            "Epoch 128/300, Batch 700/937, generator loss: 1.9120455747815268, discriminator loss: 0.819616825596042\n",
            "Epoch 128/300, Batch 800/937, generator loss: 1.9115858450825078, discriminator loss: 0.8197577509053795\n",
            "Epoch 128/300, Batch 900/937, generator loss: 1.9111620040066037, discriminator loss: 0.8198870121100894\n",
            "Saving results...\n",
            "Epoch 129/300, Batch 0/937, generator loss: 1.9109980852101687, discriminator loss: 0.8199533203446829\n",
            "Epoch 129/300, Batch 100/937, generator loss: 1.9105529468720595, discriminator loss: 0.820084019802177\n",
            "Epoch 129/300, Batch 200/937, generator loss: 1.9101156973910878, discriminator loss: 0.8202180633593739\n",
            "Epoch 129/300, Batch 300/937, generator loss: 1.9097096449999809, discriminator loss: 0.8203375705931465\n",
            "Epoch 129/300, Batch 400/937, generator loss: 1.9092711661854065, discriminator loss: 0.8204671843741821\n",
            "Epoch 129/300, Batch 500/937, generator loss: 1.9088418557057656, discriminator loss: 0.8205938711564768\n",
            "Epoch 129/300, Batch 600/937, generator loss: 1.908361970637893, discriminator loss: 0.8207443809481112\n",
            "Epoch 129/300, Batch 700/937, generator loss: 1.9079154510077547, discriminator loss: 0.8208769917924933\n",
            "Epoch 129/300, Batch 800/937, generator loss: 1.907442857884486, discriminator loss: 0.8210061323692796\n",
            "Epoch 129/300, Batch 900/937, generator loss: 1.9070046132524259, discriminator loss: 0.8211334210803196\n",
            "Saving results...\n",
            "Epoch 130/300, Batch 0/937, generator loss: 1.9068377881444856, discriminator loss: 0.8211760834352312\n",
            "Epoch 130/300, Batch 100/937, generator loss: 1.9064129934869571, discriminator loss: 0.8212956284077104\n",
            "Epoch 130/300, Batch 200/937, generator loss: 1.9060291305987123, discriminator loss: 0.8214043426482505\n",
            "Epoch 130/300, Batch 300/937, generator loss: 1.905603515782217, discriminator loss: 0.8215499292650646\n",
            "Epoch 130/300, Batch 400/937, generator loss: 1.905192966329439, discriminator loss: 0.8216612385984691\n",
            "Epoch 130/300, Batch 500/937, generator loss: 1.9047972690969348, discriminator loss: 0.8217827436335025\n",
            "Epoch 130/300, Batch 600/937, generator loss: 1.9043803995728845, discriminator loss: 0.8219026409342703\n",
            "Epoch 130/300, Batch 700/937, generator loss: 1.9039619396716263, discriminator loss: 0.8220468915553096\n",
            "Epoch 130/300, Batch 800/937, generator loss: 1.903520446518599, discriminator loss: 0.8221723051661463\n",
            "Epoch 130/300, Batch 900/937, generator loss: 1.9031024896384807, discriminator loss: 0.822305056414597\n",
            "Saving results...\n",
            "Epoch 131/300, Batch 0/937, generator loss: 1.9029396956586635, discriminator loss: 0.8223500421503991\n",
            "Epoch 131/300, Batch 100/937, generator loss: 1.9025270080469951, discriminator loss: 0.8224484567237156\n",
            "Epoch 131/300, Batch 200/937, generator loss: 1.9020645154251818, discriminator loss: 0.822594004949016\n",
            "Epoch 131/300, Batch 300/937, generator loss: 1.9016721741510154, discriminator loss: 0.82269946962191\n",
            "Epoch 131/300, Batch 400/937, generator loss: 1.9012837889184848, discriminator loss: 0.8228241145496099\n",
            "Epoch 131/300, Batch 500/937, generator loss: 1.90083784656847, discriminator loss: 0.8229490493467883\n",
            "Epoch 131/300, Batch 600/937, generator loss: 1.9004260209303954, discriminator loss: 0.8230577393511925\n",
            "Epoch 131/300, Batch 700/937, generator loss: 1.9000196988584466, discriminator loss: 0.8231874391237167\n",
            "Epoch 131/300, Batch 800/937, generator loss: 1.8996200632924733, discriminator loss: 0.8233053935159521\n",
            "Epoch 131/300, Batch 900/937, generator loss: 1.8991866756681792, discriminator loss: 0.8234374124118595\n",
            "Saving results...\n",
            "Epoch 132/300, Batch 0/937, generator loss: 1.8990204770991905, discriminator loss: 0.8234920363861948\n",
            "Epoch 132/300, Batch 100/937, generator loss: 1.8986431469876657, discriminator loss: 0.8236090964122962\n",
            "Epoch 132/300, Batch 200/937, generator loss: 1.8982292474131706, discriminator loss: 0.8237293685380361\n",
            "Epoch 132/300, Batch 300/937, generator loss: 1.897814484641942, discriminator loss: 0.8238432392252641\n",
            "Epoch 132/300, Batch 400/937, generator loss: 1.8973860710831678, discriminator loss: 0.823957367036692\n",
            "Epoch 132/300, Batch 500/937, generator loss: 1.8969539535944602, discriminator loss: 0.824073467880947\n",
            "Epoch 132/300, Batch 600/937, generator loss: 1.8965481848959787, discriminator loss: 0.8241936087006464\n",
            "Epoch 132/300, Batch 700/937, generator loss: 1.8961149121311243, discriminator loss: 0.8243061167811803\n",
            "Epoch 132/300, Batch 800/937, generator loss: 1.8957017878502225, discriminator loss: 0.8244467359834173\n",
            "Epoch 132/300, Batch 900/937, generator loss: 1.895311523852601, discriminator loss: 0.8245634691763657\n",
            "Saving results...\n",
            "Epoch 133/300, Batch 0/937, generator loss: 1.8951545837479895, discriminator loss: 0.8246083526146647\n",
            "Epoch 133/300, Batch 100/937, generator loss: 1.894739739507118, discriminator loss: 0.8247297732165021\n",
            "Epoch 133/300, Batch 200/937, generator loss: 1.8943587851288268, discriminator loss: 0.824845128600481\n",
            "Epoch 133/300, Batch 300/937, generator loss: 1.8939397662376567, discriminator loss: 0.8249602474436744\n",
            "Epoch 133/300, Batch 400/937, generator loss: 1.8935431497052124, discriminator loss: 0.8250733937976555\n",
            "Epoch 133/300, Batch 500/937, generator loss: 1.8931034572159429, discriminator loss: 0.8251955522459903\n",
            "Epoch 133/300, Batch 600/937, generator loss: 1.8926932305557864, discriminator loss: 0.8253203987886287\n",
            "Epoch 133/300, Batch 700/937, generator loss: 1.892279365871216, discriminator loss: 0.8254385971677811\n",
            "Epoch 133/300, Batch 800/937, generator loss: 1.8918860938017883, discriminator loss: 0.8255662183578176\n",
            "Epoch 133/300, Batch 900/937, generator loss: 1.8914961356783808, discriminator loss: 0.8256864998282998\n",
            "Saving results...\n",
            "Epoch 134/300, Batch 0/937, generator loss: 1.8913255851500372, discriminator loss: 0.8257538346532514\n",
            "Epoch 134/300, Batch 100/937, generator loss: 1.8909220822375865, discriminator loss: 0.8258741451359081\n",
            "Epoch 134/300, Batch 200/937, generator loss: 1.8905215966679612, discriminator loss: 0.825979901329212\n",
            "Epoch 134/300, Batch 300/937, generator loss: 1.8901297640246952, discriminator loss: 0.8260954033373339\n",
            "Epoch 134/300, Batch 400/937, generator loss: 1.8897182898464866, discriminator loss: 0.826202562210666\n",
            "Epoch 134/300, Batch 500/937, generator loss: 1.8893263214435978, discriminator loss: 0.8263218192048336\n",
            "Epoch 134/300, Batch 600/937, generator loss: 1.8889473715805645, discriminator loss: 0.8264403381018467\n",
            "Epoch 134/300, Batch 700/937, generator loss: 1.8885401140667353, discriminator loss: 0.8265677599821324\n",
            "Epoch 134/300, Batch 800/937, generator loss: 1.888172491320146, discriminator loss: 0.8266842815838723\n",
            "Epoch 134/300, Batch 900/937, generator loss: 1.8877761956621846, discriminator loss: 0.8268023491457696\n",
            "Saving results...\n",
            "Epoch 135/300, Batch 0/937, generator loss: 1.8876264137619965, discriminator loss: 0.8268504367553239\n",
            "Epoch 135/300, Batch 100/937, generator loss: 1.8872446325813907, discriminator loss: 0.826968423162941\n",
            "Epoch 135/300, Batch 200/937, generator loss: 1.8868570267376013, discriminator loss: 0.8270781890031585\n",
            "Epoch 135/300, Batch 300/937, generator loss: 1.8864394670116769, discriminator loss: 0.8271908701326878\n",
            "Epoch 135/300, Batch 400/937, generator loss: 1.886044904060287, discriminator loss: 0.8273090859650796\n",
            "Epoch 135/300, Batch 500/937, generator loss: 1.8856630578269629, discriminator loss: 0.8274180998432512\n",
            "Epoch 135/300, Batch 600/937, generator loss: 1.8852646500130144, discriminator loss: 0.8275393344745178\n",
            "Epoch 135/300, Batch 700/937, generator loss: 1.8848819484164938, discriminator loss: 0.82765344445714\n",
            "Epoch 135/300, Batch 800/937, generator loss: 1.8844868260365055, discriminator loss: 0.8277684704320062\n",
            "Epoch 135/300, Batch 900/937, generator loss: 1.8841007630023452, discriminator loss: 0.8278820968444087\n",
            "Saving results...\n",
            "Epoch 136/300, Batch 0/937, generator loss: 1.8839490798759868, discriminator loss: 0.8279272146445764\n",
            "Epoch 136/300, Batch 100/937, generator loss: 1.8835465475325293, discriminator loss: 0.8280515646527059\n",
            "Epoch 136/300, Batch 200/937, generator loss: 1.8831872736283242, discriminator loss: 0.8281435731625592\n",
            "Epoch 136/300, Batch 300/937, generator loss: 1.8828013399907166, discriminator loss: 0.8282516489338645\n",
            "Epoch 136/300, Batch 400/937, generator loss: 1.8824264427585171, discriminator loss: 0.8283626877422654\n",
            "Epoch 136/300, Batch 500/937, generator loss: 1.882058907028349, discriminator loss: 0.828467872832788\n",
            "Epoch 136/300, Batch 600/937, generator loss: 1.8816596134954389, discriminator loss: 0.8285972743810495\n",
            "Epoch 136/300, Batch 700/937, generator loss: 1.881269991877456, discriminator loss: 0.8287137413973772\n",
            "Epoch 136/300, Batch 800/937, generator loss: 1.8808957280472935, discriminator loss: 0.8288227384463838\n",
            "Epoch 136/300, Batch 900/937, generator loss: 1.8805023442562552, discriminator loss: 0.8289364396032106\n",
            "Saving results...\n",
            "Epoch 137/300, Batch 0/937, generator loss: 1.8803407311499876, discriminator loss: 0.8289797265124096\n",
            "Epoch 137/300, Batch 100/937, generator loss: 1.8799626980508415, discriminator loss: 0.8290810402586849\n",
            "Epoch 137/300, Batch 200/937, generator loss: 1.8795948592488276, discriminator loss: 0.8291986710481254\n",
            "Epoch 137/300, Batch 300/937, generator loss: 1.8791993059584466, discriminator loss: 0.829310134156532\n",
            "Epoch 137/300, Batch 400/937, generator loss: 1.8788122887033534, discriminator loss: 0.8294164893884434\n",
            "Epoch 137/300, Batch 500/937, generator loss: 1.8784473752058755, discriminator loss: 0.8295179800044356\n",
            "Epoch 137/300, Batch 600/937, generator loss: 1.8781130599103955, discriminator loss: 0.8296131495499303\n",
            "Epoch 137/300, Batch 700/937, generator loss: 1.8777148708330538, discriminator loss: 0.8297281483462033\n",
            "Epoch 137/300, Batch 800/937, generator loss: 1.8773610518966526, discriminator loss: 0.8298473691807874\n",
            "Epoch 137/300, Batch 900/937, generator loss: 1.8770034253182282, discriminator loss: 0.8299654784531884\n",
            "Saving results...\n",
            "Epoch 138/300, Batch 0/937, generator loss: 1.8768789023348773, discriminator loss: 0.8300015217323162\n",
            "Epoch 138/300, Batch 100/937, generator loss: 1.8764786525028259, discriminator loss: 0.8301083408892295\n",
            "Epoch 138/300, Batch 200/937, generator loss: 1.876109527032153, discriminator loss: 0.8302123444309112\n",
            "Epoch 138/300, Batch 300/937, generator loss: 1.875735081783608, discriminator loss: 0.8303115747358712\n",
            "Epoch 138/300, Batch 400/937, generator loss: 1.8753770537800167, discriminator loss: 0.8304273326787435\n",
            "Epoch 138/300, Batch 500/937, generator loss: 1.8750396822214768, discriminator loss: 0.8305151179790226\n",
            "Epoch 138/300, Batch 600/937, generator loss: 1.8746680127077406, discriminator loss: 0.830619157661626\n",
            "Epoch 138/300, Batch 700/937, generator loss: 1.8743132401495843, discriminator loss: 0.8307262514005275\n",
            "Epoch 138/300, Batch 800/937, generator loss: 1.8739301167290543, discriminator loss: 0.8308474320936904\n",
            "Epoch 138/300, Batch 900/937, generator loss: 1.8735753232834722, discriminator loss: 0.8309439955453627\n",
            "Saving results...\n",
            "Epoch 139/300, Batch 0/937, generator loss: 1.8734335614596738, discriminator loss: 0.8309885447919286\n",
            "Epoch 139/300, Batch 100/937, generator loss: 1.873056875354204, discriminator loss: 0.8310862659682305\n",
            "Epoch 139/300, Batch 200/937, generator loss: 1.872713157411409, discriminator loss: 0.8311734418947191\n",
            "Epoch 139/300, Batch 300/937, generator loss: 1.872351173596382, discriminator loss: 0.8312824138902828\n",
            "Epoch 139/300, Batch 400/937, generator loss: 1.8719867674459862, discriminator loss: 0.8313842726139852\n",
            "Epoch 139/300, Batch 500/937, generator loss: 1.871623570495298, discriminator loss: 0.8314941799623605\n",
            "Epoch 139/300, Batch 600/937, generator loss: 1.8712815520094555, discriminator loss: 0.8315873740784789\n",
            "Epoch 139/300, Batch 700/937, generator loss: 1.870919607409011, discriminator loss: 0.8317031790959837\n",
            "Epoch 139/300, Batch 800/937, generator loss: 1.8705479783776378, discriminator loss: 0.8318121795885122\n",
            "Epoch 139/300, Batch 900/937, generator loss: 1.870184606957547, discriminator loss: 0.8319185558236799\n",
            "Saving results...\n",
            "Epoch 140/300, Batch 0/937, generator loss: 1.870068322855597, discriminator loss: 0.8319598758528554\n",
            "Epoch 140/300, Batch 100/937, generator loss: 1.869690438346656, discriminator loss: 0.8320577614966065\n",
            "Epoch 140/300, Batch 200/937, generator loss: 1.8693415754563527, discriminator loss: 0.8321538866314822\n",
            "Epoch 140/300, Batch 300/937, generator loss: 1.868987224349014, discriminator loss: 0.8322478164859469\n",
            "Epoch 140/300, Batch 400/937, generator loss: 1.8686376256979749, discriminator loss: 0.8323442745400897\n",
            "Epoch 140/300, Batch 500/937, generator loss: 1.8683021514504514, discriminator loss: 0.8324468509493815\n",
            "Epoch 140/300, Batch 600/937, generator loss: 1.867992985899517, discriminator loss: 0.8325550480442839\n",
            "Epoch 140/300, Batch 700/937, generator loss: 1.8676489639700378, discriminator loss: 0.8326532242745673\n",
            "Epoch 140/300, Batch 800/937, generator loss: 1.8673229199135004, discriminator loss: 0.8327581857310542\n",
            "Epoch 140/300, Batch 900/937, generator loss: 1.86699365265745, discriminator loss: 0.8328615358489087\n",
            "Saving results...\n",
            "Epoch 141/300, Batch 0/937, generator loss: 1.866859864710112, discriminator loss: 0.8329057627557583\n",
            "Epoch 141/300, Batch 100/937, generator loss: 1.866539296712456, discriminator loss: 0.8329903683393067\n",
            "Epoch 141/300, Batch 200/937, generator loss: 1.866177038284215, discriminator loss: 0.8330932372412407\n",
            "Epoch 141/300, Batch 300/937, generator loss: 1.8658363952228763, discriminator loss: 0.8331951037240372\n",
            "Epoch 141/300, Batch 400/937, generator loss: 1.8654917353813456, discriminator loss: 0.8332742347481904\n",
            "Epoch 141/300, Batch 500/937, generator loss: 1.8651379847193421, discriminator loss: 0.8333877730119047\n",
            "Epoch 141/300, Batch 600/937, generator loss: 1.8648076510875435, discriminator loss: 0.8334776824292928\n",
            "Epoch 141/300, Batch 700/937, generator loss: 1.8644703451805622, discriminator loss: 0.8335924244037489\n",
            "Epoch 141/300, Batch 800/937, generator loss: 1.8641090002074323, discriminator loss: 0.8336759604402995\n",
            "Epoch 141/300, Batch 900/937, generator loss: 1.8637657003097539, discriminator loss: 0.833777911754619\n",
            "Saving results...\n",
            "Epoch 142/300, Batch 0/937, generator loss: 1.8636432613323384, discriminator loss: 0.833813024114843\n",
            "Epoch 142/300, Batch 100/937, generator loss: 1.863311679689307, discriminator loss: 0.8338923262435645\n",
            "Epoch 142/300, Batch 200/937, generator loss: 1.8629934370998575, discriminator loss: 0.8339677949722594\n",
            "Epoch 142/300, Batch 300/937, generator loss: 1.8626650591002267, discriminator loss: 0.8340862381429677\n",
            "Epoch 142/300, Batch 400/937, generator loss: 1.862347430737306, discriminator loss: 0.8341821019021369\n",
            "Epoch 142/300, Batch 500/937, generator loss: 1.8620126489948332, discriminator loss: 0.8342741830393776\n",
            "Epoch 142/300, Batch 600/937, generator loss: 1.861697006148881, discriminator loss: 0.8343771298453622\n",
            "Epoch 142/300, Batch 700/937, generator loss: 1.8613560591486966, discriminator loss: 0.8344751173549936\n",
            "Epoch 142/300, Batch 800/937, generator loss: 1.861023817967747, discriminator loss: 0.8345886069185986\n",
            "Epoch 142/300, Batch 900/937, generator loss: 1.8606807822374172, discriminator loss: 0.8346953195776884\n",
            "Saving results...\n",
            "Epoch 143/300, Batch 0/937, generator loss: 1.8605542988220296, discriminator loss: 0.8347350063953561\n",
            "Epoch 143/300, Batch 100/937, generator loss: 1.8602133543912827, discriminator loss: 0.8348266676777737\n",
            "Epoch 143/300, Batch 200/937, generator loss: 1.8598776229469458, discriminator loss: 0.8349266713628368\n",
            "Epoch 143/300, Batch 300/937, generator loss: 1.8595579704284468, discriminator loss: 0.8350135042581691\n",
            "Epoch 143/300, Batch 400/937, generator loss: 1.8592624344032354, discriminator loss: 0.8350927187253484\n",
            "Epoch 143/300, Batch 500/937, generator loss: 1.858935967875726, discriminator loss: 0.8351917131610452\n",
            "Epoch 143/300, Batch 600/937, generator loss: 1.8586019914525531, discriminator loss: 0.8352853895498938\n",
            "Epoch 143/300, Batch 700/937, generator loss: 1.8582590611989132, discriminator loss: 0.8353753459610502\n",
            "Epoch 143/300, Batch 800/937, generator loss: 1.8579316152721472, discriminator loss: 0.8354859074865227\n",
            "Epoch 143/300, Batch 900/937, generator loss: 1.8576043753184674, discriminator loss: 0.8355938443388287\n",
            "Saving results...\n",
            "Epoch 144/300, Batch 0/937, generator loss: 1.8574665817538436, discriminator loss: 0.8356316516802859\n",
            "Epoch 144/300, Batch 100/937, generator loss: 1.8571576196377408, discriminator loss: 0.8357085025225587\n",
            "Epoch 144/300, Batch 200/937, generator loss: 1.856845833374547, discriminator loss: 0.8357984000184564\n",
            "Epoch 144/300, Batch 300/937, generator loss: 1.8565168351192611, discriminator loss: 0.8358941239390398\n",
            "Epoch 144/300, Batch 400/937, generator loss: 1.8561660865420126, discriminator loss: 0.8359821523330105\n",
            "Epoch 144/300, Batch 500/937, generator loss: 1.855803626406175, discriminator loss: 0.8360791733670992\n",
            "Epoch 144/300, Batch 600/937, generator loss: 1.8555071548349038, discriminator loss: 0.8361584733708225\n",
            "Epoch 144/300, Batch 700/937, generator loss: 1.8551733227834328, discriminator loss: 0.8362612622587066\n",
            "Epoch 144/300, Batch 800/937, generator loss: 1.8548510790442982, discriminator loss: 0.836346563987553\n",
            "Epoch 144/300, Batch 900/937, generator loss: 1.8545110559616036, discriminator loss: 0.8364665622883779\n",
            "Saving results...\n",
            "Epoch 145/300, Batch 0/937, generator loss: 1.8543875252336464, discriminator loss: 0.8365059721261144\n",
            "Epoch 145/300, Batch 100/937, generator loss: 1.8540684513095274, discriminator loss: 0.8365828729769773\n",
            "Epoch 145/300, Batch 200/937, generator loss: 1.853751480209218, discriminator loss: 0.8366697088862788\n",
            "Epoch 145/300, Batch 300/937, generator loss: 1.853428794623801, discriminator loss: 0.8367647917797115\n",
            "Epoch 145/300, Batch 400/937, generator loss: 1.853129675808936, discriminator loss: 0.8368345419435924\n",
            "Epoch 145/300, Batch 500/937, generator loss: 1.852811137566081, discriminator loss: 0.8369143226488837\n",
            "Epoch 145/300, Batch 600/937, generator loss: 1.8524740239095074, discriminator loss: 0.8370248829249842\n",
            "Epoch 145/300, Batch 700/937, generator loss: 1.8521594967897514, discriminator loss: 0.8371137867269112\n",
            "Epoch 145/300, Batch 800/937, generator loss: 1.8518329195599403, discriminator loss: 0.8372030154250237\n",
            "Epoch 145/300, Batch 900/937, generator loss: 1.8515346356304667, discriminator loss: 0.8372917734990284\n",
            "Saving results...\n",
            "Epoch 146/300, Batch 0/937, generator loss: 1.8514034105298094, discriminator loss: 0.8373398826089484\n",
            "Epoch 146/300, Batch 100/937, generator loss: 1.8510765351617011, discriminator loss: 0.8374189503954222\n",
            "Epoch 146/300, Batch 200/937, generator loss: 1.8507732640011243, discriminator loss: 0.8374949032071863\n",
            "Epoch 146/300, Batch 300/937, generator loss: 1.8504632877765141, discriminator loss: 0.8375747445489975\n",
            "Epoch 146/300, Batch 400/937, generator loss: 1.8501658505838514, discriminator loss: 0.8376590495476545\n",
            "Epoch 146/300, Batch 500/937, generator loss: 1.8498328846738143, discriminator loss: 0.837755125844989\n",
            "Epoch 146/300, Batch 600/937, generator loss: 1.8495139759234955, discriminator loss: 0.8378535811349203\n",
            "Epoch 146/300, Batch 700/937, generator loss: 1.8492042736852892, discriminator loss: 0.8379337310703886\n",
            "Epoch 146/300, Batch 800/937, generator loss: 1.8489100910190905, discriminator loss: 0.8380143756834938\n",
            "Epoch 146/300, Batch 900/937, generator loss: 1.848584661293536, discriminator loss: 0.8381018438922316\n",
            "Saving results...\n",
            "Epoch 147/300, Batch 0/937, generator loss: 1.8484638378679692, discriminator loss: 0.83813468210688\n",
            "Epoch 147/300, Batch 100/937, generator loss: 1.8481450286106322, discriminator loss: 0.8382344207937764\n",
            "Epoch 147/300, Batch 200/937, generator loss: 1.8478530166865397, discriminator loss: 0.8383244674977434\n",
            "Epoch 147/300, Batch 300/937, generator loss: 1.8475677270797397, discriminator loss: 0.8383985539040628\n",
            "Epoch 147/300, Batch 400/937, generator loss: 1.847235050863479, discriminator loss: 0.8384934449803817\n",
            "Epoch 147/300, Batch 500/937, generator loss: 1.8469216695598638, discriminator loss: 0.8385750731408027\n",
            "Epoch 147/300, Batch 600/937, generator loss: 1.8466106749113471, discriminator loss: 0.8386483577372918\n",
            "Epoch 147/300, Batch 700/937, generator loss: 1.8463071046317978, discriminator loss: 0.8387490940165895\n",
            "Epoch 147/300, Batch 800/937, generator loss: 1.84602515559245, discriminator loss: 0.8388284680256772\n",
            "Epoch 147/300, Batch 900/937, generator loss: 1.8457124285804043, discriminator loss: 0.8389332341998643\n",
            "Saving results...\n",
            "Epoch 148/300, Batch 0/937, generator loss: 1.8456166001417937, discriminator loss: 0.8389574638741703\n",
            "Epoch 148/300, Batch 100/937, generator loss: 1.8453128124368818, discriminator loss: 0.8390232623452377\n",
            "Epoch 148/300, Batch 200/937, generator loss: 1.8450203656757989, discriminator loss: 0.8391031733366648\n",
            "Epoch 148/300, Batch 300/937, generator loss: 1.8447134776269196, discriminator loss: 0.8391900340233839\n",
            "Epoch 148/300, Batch 400/937, generator loss: 1.84439959051126, discriminator loss: 0.8392693517059847\n",
            "Epoch 148/300, Batch 500/937, generator loss: 1.8441098551496924, discriminator loss: 0.8393542008282807\n",
            "Epoch 148/300, Batch 600/937, generator loss: 1.843830623377518, discriminator loss: 0.8394300723286507\n",
            "Epoch 148/300, Batch 700/937, generator loss: 1.843523187299297, discriminator loss: 0.8395175372266489\n",
            "Epoch 148/300, Batch 800/937, generator loss: 1.843224116157487, discriminator loss: 0.839606716867036\n",
            "Epoch 148/300, Batch 900/937, generator loss: 1.8429279472504267, discriminator loss: 0.8396878254306674\n",
            "Saving results...\n",
            "Epoch 149/300, Batch 0/937, generator loss: 1.8428176233729938, discriminator loss: 0.8397238702939575\n",
            "Epoch 149/300, Batch 100/937, generator loss: 1.8425443839964228, discriminator loss: 0.8397955346706371\n",
            "Epoch 149/300, Batch 200/937, generator loss: 1.8422475558121272, discriminator loss: 0.8398698471764845\n",
            "Epoch 149/300, Batch 300/937, generator loss: 1.8419637364197743, discriminator loss: 0.8399451639893731\n",
            "Epoch 149/300, Batch 400/937, generator loss: 1.8416741526153084, discriminator loss: 0.840019879074234\n",
            "Epoch 149/300, Batch 500/937, generator loss: 1.8413690343823126, discriminator loss: 0.8400995872180655\n",
            "Epoch 149/300, Batch 600/937, generator loss: 1.8410769086648624, discriminator loss: 0.8401774837889812\n",
            "Epoch 149/300, Batch 700/937, generator loss: 1.8407686419401328, discriminator loss: 0.8402589193260949\n",
            "Epoch 149/300, Batch 800/937, generator loss: 1.8404633600722655, discriminator loss: 0.8403488691675836\n",
            "Epoch 149/300, Batch 900/937, generator loss: 1.8401848888460675, discriminator loss: 0.8404251299241209\n",
            "Saving results...\n",
            "Epoch 150/300, Batch 0/937, generator loss: 1.8400662948966986, discriminator loss: 0.8404606455141311\n",
            "Epoch 150/300, Batch 100/937, generator loss: 1.8397912200571531, discriminator loss: 0.8405354251410317\n",
            "Epoch 150/300, Batch 200/937, generator loss: 1.8395044675941425, discriminator loss: 0.8406114055531813\n",
            "Epoch 150/300, Batch 300/937, generator loss: 1.8392050333076764, discriminator loss: 0.8406820934215045\n",
            "Epoch 150/300, Batch 400/937, generator loss: 1.838921802974967, discriminator loss: 0.8407616423202358\n",
            "Epoch 150/300, Batch 500/937, generator loss: 1.8386151249012803, discriminator loss: 0.8408435232370927\n",
            "Epoch 150/300, Batch 600/937, generator loss: 1.8383160052294871, discriminator loss: 0.8409168373301491\n",
            "Epoch 150/300, Batch 700/937, generator loss: 1.8380070052591946, discriminator loss: 0.840990030032602\n",
            "Epoch 150/300, Batch 800/937, generator loss: 1.837699712810887, discriminator loss: 0.8410719039765252\n",
            "Epoch 150/300, Batch 900/937, generator loss: 1.8374032413023786, discriminator loss: 0.8411604452976024\n",
            "Saving results...\n",
            "Epoch 151/300, Batch 0/937, generator loss: 1.8372909080127198, discriminator loss: 0.841190753439337\n",
            "Epoch 151/300, Batch 100/937, generator loss: 1.8370160679032574, discriminator loss: 0.8412556921669675\n",
            "Epoch 151/300, Batch 200/937, generator loss: 1.8367536717056252, discriminator loss: 0.8413174140799387\n",
            "Epoch 151/300, Batch 300/937, generator loss: 1.8364922164555721, discriminator loss: 0.841385590103228\n",
            "Epoch 151/300, Batch 400/937, generator loss: 1.8362191692305527, discriminator loss: 0.8414659213212125\n",
            "Epoch 151/300, Batch 500/937, generator loss: 1.8359225663124892, discriminator loss: 0.8415464549796223\n",
            "Epoch 151/300, Batch 600/937, generator loss: 1.83561724848174, discriminator loss: 0.8416207763530265\n",
            "Epoch 151/300, Batch 700/937, generator loss: 1.8353568933388351, discriminator loss: 0.8416844949605804\n",
            "Epoch 151/300, Batch 800/937, generator loss: 1.835096668950517, discriminator loss: 0.8417658156359372\n",
            "Epoch 151/300, Batch 900/937, generator loss: 1.8348264114848918, discriminator loss: 0.8418368822290234\n",
            "Saving results...\n",
            "Epoch 152/300, Batch 0/937, generator loss: 1.8347318112764397, discriminator loss: 0.8418664398717738\n",
            "Epoch 152/300, Batch 100/937, generator loss: 1.8344431027465276, discriminator loss: 0.8419321731639899\n",
            "Epoch 152/300, Batch 200/937, generator loss: 1.8341629210717838, discriminator loss: 0.8419949622440506\n",
            "Epoch 152/300, Batch 300/937, generator loss: 1.8338540979637785, discriminator loss: 0.8420883376676871\n",
            "Epoch 152/300, Batch 400/937, generator loss: 1.833545078321908, discriminator loss: 0.8421554743150227\n",
            "Epoch 152/300, Batch 500/937, generator loss: 1.833255646854759, discriminator loss: 0.8422320955248381\n",
            "Epoch 152/300, Batch 600/937, generator loss: 1.8329827548857023, discriminator loss: 0.842304178086679\n",
            "Epoch 152/300, Batch 700/937, generator loss: 1.8326982141661332, discriminator loss: 0.8423952389244429\n",
            "Epoch 152/300, Batch 800/937, generator loss: 1.8324283001272648, discriminator loss: 0.8424764607836593\n",
            "Epoch 152/300, Batch 900/937, generator loss: 1.832136498726872, discriminator loss: 0.8425584403106144\n",
            "Saving results...\n",
            "Epoch 153/300, Batch 0/937, generator loss: 1.8320344569803446, discriminator loss: 0.8425945770444735\n",
            "Epoch 153/300, Batch 100/937, generator loss: 1.8317751329587415, discriminator loss: 0.8426626137389378\n",
            "Epoch 153/300, Batch 200/937, generator loss: 1.83153120298859, discriminator loss: 0.8427283667747568\n",
            "Epoch 153/300, Batch 300/937, generator loss: 1.831280781597551, discriminator loss: 0.8427942057114409\n",
            "Epoch 153/300, Batch 400/937, generator loss: 1.8310485774073908, discriminator loss: 0.8428598006581429\n",
            "Epoch 153/300, Batch 500/937, generator loss: 1.8307976616809267, discriminator loss: 0.8429378276631981\n",
            "Epoch 153/300, Batch 600/937, generator loss: 1.8305417735534464, discriminator loss: 0.843007490654535\n",
            "Epoch 153/300, Batch 700/937, generator loss: 1.8302882462245522, discriminator loss: 0.8430863834628028\n",
            "Epoch 153/300, Batch 800/937, generator loss: 1.8300184652936344, discriminator loss: 0.8431691934651814\n",
            "Epoch 153/300, Batch 900/937, generator loss: 1.8297491376759054, discriminator loss: 0.843248335564732\n",
            "Saving results...\n",
            "Epoch 154/300, Batch 0/937, generator loss: 1.8296631777106187, discriminator loss: 0.8432742558703015\n",
            "Epoch 154/300, Batch 100/937, generator loss: 1.8293885108664238, discriminator loss: 0.8433545016256472\n",
            "Epoch 154/300, Batch 200/937, generator loss: 1.8291343963900493, discriminator loss: 0.8434170461486621\n",
            "Epoch 154/300, Batch 300/937, generator loss: 1.8288631830632014, discriminator loss: 0.8434861998644063\n",
            "Epoch 154/300, Batch 400/937, generator loss: 1.828584801234695, discriminator loss: 0.843558983665535\n",
            "Epoch 154/300, Batch 500/937, generator loss: 1.8283470845535152, discriminator loss: 0.8436378749239154\n",
            "Epoch 154/300, Batch 600/937, generator loss: 1.8280691145247954, discriminator loss: 0.8437096758214471\n",
            "Epoch 154/300, Batch 700/937, generator loss: 1.827814310961963, discriminator loss: 0.8437749384480911\n",
            "Epoch 154/300, Batch 800/937, generator loss: 1.8275632305173493, discriminator loss: 0.8438425780807925\n",
            "Epoch 154/300, Batch 900/937, generator loss: 1.8272918263137936, discriminator loss: 0.8439125614117935\n",
            "Saving results...\n",
            "Epoch 155/300, Batch 0/937, generator loss: 1.8271905530673287, discriminator loss: 0.8439427453095719\n",
            "Epoch 155/300, Batch 100/937, generator loss: 1.826944725161956, discriminator loss: 0.8440059586133591\n",
            "Epoch 155/300, Batch 200/937, generator loss: 1.8266937219383312, discriminator loss: 0.8440721541253391\n",
            "Epoch 155/300, Batch 300/937, generator loss: 1.8264388011038637, discriminator loss: 0.8441406435416332\n",
            "Epoch 155/300, Batch 400/937, generator loss: 1.8261925656726121, discriminator loss: 0.8442022113006766\n",
            "Epoch 155/300, Batch 500/937, generator loss: 1.825941980626601, discriminator loss: 0.8442769658740488\n",
            "Epoch 155/300, Batch 600/937, generator loss: 1.8256700570580433, discriminator loss: 0.8443402001464267\n",
            "Epoch 155/300, Batch 700/937, generator loss: 1.8253953913711733, discriminator loss: 0.8444094371398578\n",
            "Epoch 155/300, Batch 800/937, generator loss: 1.8251378318512885, discriminator loss: 0.8444970001323286\n",
            "Epoch 155/300, Batch 900/937, generator loss: 1.8248915175600953, discriminator loss: 0.8445537634085404\n",
            "Saving results...\n",
            "Epoch 156/300, Batch 0/937, generator loss: 1.824780272476228, discriminator loss: 0.8445904324109489\n",
            "Epoch 156/300, Batch 100/937, generator loss: 1.824567704285486, discriminator loss: 0.8446399780515352\n",
            "Epoch 156/300, Batch 200/937, generator loss: 1.8243292479986202, discriminator loss: 0.8447053709813739\n",
            "Epoch 156/300, Batch 300/937, generator loss: 1.8240873882274617, discriminator loss: 0.8447618285354409\n",
            "Epoch 156/300, Batch 400/937, generator loss: 1.823815908174952, discriminator loss: 0.8448356308540291\n",
            "Epoch 156/300, Batch 500/937, generator loss: 1.8235479536030543, discriminator loss: 0.8449019988444941\n",
            "Epoch 156/300, Batch 600/937, generator loss: 1.8233081754378364, discriminator loss: 0.8449686494730421\n",
            "Epoch 156/300, Batch 700/937, generator loss: 1.8230633971736958, discriminator loss: 0.8450348457027061\n",
            "Epoch 156/300, Batch 800/937, generator loss: 1.8228058030903265, discriminator loss: 0.8450930235109279\n",
            "Epoch 156/300, Batch 900/937, generator loss: 1.822539367869615, discriminator loss: 0.8451756197775875\n",
            "Saving results...\n",
            "Epoch 157/300, Batch 0/937, generator loss: 1.822443422814714, discriminator loss: 0.8451972457322668\n",
            "Epoch 157/300, Batch 100/937, generator loss: 1.822203269739545, discriminator loss: 0.8452454550830609\n",
            "Epoch 157/300, Batch 200/937, generator loss: 1.8219382928532242, discriminator loss: 0.8452989065898394\n",
            "Epoch 157/300, Batch 300/937, generator loss: 1.8216940186105552, discriminator loss: 0.8453541905856441\n",
            "Epoch 157/300, Batch 400/937, generator loss: 1.8214299858812477, discriminator loss: 0.8454305403489645\n",
            "Epoch 157/300, Batch 500/937, generator loss: 1.8211988029005275, discriminator loss: 0.8454865786638955\n",
            "Epoch 157/300, Batch 600/937, generator loss: 1.8209512113350583, discriminator loss: 0.8455514295219722\n",
            "Epoch 157/300, Batch 700/937, generator loss: 1.82068983795283, discriminator loss: 0.8456261486464284\n",
            "Epoch 157/300, Batch 800/937, generator loss: 1.8204558190693518, discriminator loss: 0.8456875063519749\n",
            "Epoch 157/300, Batch 900/937, generator loss: 1.8202165645789024, discriminator loss: 0.8457575577147932\n",
            "Saving results...\n",
            "Epoch 158/300, Batch 0/937, generator loss: 1.8201176854855, discriminator loss: 0.8457830181027021\n",
            "Epoch 158/300, Batch 100/937, generator loss: 1.8198835470243881, discriminator loss: 0.8458552326116966\n",
            "Epoch 158/300, Batch 200/937, generator loss: 1.8196417245992595, discriminator loss: 0.8459118411060632\n",
            "Epoch 158/300, Batch 300/937, generator loss: 1.819392109418753, discriminator loss: 0.8459835963346778\n",
            "Epoch 158/300, Batch 400/937, generator loss: 1.8191732779951597, discriminator loss: 0.8460410693715651\n",
            "Epoch 158/300, Batch 500/937, generator loss: 1.818938444500346, discriminator loss: 0.8461114924194375\n",
            "Epoch 158/300, Batch 600/937, generator loss: 1.8186988248859044, discriminator loss: 0.8461798682698707\n",
            "Epoch 158/300, Batch 700/937, generator loss: 1.8184576288080119, discriminator loss: 0.8462544649406032\n",
            "Epoch 158/300, Batch 800/937, generator loss: 1.8182083918729446, discriminator loss: 0.8463097771427806\n",
            "Epoch 158/300, Batch 900/937, generator loss: 1.817958662161159, discriminator loss: 0.8463747460293478\n",
            "Saving results...\n",
            "Epoch 159/300, Batch 0/937, generator loss: 1.817863964559757, discriminator loss: 0.8464034730939097\n",
            "Epoch 159/300, Batch 100/937, generator loss: 1.817643814035371, discriminator loss: 0.8464463726357747\n",
            "Epoch 159/300, Batch 200/937, generator loss: 1.8174118626082068, discriminator loss: 0.8465047527077847\n",
            "Epoch 159/300, Batch 300/937, generator loss: 1.8171731471661627, discriminator loss: 0.8465615010189906\n",
            "Epoch 159/300, Batch 400/937, generator loss: 1.816940626163922, discriminator loss: 0.8466147468511868\n",
            "Epoch 159/300, Batch 500/937, generator loss: 1.8167007227701217, discriminator loss: 0.8466837824159325\n",
            "Epoch 159/300, Batch 600/937, generator loss: 1.8164658028441536, discriminator loss: 0.8467270501305829\n",
            "Epoch 159/300, Batch 700/937, generator loss: 1.816217447186293, discriminator loss: 0.8467946430973706\n",
            "Epoch 159/300, Batch 800/937, generator loss: 1.8160028390768232, discriminator loss: 0.846858564499235\n",
            "Epoch 159/300, Batch 900/937, generator loss: 1.815772849712595, discriminator loss: 0.8469239651076356\n",
            "Saving results...\n",
            "Epoch 160/300, Batch 0/937, generator loss: 1.8156917022368706, discriminator loss: 0.8469535918590844\n",
            "Epoch 160/300, Batch 100/937, generator loss: 1.8154589256309577, discriminator loss: 0.8470199715928577\n",
            "Epoch 160/300, Batch 200/937, generator loss: 1.8152424620710599, discriminator loss: 0.847067925181492\n",
            "Epoch 160/300, Batch 300/937, generator loss: 1.8150359267946314, discriminator loss: 0.8471275357487028\n",
            "Epoch 160/300, Batch 400/937, generator loss: 1.8148099562252809, discriminator loss: 0.847193591417858\n",
            "Epoch 160/300, Batch 500/937, generator loss: 1.8145736641985482, discriminator loss: 0.847252428661023\n",
            "Epoch 160/300, Batch 600/937, generator loss: 1.814356158953658, discriminator loss: 0.8473061399874883\n",
            "Epoch 160/300, Batch 700/937, generator loss: 1.8141344867166331, discriminator loss: 0.8473723379809776\n",
            "Epoch 160/300, Batch 800/937, generator loss: 1.8138831575092536, discriminator loss: 0.8474386689452483\n",
            "Epoch 160/300, Batch 900/937, generator loss: 1.8136278085310855, discriminator loss: 0.847497955224779\n",
            "Saving results...\n",
            "Epoch 161/300, Batch 0/937, generator loss: 1.8135595483628282, discriminator loss: 0.8475301494744688\n",
            "Epoch 161/300, Batch 100/937, generator loss: 1.8133293904907906, discriminator loss: 0.847577860095484\n",
            "Epoch 161/300, Batch 200/937, generator loss: 1.8131168275871496, discriminator loss: 0.8476319461409869\n",
            "Epoch 161/300, Batch 300/937, generator loss: 1.8128850928406244, discriminator loss: 0.8476998777275505\n",
            "Epoch 161/300, Batch 400/937, generator loss: 1.812656031221139, discriminator loss: 0.8477536192266699\n",
            "Epoch 161/300, Batch 500/937, generator loss: 1.8124333070646839, discriminator loss: 0.8478112792826273\n",
            "Epoch 161/300, Batch 600/937, generator loss: 1.8122174910007889, discriminator loss: 0.8478789602650846\n",
            "Epoch 161/300, Batch 700/937, generator loss: 1.8119936113526403, discriminator loss: 0.8479415120441646\n",
            "Epoch 161/300, Batch 800/937, generator loss: 1.8117574601942996, discriminator loss: 0.8479982668932812\n",
            "Epoch 161/300, Batch 900/937, generator loss: 1.8115213810329243, discriminator loss: 0.8480603666933504\n",
            "Saving results...\n",
            "Epoch 162/300, Batch 0/937, generator loss: 1.8114401362132262, discriminator loss: 0.8480785473789982\n",
            "Epoch 162/300, Batch 100/937, generator loss: 1.811212326115211, discriminator loss: 0.8481340870339183\n",
            "Epoch 162/300, Batch 200/937, generator loss: 1.8109834323086555, discriminator loss: 0.8481877979569429\n",
            "Epoch 162/300, Batch 300/937, generator loss: 1.8107657331146547, discriminator loss: 0.8482416190326776\n",
            "Epoch 162/300, Batch 400/937, generator loss: 1.8105358552488626, discriminator loss: 0.8483018211610895\n",
            "Epoch 162/300, Batch 500/937, generator loss: 1.8103067523018155, discriminator loss: 0.8483599067276371\n",
            "Epoch 162/300, Batch 600/937, generator loss: 1.8101183958353813, discriminator loss: 0.8484016180490195\n",
            "Epoch 162/300, Batch 700/937, generator loss: 1.8099160327458446, discriminator loss: 0.8484630896588294\n",
            "Epoch 162/300, Batch 800/937, generator loss: 1.8096991157343465, discriminator loss: 0.8485191658729533\n",
            "Epoch 162/300, Batch 900/937, generator loss: 1.8094756713878148, discriminator loss: 0.8485774704695193\n",
            "Saving results...\n",
            "Epoch 163/300, Batch 0/937, generator loss: 1.8093704508879822, discriminator loss: 0.8486093024742037\n",
            "Epoch 163/300, Batch 100/937, generator loss: 1.8091474859355177, discriminator loss: 0.8486516433723125\n",
            "Epoch 163/300, Batch 200/937, generator loss: 1.8089308357423821, discriminator loss: 0.848688613568329\n",
            "Epoch 163/300, Batch 300/937, generator loss: 1.808720423114023, discriminator loss: 0.8487522570721401\n",
            "Epoch 163/300, Batch 400/937, generator loss: 1.8085022308143537, discriminator loss: 0.8488071301368976\n",
            "Epoch 163/300, Batch 500/937, generator loss: 1.8082864521876705, discriminator loss: 0.848863162252991\n",
            "Epoch 163/300, Batch 600/937, generator loss: 1.808066475863072, discriminator loss: 0.8489088428175425\n",
            "Epoch 163/300, Batch 700/937, generator loss: 1.8078428627849805, discriminator loss: 0.848978487885931\n",
            "Epoch 163/300, Batch 800/937, generator loss: 1.8076512473162594, discriminator loss: 0.8490304903059338\n",
            "Epoch 163/300, Batch 900/937, generator loss: 1.8074509241070984, discriminator loss: 0.8490878846637953\n",
            "Saving results...\n",
            "Epoch 164/300, Batch 0/937, generator loss: 1.807375285828457, discriminator loss: 0.8491144622363015\n",
            "Epoch 164/300, Batch 100/937, generator loss: 1.8071689642820192, discriminator loss: 0.8491631751678158\n",
            "Epoch 164/300, Batch 200/937, generator loss: 1.806973073544245, discriminator loss: 0.8492051215253209\n",
            "Epoch 164/300, Batch 300/937, generator loss: 1.8067532394648655, discriminator loss: 0.8492535520870369\n",
            "Epoch 164/300, Batch 400/937, generator loss: 1.806539336512375, discriminator loss: 0.84930882236192\n",
            "Epoch 164/300, Batch 500/937, generator loss: 1.8063133245196648, discriminator loss: 0.8493719128999414\n",
            "Epoch 164/300, Batch 600/937, generator loss: 1.8060935044570838, discriminator loss: 0.849425535300388\n",
            "Epoch 164/300, Batch 700/937, generator loss: 1.8058747946395532, discriminator loss: 0.849476203102613\n",
            "Epoch 164/300, Batch 800/937, generator loss: 1.8056649127466424, discriminator loss: 0.8495425887888738\n",
            "Epoch 164/300, Batch 900/937, generator loss: 1.8054206133670057, discriminator loss: 0.8496087672112803\n",
            "Saving results...\n",
            "Epoch 165/300, Batch 0/937, generator loss: 1.8053500401370453, discriminator loss: 0.8496234664891563\n",
            "Epoch 165/300, Batch 100/937, generator loss: 1.805131199708477, discriminator loss: 0.8496716361922312\n",
            "Epoch 165/300, Batch 200/937, generator loss: 1.8049190854390333, discriminator loss: 0.8497284532604791\n",
            "Epoch 165/300, Batch 300/937, generator loss: 1.8047111676463228, discriminator loss: 0.8497831269858391\n",
            "Epoch 165/300, Batch 400/937, generator loss: 1.804496249725312, discriminator loss: 0.8498384283758872\n",
            "Epoch 165/300, Batch 500/937, generator loss: 1.804311044190901, discriminator loss: 0.849884315376008\n",
            "Epoch 165/300, Batch 600/937, generator loss: 1.8040927603577572, discriminator loss: 0.849929822956837\n",
            "Epoch 165/300, Batch 700/937, generator loss: 1.80386567724357, discriminator loss: 0.8499860309272881\n",
            "Epoch 165/300, Batch 800/937, generator loss: 1.803661172806326, discriminator loss: 0.8500451850114135\n",
            "Epoch 165/300, Batch 900/937, generator loss: 1.8034365104566168, discriminator loss: 0.8501059477869133\n",
            "Saving results...\n",
            "Epoch 166/300, Batch 0/937, generator loss: 1.8033692679870237, discriminator loss: 0.8501263383939793\n",
            "Epoch 166/300, Batch 100/937, generator loss: 1.8031747976934287, discriminator loss: 0.850172714061737\n",
            "Epoch 166/300, Batch 200/937, generator loss: 1.8029799932107606, discriminator loss: 0.8502219429998871\n",
            "Epoch 166/300, Batch 300/937, generator loss: 1.8027988891478317, discriminator loss: 0.8502684057150526\n",
            "Epoch 166/300, Batch 400/937, generator loss: 1.8025770212528052, discriminator loss: 0.8503161847131206\n",
            "Epoch 166/300, Batch 500/937, generator loss: 1.802362840487611, discriminator loss: 0.850368406742277\n",
            "Epoch 166/300, Batch 600/937, generator loss: 1.8021831191835191, discriminator loss: 0.8504117732526557\n",
            "Epoch 166/300, Batch 700/937, generator loss: 1.8019686099831254, discriminator loss: 0.8504624223887719\n",
            "Epoch 166/300, Batch 800/937, generator loss: 1.8017613479033574, discriminator loss: 0.850527494667338\n",
            "Epoch 166/300, Batch 900/937, generator loss: 1.801550972441355, discriminator loss: 0.8505761557505916\n",
            "Saving results...\n",
            "Epoch 167/300, Batch 0/937, generator loss: 1.8014787715857807, discriminator loss: 0.8505996292637916\n",
            "Epoch 167/300, Batch 100/937, generator loss: 1.8012913848068968, discriminator loss: 0.8506439378378735\n",
            "Epoch 167/300, Batch 200/937, generator loss: 1.8011167769205945, discriminator loss: 0.8506994884475431\n",
            "Epoch 167/300, Batch 300/937, generator loss: 1.8009086333164863, discriminator loss: 0.850741758953059\n",
            "Epoch 167/300, Batch 400/937, generator loss: 1.8007104009724222, discriminator loss: 0.8507893762713197\n",
            "Epoch 167/300, Batch 500/937, generator loss: 1.800506257373355, discriminator loss: 0.8508402575634668\n",
            "Epoch 167/300, Batch 600/937, generator loss: 1.8003172966176115, discriminator loss: 0.8508841496978836\n",
            "Epoch 167/300, Batch 700/937, generator loss: 1.800145079102782, discriminator loss: 0.8509335591706524\n",
            "Epoch 167/300, Batch 800/937, generator loss: 1.799923345935081, discriminator loss: 0.8509878467652844\n",
            "Epoch 167/300, Batch 900/937, generator loss: 1.799734944656959, discriminator loss: 0.8510302915112508\n",
            "Saving results...\n",
            "Epoch 168/300, Batch 0/937, generator loss: 1.7996644509854398, discriminator loss: 0.8510379308228407\n",
            "Epoch 168/300, Batch 100/937, generator loss: 1.799459384338378, discriminator loss: 0.8510767764984607\n",
            "Epoch 168/300, Batch 200/937, generator loss: 1.7992697150815837, discriminator loss: 0.8511223678085776\n",
            "Epoch 168/300, Batch 300/937, generator loss: 1.7990767176728877, discriminator loss: 0.8511605503841919\n",
            "Epoch 168/300, Batch 400/937, generator loss: 1.7988597655064882, discriminator loss: 0.8512118365972162\n",
            "Epoch 168/300, Batch 500/937, generator loss: 1.7986592777385293, discriminator loss: 0.8512686808357097\n",
            "Epoch 168/300, Batch 600/937, generator loss: 1.7984525329246623, discriminator loss: 0.8513165583135907\n",
            "Epoch 168/300, Batch 700/937, generator loss: 1.7982600818843404, discriminator loss: 0.8513727865070546\n",
            "Epoch 168/300, Batch 800/937, generator loss: 1.7980453435088382, discriminator loss: 0.8514168019317478\n",
            "Epoch 168/300, Batch 900/937, generator loss: 1.7978311187453677, discriminator loss: 0.8514725483378254\n",
            "Saving results...\n",
            "Epoch 169/300, Batch 0/937, generator loss: 1.797757296163616, discriminator loss: 0.8514954572103607\n",
            "Epoch 169/300, Batch 100/937, generator loss: 1.7975601078788308, discriminator loss: 0.851541813385812\n",
            "Epoch 169/300, Batch 200/937, generator loss: 1.7973539045631675, discriminator loss: 0.8515807794538843\n",
            "Epoch 169/300, Batch 300/937, generator loss: 1.7971722332739253, discriminator loss: 0.8516153908181849\n",
            "Epoch 169/300, Batch 400/937, generator loss: 1.7969697351348428, discriminator loss: 0.8516688147578042\n",
            "Epoch 169/300, Batch 500/937, generator loss: 1.7967745791729297, discriminator loss: 0.8517151344972068\n",
            "Epoch 169/300, Batch 600/937, generator loss: 1.7965700698980758, discriminator loss: 0.8517676459480691\n",
            "Epoch 169/300, Batch 700/937, generator loss: 1.7964034426173636, discriminator loss: 0.8518049565944209\n",
            "Epoch 169/300, Batch 800/937, generator loss: 1.796224343127229, discriminator loss: 0.8518574813427346\n",
            "Epoch 169/300, Batch 900/937, generator loss: 1.7960157699154367, discriminator loss: 0.8519126184706962\n",
            "Saving results...\n",
            "Epoch 170/300, Batch 0/937, generator loss: 1.7959311258017487, discriminator loss: 0.8519328751882369\n",
            "Epoch 170/300, Batch 100/937, generator loss: 1.7957623958203972, discriminator loss: 0.8519709006804213\n",
            "Epoch 170/300, Batch 200/937, generator loss: 1.7955633811611318, discriminator loss: 0.8520239164743508\n",
            "Epoch 170/300, Batch 300/937, generator loss: 1.7953731997646327, discriminator loss: 0.852063249325736\n",
            "Epoch 170/300, Batch 400/937, generator loss: 1.795191464355321, discriminator loss: 0.8521027308186003\n",
            "Epoch 170/300, Batch 500/937, generator loss: 1.794980002337289, discriminator loss: 0.852150976614784\n",
            "Epoch 170/300, Batch 600/937, generator loss: 1.7947703895707017, discriminator loss: 0.8522012315971922\n",
            "Epoch 170/300, Batch 700/937, generator loss: 1.794586267921339, discriminator loss: 0.8522449805045894\n",
            "Epoch 170/300, Batch 800/937, generator loss: 1.7943871296692455, discriminator loss: 0.852293566137679\n",
            "Epoch 170/300, Batch 900/937, generator loss: 1.7941792275898896, discriminator loss: 0.8523478879233966\n",
            "Saving results...\n",
            "Epoch 171/300, Batch 0/937, generator loss: 1.79412882521644, discriminator loss: 0.8523634430226112\n",
            "Epoch 171/300, Batch 100/937, generator loss: 1.793965873390408, discriminator loss: 0.8523875567007179\n",
            "Epoch 171/300, Batch 200/937, generator loss: 1.793811110098387, discriminator loss: 0.8524266748133925\n",
            "Epoch 171/300, Batch 300/937, generator loss: 1.7936167160896952, discriminator loss: 0.8524784003051631\n",
            "Epoch 171/300, Batch 400/937, generator loss: 1.7934373743288676, discriminator loss: 0.8525254335865309\n",
            "Epoch 171/300, Batch 500/937, generator loss: 1.7932393038135943, discriminator loss: 0.8525684355733052\n",
            "Epoch 171/300, Batch 600/937, generator loss: 1.7930480404203246, discriminator loss: 0.85261087832876\n",
            "Epoch 171/300, Batch 700/937, generator loss: 1.7928724676073575, discriminator loss: 0.8526391808384706\n",
            "Epoch 171/300, Batch 800/937, generator loss: 1.7927126430585227, discriminator loss: 0.8526731256145634\n",
            "Epoch 171/300, Batch 900/937, generator loss: 1.7925565701118056, discriminator loss: 0.852727885919437\n",
            "Saving results...\n",
            "Epoch 172/300, Batch 0/937, generator loss: 1.792475921524539, discriminator loss: 0.8527517070559975\n",
            "Epoch 172/300, Batch 100/937, generator loss: 1.7923147969104278, discriminator loss: 0.8527888341217682\n",
            "Epoch 172/300, Batch 200/937, generator loss: 1.792119056177188, discriminator loss: 0.8528320220781651\n",
            "Epoch 172/300, Batch 300/937, generator loss: 1.791949630553306, discriminator loss: 0.8528788547443442\n",
            "Epoch 172/300, Batch 400/937, generator loss: 1.7917597928998406, discriminator loss: 0.8529237923877646\n",
            "Epoch 172/300, Batch 500/937, generator loss: 1.791578998760122, discriminator loss: 0.8529720158169265\n",
            "Epoch 172/300, Batch 600/937, generator loss: 1.7913849677622598, discriminator loss: 0.8530165304068129\n",
            "Epoch 172/300, Batch 700/937, generator loss: 1.7912129426740449, discriminator loss: 0.8530545569276675\n",
            "Epoch 172/300, Batch 800/937, generator loss: 1.7910221814295952, discriminator loss: 0.8531008352548619\n",
            "Epoch 172/300, Batch 900/937, generator loss: 1.790846966682432, discriminator loss: 0.8531374078876671\n",
            "Saving results...\n",
            "Epoch 173/300, Batch 0/937, generator loss: 1.7907783754896238, discriminator loss: 0.8531575851725485\n",
            "Epoch 173/300, Batch 100/937, generator loss: 1.79061807397357, discriminator loss: 0.8531937077914334\n",
            "Epoch 173/300, Batch 200/937, generator loss: 1.790470976356649, discriminator loss: 0.8532248173610506\n",
            "Epoch 173/300, Batch 300/937, generator loss: 1.7902872307649442, discriminator loss: 0.8532748727000576\n",
            "Epoch 173/300, Batch 400/937, generator loss: 1.7901376382223817, discriminator loss: 0.8533052561955473\n",
            "Epoch 173/300, Batch 500/937, generator loss: 1.789983853894807, discriminator loss: 0.853342928430484\n",
            "Epoch 173/300, Batch 600/937, generator loss: 1.7898006879937236, discriminator loss: 0.8533855637981727\n",
            "Epoch 173/300, Batch 700/937, generator loss: 1.7896399508306688, discriminator loss: 0.8534234130263989\n",
            "Epoch 173/300, Batch 800/937, generator loss: 1.789466013006578, discriminator loss: 0.8534657614613262\n",
            "Epoch 173/300, Batch 900/937, generator loss: 1.7892824961624094, discriminator loss: 0.8535123593734365\n",
            "Saving results...\n",
            "Epoch 174/300, Batch 0/937, generator loss: 1.789215170475224, discriminator loss: 0.8535199703999294\n",
            "Epoch 174/300, Batch 100/937, generator loss: 1.7890688870224678, discriminator loss: 0.8535524658434364\n",
            "Epoch 174/300, Batch 200/937, generator loss: 1.7889450923799544, discriminator loss: 0.8535897440571244\n",
            "Epoch 174/300, Batch 300/937, generator loss: 1.7887780982705983, discriminator loss: 0.8536297022655568\n",
            "Epoch 174/300, Batch 400/937, generator loss: 1.788612155174087, discriminator loss: 0.8536602962616903\n",
            "Epoch 174/300, Batch 500/937, generator loss: 1.7884494601826988, discriminator loss: 0.8537029832852483\n",
            "Epoch 174/300, Batch 600/937, generator loss: 1.7882663628242743, discriminator loss: 0.8537373845642396\n",
            "Epoch 174/300, Batch 700/937, generator loss: 1.7880952167567337, discriminator loss: 0.8537854010805855\n",
            "Epoch 174/300, Batch 800/937, generator loss: 1.7878983310873033, discriminator loss: 0.8538261941193869\n",
            "Epoch 174/300, Batch 900/937, generator loss: 1.787715594541826, discriminator loss: 0.8538782103806982\n",
            "Saving results...\n",
            "Epoch 175/300, Batch 0/937, generator loss: 1.7876429767541206, discriminator loss: 0.8538968183871563\n",
            "Epoch 175/300, Batch 100/937, generator loss: 1.7874748804498537, discriminator loss: 0.8539388094992575\n",
            "Epoch 175/300, Batch 200/937, generator loss: 1.7873110329513922, discriminator loss: 0.8539676528450164\n",
            "Epoch 175/300, Batch 300/937, generator loss: 1.7871480969483418, discriminator loss: 0.8540099556252715\n",
            "Epoch 175/300, Batch 400/937, generator loss: 1.786995708426417, discriminator loss: 0.854033484397011\n",
            "Epoch 175/300, Batch 500/937, generator loss: 1.7868274934764716, discriminator loss: 0.8540649535820486\n",
            "Epoch 175/300, Batch 600/937, generator loss: 1.7866531201688443, discriminator loss: 0.8541136896263083\n",
            "Epoch 175/300, Batch 700/937, generator loss: 1.7864678219598158, discriminator loss: 0.8541450681642953\n",
            "Epoch 175/300, Batch 800/937, generator loss: 1.7862828109899371, discriminator loss: 0.8541836904228637\n",
            "Epoch 175/300, Batch 900/937, generator loss: 1.7861285217735554, discriminator loss: 0.8542292336398616\n",
            "Saving results...\n",
            "Epoch 176/300, Batch 0/937, generator loss: 1.7860558331007304, discriminator loss: 0.8542436370845605\n",
            "Epoch 176/300, Batch 100/937, generator loss: 1.7859185764806529, discriminator loss: 0.854282509618833\n",
            "Epoch 176/300, Batch 200/937, generator loss: 1.7857674427719599, discriminator loss: 0.8543230415151898\n",
            "Epoch 176/300, Batch 300/937, generator loss: 1.7856312943594566, discriminator loss: 0.8543467937368145\n",
            "Epoch 176/300, Batch 400/937, generator loss: 1.7854679783942011, discriminator loss: 0.854387694177124\n",
            "Epoch 176/300, Batch 500/937, generator loss: 1.7853012473214682, discriminator loss: 0.854432836774658\n",
            "Epoch 176/300, Batch 600/937, generator loss: 1.7851381379604716, discriminator loss: 0.8544667318484843\n",
            "Epoch 176/300, Batch 700/937, generator loss: 1.7849617989222786, discriminator loss: 0.854507182558673\n",
            "Epoch 176/300, Batch 800/937, generator loss: 1.7847880744860274, discriminator loss: 0.8545528607864246\n",
            "Epoch 176/300, Batch 900/937, generator loss: 1.7846274946044256, discriminator loss: 0.8545873062043873\n",
            "Saving results...\n",
            "Epoch 177/300, Batch 0/937, generator loss: 1.78456584749252, discriminator loss: 0.8546019317572254\n",
            "Epoch 177/300, Batch 100/937, generator loss: 1.7844035774769285, discriminator loss: 0.8546290551536807\n",
            "Epoch 177/300, Batch 200/937, generator loss: 1.78424008804321, discriminator loss: 0.8546534768297025\n",
            "Epoch 177/300, Batch 300/937, generator loss: 1.784096541953955, discriminator loss: 0.8546865007332424\n",
            "Epoch 177/300, Batch 400/937, generator loss: 1.783936827652078, discriminator loss: 0.8547232309264348\n",
            "Epoch 177/300, Batch 500/937, generator loss: 1.7837478771226198, discriminator loss: 0.8547597702980256\n",
            "Epoch 177/300, Batch 600/937, generator loss: 1.7835994689329926, discriminator loss: 0.8547872122616408\n",
            "Epoch 177/300, Batch 700/937, generator loss: 1.7834334487612273, discriminator loss: 0.8548272934848573\n",
            "Epoch 177/300, Batch 800/937, generator loss: 1.7832496279783876, discriminator loss: 0.8548584522592603\n",
            "Epoch 177/300, Batch 900/937, generator loss: 1.7830870152838763, discriminator loss: 0.8549015213926693\n",
            "Saving results...\n",
            "Epoch 178/300, Batch 0/937, generator loss: 1.783022213430105, discriminator loss: 0.8549202661945571\n",
            "Epoch 178/300, Batch 100/937, generator loss: 1.782879175449278, discriminator loss: 0.8549478536722928\n",
            "Epoch 178/300, Batch 200/937, generator loss: 1.7827328614170401, discriminator loss: 0.8549837709021825\n",
            "Epoch 178/300, Batch 300/937, generator loss: 1.7825785393762335, discriminator loss: 0.8550158815199711\n",
            "Epoch 178/300, Batch 400/937, generator loss: 1.7824247833479399, discriminator loss: 0.8550459436576142\n",
            "Epoch 178/300, Batch 500/937, generator loss: 1.782280242775443, discriminator loss: 0.8550760211537156\n",
            "Epoch 178/300, Batch 600/937, generator loss: 1.782117961662482, discriminator loss: 0.855110471060448\n",
            "Epoch 178/300, Batch 700/937, generator loss: 1.7819660076828152, discriminator loss: 0.8551424241712179\n",
            "Epoch 178/300, Batch 800/937, generator loss: 1.7817950531453273, discriminator loss: 0.8551735039664253\n",
            "Epoch 178/300, Batch 900/937, generator loss: 1.7816166744804056, discriminator loss: 0.855212398473271\n",
            "Saving results...\n",
            "Epoch 179/300, Batch 0/937, generator loss: 1.781557394805486, discriminator loss: 0.8552230618136621\n",
            "Epoch 179/300, Batch 100/937, generator loss: 1.7813990599639975, discriminator loss: 0.855255077727546\n",
            "Epoch 179/300, Batch 200/937, generator loss: 1.7812307109549457, discriminator loss: 0.8552818333542023\n",
            "Epoch 179/300, Batch 300/937, generator loss: 1.7810850264551112, discriminator loss: 0.8553212176677951\n",
            "Epoch 179/300, Batch 400/937, generator loss: 1.780924845523839, discriminator loss: 0.8553587584262081\n",
            "Epoch 179/300, Batch 500/937, generator loss: 1.780772584374085, discriminator loss: 0.8553848738847833\n",
            "Epoch 179/300, Batch 600/937, generator loss: 1.7806440195350808, discriminator loss: 0.8554090912758827\n",
            "Epoch 179/300, Batch 700/937, generator loss: 1.7804794259206935, discriminator loss: 0.8554424090668455\n",
            "Epoch 179/300, Batch 800/937, generator loss: 1.7803167060825051, discriminator loss: 0.8554800103101604\n",
            "Epoch 179/300, Batch 900/937, generator loss: 1.7801612341467727, discriminator loss: 0.8555199988032597\n",
            "Saving results...\n",
            "Epoch 180/300, Batch 0/937, generator loss: 1.7801016663293694, discriminator loss: 0.8555380868672916\n",
            "Epoch 180/300, Batch 100/937, generator loss: 1.7799718186345184, discriminator loss: 0.8555515267595011\n",
            "Epoch 180/300, Batch 200/937, generator loss: 1.779817277318766, discriminator loss: 0.8555892267803223\n",
            "Epoch 180/300, Batch 300/937, generator loss: 1.7796717714268833, discriminator loss: 0.855619179781169\n",
            "Epoch 180/300, Batch 400/937, generator loss: 1.779553208444508, discriminator loss: 0.8556416999249017\n",
            "Epoch 180/300, Batch 500/937, generator loss: 1.7793752817994402, discriminator loss: 0.855689290647376\n",
            "Epoch 180/300, Batch 600/937, generator loss: 1.7792317950241214, discriminator loss: 0.8557300763872836\n",
            "Epoch 180/300, Batch 700/937, generator loss: 1.779075072443279, discriminator loss: 0.8557696935296233\n",
            "Epoch 180/300, Batch 800/937, generator loss: 1.7789225160399214, discriminator loss: 0.8558139438800753\n",
            "Epoch 180/300, Batch 900/937, generator loss: 1.7787725693549141, discriminator loss: 0.8558389451999457\n",
            "Saving results...\n",
            "Epoch 181/300, Batch 0/937, generator loss: 1.77871650425506, discriminator loss: 0.8558522255946464\n",
            "Epoch 181/300, Batch 100/937, generator loss: 1.7785776872623171, discriminator loss: 0.8558814411867621\n",
            "Epoch 181/300, Batch 200/937, generator loss: 1.7784290846282893, discriminator loss: 0.8558998690120652\n",
            "Epoch 181/300, Batch 300/937, generator loss: 1.7782793069310583, discriminator loss: 0.8559237371392412\n",
            "Epoch 181/300, Batch 400/937, generator loss: 1.7781385060964576, discriminator loss: 0.8559532795803038\n",
            "Epoch 181/300, Batch 500/937, generator loss: 1.7779847026045783, discriminator loss: 0.855989900464877\n",
            "Epoch 181/300, Batch 600/937, generator loss: 1.7778173130817685, discriminator loss: 0.8560205168367983\n",
            "Epoch 181/300, Batch 700/937, generator loss: 1.7776707649321257, discriminator loss: 0.8560549918156652\n",
            "Epoch 181/300, Batch 800/937, generator loss: 1.77752136532367, discriminator loss: 0.856098947824947\n",
            "Epoch 181/300, Batch 900/937, generator loss: 1.777387322970872, discriminator loss: 0.8561289271072097\n",
            "Saving results...\n",
            "Epoch 182/300, Batch 0/937, generator loss: 1.7773177967228255, discriminator loss: 0.8561459682799477\n",
            "Epoch 182/300, Batch 100/937, generator loss: 1.777166521517787, discriminator loss: 0.8561734090299685\n",
            "Epoch 182/300, Batch 200/937, generator loss: 1.7770217023073998, discriminator loss: 0.856195696703207\n",
            "Epoch 182/300, Batch 300/937, generator loss: 1.7768829699772195, discriminator loss: 0.8562171812131393\n",
            "Epoch 182/300, Batch 400/937, generator loss: 1.7767546251744417, discriminator loss: 0.8562442527008897\n",
            "Epoch 182/300, Batch 500/937, generator loss: 1.776618788271753, discriminator loss: 0.8562711787974123\n",
            "Epoch 182/300, Batch 600/937, generator loss: 1.776481555661189, discriminator loss: 0.8563077828182808\n",
            "Epoch 182/300, Batch 700/937, generator loss: 1.776342885855268, discriminator loss: 0.8563431427136412\n",
            "Epoch 182/300, Batch 800/937, generator loss: 1.7761894255355586, discriminator loss: 0.8563928625745206\n",
            "Epoch 182/300, Batch 900/937, generator loss: 1.7760532566678942, discriminator loss: 0.856429091652822\n",
            "Saving results...\n",
            "Epoch 183/300, Batch 0/937, generator loss: 1.7759933124286027, discriminator loss: 0.8564380201257072\n",
            "Epoch 183/300, Batch 100/937, generator loss: 1.7758608160815474, discriminator loss: 0.856458002545548\n",
            "Epoch 183/300, Batch 200/937, generator loss: 1.7757171690634148, discriminator loss: 0.856488287596617\n",
            "Epoch 183/300, Batch 300/937, generator loss: 1.7755752807665728, discriminator loss: 0.856515010477349\n",
            "Epoch 183/300, Batch 400/937, generator loss: 1.7754437398592717, discriminator loss: 0.856543979917212\n",
            "Epoch 183/300, Batch 500/937, generator loss: 1.7753016434418742, discriminator loss: 0.8565750210318612\n",
            "Epoch 183/300, Batch 600/937, generator loss: 1.775167122655411, discriminator loss: 0.8566162146314763\n",
            "Epoch 183/300, Batch 700/937, generator loss: 1.7750395265399055, discriminator loss: 0.85664369691781\n",
            "Epoch 183/300, Batch 800/937, generator loss: 1.7748751117872685, discriminator loss: 0.8566870241071896\n",
            "Epoch 183/300, Batch 900/937, generator loss: 1.7747225367786121, discriminator loss: 0.8567221453367766\n",
            "Saving results...\n",
            "Epoch 184/300, Batch 0/937, generator loss: 1.774671206067935, discriminator loss: 0.8567392412594684\n",
            "Epoch 184/300, Batch 100/937, generator loss: 1.7745408213270084, discriminator loss: 0.8567588615245344\n",
            "Epoch 184/300, Batch 200/937, generator loss: 1.7744005697624898, discriminator loss: 0.8567859236030788\n",
            "Epoch 184/300, Batch 300/937, generator loss: 1.774251700777709, discriminator loss: 0.8568076096558686\n",
            "Epoch 184/300, Batch 400/937, generator loss: 1.7741202002314356, discriminator loss: 0.8568314442706173\n",
            "Epoch 184/300, Batch 500/937, generator loss: 1.7740034672784428, discriminator loss: 0.8568607344111284\n",
            "Epoch 184/300, Batch 600/937, generator loss: 1.7738613623239168, discriminator loss: 0.8568823155572124\n",
            "Epoch 184/300, Batch 700/937, generator loss: 1.7737192440301803, discriminator loss: 0.8569118189940412\n",
            "Epoch 184/300, Batch 800/937, generator loss: 1.7735759219613223, discriminator loss: 0.8569396998530273\n",
            "Epoch 184/300, Batch 900/937, generator loss: 1.7734399981681657, discriminator loss: 0.8569735770806161\n",
            "Saving results...\n",
            "Epoch 185/300, Batch 0/937, generator loss: 1.7733788625929623, discriminator loss: 0.8569859789375103\n",
            "Epoch 185/300, Batch 100/937, generator loss: 1.7732716216165727, discriminator loss: 0.8569938995851073\n",
            "Epoch 185/300, Batch 200/937, generator loss: 1.7731466317761235, discriminator loss: 0.8570318968244217\n",
            "Epoch 185/300, Batch 300/937, generator loss: 1.7729926842044066, discriminator loss: 0.8570705415802022\n",
            "Epoch 185/300, Batch 400/937, generator loss: 1.7728493578919238, discriminator loss: 0.85709743710485\n",
            "Epoch 185/300, Batch 500/937, generator loss: 1.7727054373691054, discriminator loss: 0.8571257560655975\n",
            "Epoch 185/300, Batch 600/937, generator loss: 1.7725547176510341, discriminator loss: 0.8571441643964689\n",
            "Epoch 185/300, Batch 700/937, generator loss: 1.7724404605596635, discriminator loss: 0.8571708276938927\n",
            "Epoch 185/300, Batch 800/937, generator loss: 1.7723020166413666, discriminator loss: 0.8571960171825059\n",
            "Epoch 185/300, Batch 900/937, generator loss: 1.772170895473724, discriminator loss: 0.8572149067633452\n",
            "Saving results...\n",
            "Epoch 186/300, Batch 0/937, generator loss: 1.7721285359560135, discriminator loss: 0.8572277799500492\n",
            "Epoch 186/300, Batch 100/937, generator loss: 1.7720008527596889, discriminator loss: 0.8572494231229526\n",
            "Epoch 186/300, Batch 200/937, generator loss: 1.7718591271300905, discriminator loss: 0.8572707356144952\n",
            "Epoch 186/300, Batch 300/937, generator loss: 1.7717209878802818, discriminator loss: 0.8572971523316778\n",
            "Epoch 186/300, Batch 400/937, generator loss: 1.7716038401674732, discriminator loss: 0.8573082106384278\n",
            "Epoch 186/300, Batch 500/937, generator loss: 1.771471785760236, discriminator loss: 0.8573390844797087\n",
            "Epoch 186/300, Batch 600/937, generator loss: 1.7713316760696807, discriminator loss: 0.8573622871434533\n",
            "Epoch 186/300, Batch 700/937, generator loss: 1.7712178916302974, discriminator loss: 0.857387480747299\n",
            "Epoch 186/300, Batch 800/937, generator loss: 1.7710782104777347, discriminator loss: 0.8574260891792495\n",
            "Epoch 186/300, Batch 900/937, generator loss: 1.7709506711360032, discriminator loss: 0.8574516808838727\n",
            "Saving results...\n",
            "Epoch 187/300, Batch 0/937, generator loss: 1.7708961693306597, discriminator loss: 0.8574653441437424\n",
            "Epoch 187/300, Batch 100/937, generator loss: 1.7707869875928786, discriminator loss: 0.8574763901888671\n",
            "Epoch 187/300, Batch 200/937, generator loss: 1.7706540182091848, discriminator loss: 0.8575067657476588\n",
            "Epoch 187/300, Batch 300/937, generator loss: 1.7705257590836356, discriminator loss: 0.8575319437724398\n",
            "Epoch 187/300, Batch 400/937, generator loss: 1.7704079077423278, discriminator loss: 0.8575487961268224\n",
            "Epoch 187/300, Batch 500/937, generator loss: 1.7702875203867043, discriminator loss: 0.857568329803844\n",
            "Epoch 187/300, Batch 600/937, generator loss: 1.7701581155470025, discriminator loss: 0.8575982345083158\n",
            "Epoch 187/300, Batch 700/937, generator loss: 1.7700197572617868, discriminator loss: 0.8576222024239657\n",
            "Epoch 187/300, Batch 800/937, generator loss: 1.7698938420937422, discriminator loss: 0.8576497800533927\n",
            "Epoch 187/300, Batch 900/937, generator loss: 1.76975249473734, discriminator loss: 0.8576892587048085\n",
            "Saving results...\n",
            "Epoch 188/300, Batch 0/937, generator loss: 1.7696942334658776, discriminator loss: 0.857706431311185\n",
            "Epoch 188/300, Batch 100/937, generator loss: 1.769573104329348, discriminator loss: 0.8577215003653937\n",
            "Epoch 188/300, Batch 200/937, generator loss: 1.769454423905195, discriminator loss: 0.8577491837732137\n",
            "Epoch 188/300, Batch 300/937, generator loss: 1.7693188576241834, discriminator loss: 0.8577669594177781\n",
            "Epoch 188/300, Batch 400/937, generator loss: 1.7692031102935017, discriminator loss: 0.857785898497811\n",
            "Epoch 188/300, Batch 500/937, generator loss: 1.7690708617435353, discriminator loss: 0.8578184432944651\n",
            "Epoch 188/300, Batch 600/937, generator loss: 1.768960313019399, discriminator loss: 0.8578396701186946\n",
            "Epoch 188/300, Batch 700/937, generator loss: 1.7688295255806998, discriminator loss: 0.857866071878799\n",
            "Epoch 188/300, Batch 800/937, generator loss: 1.7686878120494263, discriminator loss: 0.8578977079610541\n",
            "Epoch 188/300, Batch 900/937, generator loss: 1.7685722520789084, discriminator loss: 0.85792170409472\n",
            "Saving results...\n",
            "Epoch 189/300, Batch 0/937, generator loss: 1.7685112802520178, discriminator loss: 0.8579295423530876\n",
            "Epoch 189/300, Batch 100/937, generator loss: 1.7683903429576615, discriminator loss: 0.8579573734826288\n",
            "Epoch 189/300, Batch 200/937, generator loss: 1.7682625097239488, discriminator loss: 0.8579728110367532\n",
            "Epoch 189/300, Batch 300/937, generator loss: 1.768125666599109, discriminator loss: 0.8580008802604373\n",
            "Epoch 189/300, Batch 400/937, generator loss: 1.7680075549135177, discriminator loss: 0.8580181281151247\n",
            "Epoch 189/300, Batch 500/937, generator loss: 1.767888196832241, discriminator loss: 0.8580336960369611\n",
            "Epoch 189/300, Batch 600/937, generator loss: 1.7677488989679984, discriminator loss: 0.8580559195240198\n",
            "Epoch 189/300, Batch 700/937, generator loss: 1.7676154995471625, discriminator loss: 0.8580904120202274\n",
            "Epoch 189/300, Batch 800/937, generator loss: 1.7674986408016327, discriminator loss: 0.8581171470833199\n",
            "Epoch 189/300, Batch 900/937, generator loss: 1.7673702940813794, discriminator loss: 0.8581383566728826\n",
            "Saving results...\n",
            "Epoch 190/300, Batch 0/937, generator loss: 1.7673178827927793, discriminator loss: 0.8581481330496096\n",
            "Epoch 190/300, Batch 100/937, generator loss: 1.767201682471207, discriminator loss: 0.8581612721966329\n",
            "Epoch 190/300, Batch 200/937, generator loss: 1.767084546181567, discriminator loss: 0.858182452347482\n",
            "Epoch 190/300, Batch 300/937, generator loss: 1.766952632546519, discriminator loss: 0.8581955487099916\n",
            "Epoch 190/300, Batch 400/937, generator loss: 1.766835867858207, discriminator loss: 0.8582139559368108\n",
            "Epoch 190/300, Batch 500/937, generator loss: 1.766719395392033, discriminator loss: 0.8582378566023391\n",
            "Epoch 190/300, Batch 600/937, generator loss: 1.7665950242380775, discriminator loss: 0.858262840734525\n",
            "Epoch 190/300, Batch 700/937, generator loss: 1.7664785685654552, discriminator loss: 0.858290131076391\n",
            "Epoch 190/300, Batch 800/937, generator loss: 1.766346259278671, discriminator loss: 0.8583160319991299\n",
            "Epoch 190/300, Batch 900/937, generator loss: 1.7662056682192193, discriminator loss: 0.8583506808031255\n",
            "Saving results...\n",
            "Epoch 191/300, Batch 0/937, generator loss: 1.76616573095712, discriminator loss: 0.8583540157281963\n",
            "Epoch 191/300, Batch 100/937, generator loss: 1.7660393872084792, discriminator loss: 0.8583746606392394\n",
            "Epoch 191/300, Batch 200/937, generator loss: 1.7659353824985398, discriminator loss: 0.8583927419102164\n",
            "Epoch 191/300, Batch 300/937, generator loss: 1.7658143865553988, discriminator loss: 0.85840686547385\n",
            "Epoch 191/300, Batch 400/937, generator loss: 1.765688432646483, discriminator loss: 0.8584315868146984\n",
            "Epoch 191/300, Batch 500/937, generator loss: 1.7655512292091589, discriminator loss: 0.8584575481086941\n",
            "Epoch 191/300, Batch 600/937, generator loss: 1.765428184910946, discriminator loss: 0.8584806969523586\n",
            "Epoch 191/300, Batch 700/937, generator loss: 1.76530924081042, discriminator loss: 0.8585036016754684\n",
            "Epoch 191/300, Batch 800/937, generator loss: 1.7651995694437432, discriminator loss: 0.8585257122618793\n",
            "Epoch 191/300, Batch 900/937, generator loss: 1.7650751565316558, discriminator loss: 0.8585494421820966\n",
            "Saving results...\n",
            "Epoch 192/300, Batch 0/937, generator loss: 1.7650363521818315, discriminator loss: 0.8585581803316206\n",
            "Epoch 192/300, Batch 100/937, generator loss: 1.7649357421471474, discriminator loss: 0.8585637285636135\n",
            "Epoch 192/300, Batch 200/937, generator loss: 1.7648174702259076, discriminator loss: 0.8585822269818972\n",
            "Epoch 192/300, Batch 300/937, generator loss: 1.7647038321953357, discriminator loss: 0.8586009651099182\n",
            "Epoch 192/300, Batch 400/937, generator loss: 1.7645898974628411, discriminator loss: 0.858618871022248\n",
            "Epoch 192/300, Batch 500/937, generator loss: 1.7644940888864173, discriminator loss: 0.858631870851909\n",
            "Epoch 192/300, Batch 600/937, generator loss: 1.764373909114837, discriminator loss: 0.8586514537321102\n",
            "Epoch 192/300, Batch 700/937, generator loss: 1.7642283093414117, discriminator loss: 0.8586722460513805\n",
            "Epoch 192/300, Batch 800/937, generator loss: 1.7641131895470015, discriminator loss: 0.8586967754816232\n",
            "Epoch 192/300, Batch 900/937, generator loss: 1.7639868127647311, discriminator loss: 0.8587132985019726\n",
            "Saving results...\n",
            "Epoch 193/300, Batch 0/937, generator loss: 1.7639440358663327, discriminator loss: 0.8587199568999296\n",
            "Epoch 193/300, Batch 100/937, generator loss: 1.7638230994802466, discriminator loss: 0.8587309746657537\n",
            "Epoch 193/300, Batch 200/937, generator loss: 1.7637037780570497, discriminator loss: 0.8587342004810385\n",
            "Epoch 193/300, Batch 300/937, generator loss: 1.7636004244583898, discriminator loss: 0.8587502079408568\n",
            "Epoch 193/300, Batch 400/937, generator loss: 1.7635080486207486, discriminator loss: 0.8587693980519467\n",
            "Epoch 193/300, Batch 500/937, generator loss: 1.7633998610363684, discriminator loss: 0.8587907630415645\n",
            "Epoch 193/300, Batch 600/937, generator loss: 1.763291353508036, discriminator loss: 0.8588123648775324\n",
            "Epoch 193/300, Batch 700/937, generator loss: 1.7631810439045865, discriminator loss: 0.8588355387423445\n",
            "Epoch 193/300, Batch 800/937, generator loss: 1.7630744194249002, discriminator loss: 0.8588593812154596\n",
            "Epoch 193/300, Batch 900/937, generator loss: 1.762969630735078, discriminator loss: 0.8588888295847289\n",
            "Saving results...\n",
            "Epoch 194/300, Batch 0/937, generator loss: 1.7629236680280946, discriminator loss: 0.8588949472343477\n",
            "Epoch 194/300, Batch 100/937, generator loss: 1.7628137129273307, discriminator loss: 0.8589141905964512\n",
            "Epoch 194/300, Batch 200/937, generator loss: 1.7627144757052733, discriminator loss: 0.8589143092829575\n",
            "Epoch 194/300, Batch 300/937, generator loss: 1.762606302799256, discriminator loss: 0.8589369177899155\n",
            "Epoch 194/300, Batch 400/937, generator loss: 1.762488223336867, discriminator loss: 0.8589471998478047\n",
            "Epoch 194/300, Batch 500/937, generator loss: 1.7623666833241771, discriminator loss: 0.8589609217080715\n",
            "Epoch 194/300, Batch 600/937, generator loss: 1.7622523325856958, discriminator loss: 0.8589792366706737\n",
            "Epoch 194/300, Batch 700/937, generator loss: 1.7621592391304144, discriminator loss: 0.858991534224683\n",
            "Epoch 194/300, Batch 800/937, generator loss: 1.7620379027941018, discriminator loss: 0.8590115297030275\n",
            "Epoch 194/300, Batch 900/937, generator loss: 1.7619307546508607, discriminator loss: 0.8590472623105426\n",
            "Saving results...\n",
            "Epoch 195/300, Batch 0/937, generator loss: 1.761885126222897, discriminator loss: 0.8590535104319441\n",
            "Epoch 195/300, Batch 100/937, generator loss: 1.7617838902823768, discriminator loss: 0.8590698351554733\n",
            "Epoch 195/300, Batch 200/937, generator loss: 1.761691269409782, discriminator loss: 0.85906980103216\n",
            "Epoch 195/300, Batch 300/937, generator loss: 1.7615650878934124, discriminator loss: 0.8590726072180611\n",
            "Epoch 195/300, Batch 400/937, generator loss: 1.761463730003002, discriminator loss: 0.8590873910376646\n",
            "Epoch 195/300, Batch 500/937, generator loss: 1.7613525815202689, discriminator loss: 0.85910767266702\n",
            "Epoch 195/300, Batch 600/937, generator loss: 1.7612215571237002, discriminator loss: 0.8591260420114082\n",
            "Epoch 195/300, Batch 700/937, generator loss: 1.7611146764302086, discriminator loss: 0.8591485001138994\n",
            "Epoch 195/300, Batch 800/937, generator loss: 1.7609941113708782, discriminator loss: 0.8591679146164962\n",
            "Epoch 195/300, Batch 900/937, generator loss: 1.7608748058573906, discriminator loss: 0.8591885253647131\n",
            "Saving results...\n",
            "Epoch 196/300, Batch 0/937, generator loss: 1.760839089121697, discriminator loss: 0.8591964561277494\n",
            "Epoch 196/300, Batch 100/937, generator loss: 1.7607658263450474, discriminator loss: 0.859205320818808\n",
            "Epoch 196/300, Batch 200/937, generator loss: 1.7606619360942264, discriminator loss: 0.8592213190628047\n",
            "Epoch 196/300, Batch 300/937, generator loss: 1.7605665692378907, discriminator loss: 0.8592362146313395\n",
            "Epoch 196/300, Batch 400/937, generator loss: 1.760484239520714, discriminator loss: 0.8592505195330638\n",
            "Epoch 196/300, Batch 500/937, generator loss: 1.7603738215069544, discriminator loss: 0.8592617669976549\n",
            "Epoch 196/300, Batch 600/937, generator loss: 1.7602878627640948, discriminator loss: 0.8592873920831301\n",
            "Epoch 196/300, Batch 700/937, generator loss: 1.7601818068880035, discriminator loss: 0.8593109408871155\n",
            "Epoch 196/300, Batch 800/937, generator loss: 1.7600851798850161, discriminator loss: 0.8593280091152661\n",
            "Epoch 196/300, Batch 900/937, generator loss: 1.7599561650010855, discriminator loss: 0.8593505508620205\n",
            "Saving results...\n",
            "Epoch 197/300, Batch 0/937, generator loss: 1.7599052532460335, discriminator loss: 0.859354698332019\n",
            "Epoch 197/300, Batch 100/937, generator loss: 1.759801059801246, discriminator loss: 0.8593696185808175\n",
            "Epoch 197/300, Batch 200/937, generator loss: 1.7597018008911671, discriminator loss: 0.8593847932313465\n",
            "Epoch 197/300, Batch 300/937, generator loss: 1.7596030742492559, discriminator loss: 0.8593960840089602\n",
            "Epoch 197/300, Batch 400/937, generator loss: 1.7594945102232495, discriminator loss: 0.8594124563957035\n",
            "Epoch 197/300, Batch 500/937, generator loss: 1.7594087617708003, discriminator loss: 0.8594241509508601\n",
            "Epoch 197/300, Batch 600/937, generator loss: 1.7592873293664455, discriminator loss: 0.8594373388003373\n",
            "Epoch 197/300, Batch 700/937, generator loss: 1.7591888724747515, discriminator loss: 0.8594549832075965\n",
            "Epoch 197/300, Batch 800/937, generator loss: 1.7591218004023361, discriminator loss: 0.8594686426471896\n",
            "Epoch 197/300, Batch 900/937, generator loss: 1.7590392764886889, discriminator loss: 0.8594952148665661\n",
            "Saving results...\n",
            "Epoch 198/300, Batch 0/937, generator loss: 1.75898970555576, discriminator loss: 0.8595070389666168\n",
            "Epoch 198/300, Batch 100/937, generator loss: 1.7589089721594593, discriminator loss: 0.8595140360463667\n",
            "Epoch 198/300, Batch 200/937, generator loss: 1.758800903066809, discriminator loss: 0.8595383388900026\n",
            "Epoch 198/300, Batch 300/937, generator loss: 1.75869658612313, discriminator loss: 0.8595470183679659\n",
            "Epoch 198/300, Batch 400/937, generator loss: 1.7585828388068179, discriminator loss: 0.8595596040034841\n",
            "Epoch 198/300, Batch 500/937, generator loss: 1.7584892863743529, discriminator loss: 0.8595772906573551\n",
            "Epoch 198/300, Batch 600/937, generator loss: 1.7583879587987534, discriminator loss: 0.8596031883967868\n",
            "Epoch 198/300, Batch 700/937, generator loss: 1.7582941155135559, discriminator loss: 0.8596227557820769\n",
            "Epoch 198/300, Batch 800/937, generator loss: 1.7581847651346936, discriminator loss: 0.8596297617632683\n",
            "Epoch 198/300, Batch 900/937, generator loss: 1.7580967910893002, discriminator loss: 0.8596503053050353\n",
            "Saving results...\n",
            "Epoch 199/300, Batch 0/937, generator loss: 1.7580610793467495, discriminator loss: 0.8596560318300749\n",
            "Epoch 199/300, Batch 100/937, generator loss: 1.757978978950788, discriminator loss: 0.8596583032016791\n",
            "Epoch 199/300, Batch 200/937, generator loss: 1.757888350407297, discriminator loss: 0.8596700779660679\n",
            "Epoch 199/300, Batch 300/937, generator loss: 1.7578038479771787, discriminator loss: 0.8596757760880637\n",
            "Epoch 199/300, Batch 400/937, generator loss: 1.7577226154948942, discriminator loss: 0.8596860524034108\n",
            "Epoch 199/300, Batch 500/937, generator loss: 1.757632354659466, discriminator loss: 0.8596984099164203\n",
            "Epoch 199/300, Batch 600/937, generator loss: 1.757557274542788, discriminator loss: 0.8597052124272075\n",
            "Epoch 199/300, Batch 700/937, generator loss: 1.757466464999413, discriminator loss: 0.8597204005025448\n",
            "Epoch 199/300, Batch 800/937, generator loss: 1.7573678189196629, discriminator loss: 0.8597455857958523\n",
            "Epoch 199/300, Batch 900/937, generator loss: 1.7572681382999928, discriminator loss: 0.8597734323250721\n",
            "Saving results...\n",
            "Epoch 200/300, Batch 0/937, generator loss: 1.7572365871759508, discriminator loss: 0.8597823067141531\n",
            "Epoch 200/300, Batch 100/937, generator loss: 1.757158541380121, discriminator loss: 0.8597803723928069\n",
            "Epoch 200/300, Batch 200/937, generator loss: 1.7570564361535048, discriminator loss: 0.8597951992182484\n",
            "Epoch 200/300, Batch 300/937, generator loss: 1.7569412434298812, discriminator loss: 0.8598078581614084\n",
            "Epoch 200/300, Batch 400/937, generator loss: 1.7568616296073112, discriminator loss: 0.8598129324617263\n",
            "Epoch 200/300, Batch 500/937, generator loss: 1.7567693574362666, discriminator loss: 0.8598226499945066\n",
            "Epoch 200/300, Batch 600/937, generator loss: 1.7566811661262565, discriminator loss: 0.8598340466333119\n",
            "Epoch 200/300, Batch 700/937, generator loss: 1.7565898914323748, discriminator loss: 0.8598526481736326\n",
            "Epoch 200/300, Batch 800/937, generator loss: 1.7564975568117187, discriminator loss: 0.8598604021720384\n",
            "Epoch 200/300, Batch 900/937, generator loss: 1.756382425653658, discriminator loss: 0.8598742118634872\n",
            "Saving results...\n",
            "Epoch 201/300, Batch 0/937, generator loss: 1.7563541146799566, discriminator loss: 0.8598814622204003\n",
            "Epoch 201/300, Batch 100/937, generator loss: 1.7562675151171052, discriminator loss: 0.8598866671030242\n",
            "Epoch 201/300, Batch 200/937, generator loss: 1.7561924890426497, discriminator loss: 0.8598889211287234\n",
            "Epoch 201/300, Batch 300/937, generator loss: 1.756088848909648, discriminator loss: 0.8599094493994421\n",
            "Epoch 201/300, Batch 400/937, generator loss: 1.7560009197493174, discriminator loss: 0.859905373175957\n",
            "Epoch 201/300, Batch 500/937, generator loss: 1.7559025654151381, discriminator loss: 0.8599244670460859\n",
            "Epoch 201/300, Batch 600/937, generator loss: 1.7558133270552891, discriminator loss: 0.8599359058998707\n",
            "Epoch 201/300, Batch 700/937, generator loss: 1.7557125881823967, discriminator loss: 0.8599455511694901\n",
            "Epoch 201/300, Batch 800/937, generator loss: 1.7555870572745471, discriminator loss: 0.8599684524858393\n",
            "Epoch 201/300, Batch 900/937, generator loss: 1.7554928966258785, discriminator loss: 0.8599771080737909\n",
            "Saving results...\n",
            "Epoch 202/300, Batch 0/937, generator loss: 1.7554626711692478, discriminator loss: 0.8599780060937612\n",
            "Epoch 202/300, Batch 100/937, generator loss: 1.7553810270142634, discriminator loss: 0.8599820717731325\n",
            "Epoch 202/300, Batch 200/937, generator loss: 1.7552807795178813, discriminator loss: 0.8599869814240252\n",
            "Epoch 202/300, Batch 300/937, generator loss: 1.755180808830651, discriminator loss: 0.8599902985208537\n",
            "Epoch 202/300, Batch 400/937, generator loss: 1.7551033305496477, discriminator loss: 0.8599933873526079\n",
            "Epoch 202/300, Batch 500/937, generator loss: 1.7550106593909558, discriminator loss: 0.8600001587917676\n",
            "Epoch 202/300, Batch 600/937, generator loss: 1.7549207329558574, discriminator loss: 0.8600133147253811\n",
            "Epoch 202/300, Batch 700/937, generator loss: 1.7548316873062721, discriminator loss: 0.860033383123309\n",
            "Epoch 202/300, Batch 800/937, generator loss: 1.7547474553875746, discriminator loss: 0.8600425664539417\n",
            "Epoch 202/300, Batch 900/937, generator loss: 1.754649475026071, discriminator loss: 0.8600528813356543\n",
            "Saving results...\n",
            "Epoch 203/300, Batch 0/937, generator loss: 1.754612500938615, discriminator loss: 0.8600556788195549\n",
            "Epoch 203/300, Batch 100/937, generator loss: 1.7545268119134418, discriminator loss: 0.8600536008480982\n",
            "Epoch 203/300, Batch 200/937, generator loss: 1.7544320892575704, discriminator loss: 0.8600682421696714\n",
            "Epoch 203/300, Batch 300/937, generator loss: 1.7543664219889872, discriminator loss: 0.8600663142337452\n",
            "Epoch 203/300, Batch 400/937, generator loss: 1.7542659155068372, discriminator loss: 0.860077483296248\n",
            "Epoch 203/300, Batch 500/937, generator loss: 1.7541845881798668, discriminator loss: 0.8600915874958481\n",
            "Epoch 203/300, Batch 600/937, generator loss: 1.7540903882142136, discriminator loss: 0.860109051761\n",
            "Epoch 203/300, Batch 700/937, generator loss: 1.754004404158589, discriminator loss: 0.8601137603370225\n",
            "Epoch 203/300, Batch 800/937, generator loss: 1.7538934420249181, discriminator loss: 0.860133767953198\n",
            "Epoch 203/300, Batch 900/937, generator loss: 1.753784092541687, discriminator loss: 0.8601501743146279\n",
            "Saving results...\n",
            "Epoch 204/300, Batch 0/937, generator loss: 1.7537478677735625, discriminator loss: 0.8601567965566222\n",
            "Epoch 204/300, Batch 100/937, generator loss: 1.7536540103688338, discriminator loss: 0.8601605051779561\n",
            "Epoch 204/300, Batch 200/937, generator loss: 1.7535521977434305, discriminator loss: 0.8601807445587208\n",
            "Epoch 204/300, Batch 300/937, generator loss: 1.7534535712930892, discriminator loss: 0.8601931222098192\n",
            "Epoch 204/300, Batch 400/937, generator loss: 1.753370989231494, discriminator loss: 0.860198142229977\n",
            "Epoch 204/300, Batch 500/937, generator loss: 1.7532875826308674, discriminator loss: 0.8602024853383161\n",
            "Epoch 204/300, Batch 600/937, generator loss: 1.753189554788252, discriminator loss: 0.8602081317183965\n",
            "Epoch 204/300, Batch 700/937, generator loss: 1.753098233324533, discriminator loss: 0.8602149016582228\n",
            "Epoch 204/300, Batch 800/937, generator loss: 1.7529946824828095, discriminator loss: 0.8602310222401816\n",
            "Epoch 204/300, Batch 900/937, generator loss: 1.7529006777705074, discriminator loss: 0.8602361944639647\n",
            "Saving results...\n",
            "Epoch 205/300, Batch 0/937, generator loss: 1.7528665756304953, discriminator loss: 0.8602434235962413\n",
            "Epoch 205/300, Batch 100/937, generator loss: 1.7528086625355999, discriminator loss: 0.8602506905718469\n",
            "Epoch 205/300, Batch 200/937, generator loss: 1.7527495685294068, discriminator loss: 0.860250221984167\n",
            "Epoch 205/300, Batch 300/937, generator loss: 1.752664597337853, discriminator loss: 0.8602666367751703\n",
            "Epoch 205/300, Batch 400/937, generator loss: 1.7525787981308112, discriminator loss: 0.8602747999641607\n",
            "Epoch 205/300, Batch 500/937, generator loss: 1.752506484267246, discriminator loss: 0.8602774754314697\n",
            "Epoch 205/300, Batch 600/937, generator loss: 1.752427867865677, discriminator loss: 0.8602868791858264\n",
            "Epoch 205/300, Batch 700/937, generator loss: 1.7523458639618672, discriminator loss: 0.8602954795757781\n",
            "Epoch 205/300, Batch 800/937, generator loss: 1.752263352746915, discriminator loss: 0.8603082677326098\n",
            "Epoch 205/300, Batch 900/937, generator loss: 1.752175505721202, discriminator loss: 0.8603152237340618\n",
            "Saving results...\n",
            "Epoch 206/300, Batch 0/937, generator loss: 1.7521452332624001, discriminator loss: 0.8603213183079997\n",
            "Epoch 206/300, Batch 100/937, generator loss: 1.7520606760329995, discriminator loss: 0.8603113722106593\n",
            "Epoch 206/300, Batch 200/937, generator loss: 1.75197163604895, discriminator loss: 0.8603182519254262\n",
            "Epoch 206/300, Batch 300/937, generator loss: 1.7518956683632798, discriminator loss: 0.8603277222097244\n",
            "Epoch 206/300, Batch 400/937, generator loss: 1.7518204414626892, discriminator loss: 0.8603318045725525\n",
            "Epoch 206/300, Batch 500/937, generator loss: 1.7517494779337004, discriminator loss: 0.860340766828279\n",
            "Epoch 206/300, Batch 600/937, generator loss: 1.751676925477028, discriminator loss: 0.8603457903430471\n",
            "Epoch 206/300, Batch 700/937, generator loss: 1.7515947234626092, discriminator loss: 0.8603604620511645\n",
            "Epoch 206/300, Batch 800/937, generator loss: 1.751494472033332, discriminator loss: 0.8603757159257431\n",
            "Epoch 206/300, Batch 900/937, generator loss: 1.7514023422044736, discriminator loss: 0.8603892785786511\n",
            "Saving results...\n",
            "Epoch 207/300, Batch 0/937, generator loss: 1.7513608850389688, discriminator loss: 0.8603921529943549\n",
            "Epoch 207/300, Batch 100/937, generator loss: 1.7512839088909997, discriminator loss: 0.8603923021022418\n",
            "Epoch 207/300, Batch 200/937, generator loss: 1.7512034259315965, discriminator loss: 0.8603959303254091\n",
            "Epoch 207/300, Batch 300/937, generator loss: 1.751132063522381, discriminator loss: 0.860398751204707\n",
            "Epoch 207/300, Batch 400/937, generator loss: 1.7510663467722927, discriminator loss: 0.8604077786877276\n",
            "Epoch 207/300, Batch 500/937, generator loss: 1.7509734306115494, discriminator loss: 0.8604146227672747\n",
            "Epoch 207/300, Batch 600/937, generator loss: 1.7508907036310504, discriminator loss: 0.8604220546458811\n",
            "Epoch 207/300, Batch 700/937, generator loss: 1.7508190638700811, discriminator loss: 0.8604364176299838\n",
            "Epoch 207/300, Batch 800/937, generator loss: 1.7507502675025854, discriminator loss: 0.8604494981002541\n",
            "Epoch 207/300, Batch 900/937, generator loss: 1.7506676141154678, discriminator loss: 0.8604556217722356\n",
            "Saving results...\n",
            "Epoch 208/300, Batch 0/937, generator loss: 1.7506252648215739, discriminator loss: 0.8604622352939327\n",
            "Epoch 208/300, Batch 100/937, generator loss: 1.750565639381397, discriminator loss: 0.8604584408811622\n",
            "Epoch 208/300, Batch 200/937, generator loss: 1.7504942933968681, discriminator loss: 0.8604539218000887\n",
            "Epoch 208/300, Batch 300/937, generator loss: 1.7504186527362253, discriminator loss: 0.860456008062688\n",
            "Epoch 208/300, Batch 400/937, generator loss: 1.7503497795566012, discriminator loss: 0.8604688944736664\n",
            "Epoch 208/300, Batch 500/937, generator loss: 1.750266249549175, discriminator loss: 0.8604736605069697\n",
            "Epoch 208/300, Batch 600/937, generator loss: 1.7502053340855777, discriminator loss: 0.8604815679721178\n",
            "Epoch 208/300, Batch 700/937, generator loss: 1.7501269106708979, discriminator loss: 0.8604783112237218\n",
            "Epoch 208/300, Batch 800/937, generator loss: 1.7500554015269894, discriminator loss: 0.8604772591158444\n",
            "Epoch 208/300, Batch 900/937, generator loss: 1.749970802103542, discriminator loss: 0.8604971329249265\n",
            "Saving results...\n",
            "Epoch 209/300, Batch 0/937, generator loss: 1.749949841770723, discriminator loss: 0.8604975576378149\n",
            "Epoch 209/300, Batch 100/937, generator loss: 1.7498747773369399, discriminator loss: 0.8604987693240645\n",
            "Epoch 209/300, Batch 200/937, generator loss: 1.7497971072395153, discriminator loss: 0.8605029599038873\n",
            "Epoch 209/300, Batch 300/937, generator loss: 1.7497238037184266, discriminator loss: 0.860505966480187\n",
            "Epoch 209/300, Batch 400/937, generator loss: 1.749674574603635, discriminator loss: 0.8605068466679386\n",
            "Epoch 209/300, Batch 500/937, generator loss: 1.7496011420283386, discriminator loss: 0.8605144502296811\n",
            "Epoch 209/300, Batch 600/937, generator loss: 1.7495191803895322, discriminator loss: 0.8605182478830106\n",
            "Epoch 209/300, Batch 700/937, generator loss: 1.7494425641105857, discriminator loss: 0.860519742646679\n",
            "Epoch 209/300, Batch 800/937, generator loss: 1.7493935379787073, discriminator loss: 0.8605258979156933\n",
            "Epoch 209/300, Batch 900/937, generator loss: 1.7493263520870352, discriminator loss: 0.8605344396113745\n",
            "Saving results...\n",
            "Epoch 210/300, Batch 0/937, generator loss: 1.7493051545892624, discriminator loss: 0.8605337906788351\n",
            "Epoch 210/300, Batch 100/937, generator loss: 1.7492276700561205, discriminator loss: 0.8605434734530452\n",
            "Epoch 210/300, Batch 200/937, generator loss: 1.749171540272361, discriminator loss: 0.8605531413677785\n",
            "Epoch 210/300, Batch 300/937, generator loss: 1.749106414966386, discriminator loss: 0.8605545012223172\n",
            "Epoch 210/300, Batch 400/937, generator loss: 1.7490369673761876, discriminator loss: 0.8605644542543138\n",
            "Epoch 210/300, Batch 500/937, generator loss: 1.7489625766505723, discriminator loss: 0.8605727201015517\n",
            "Epoch 210/300, Batch 600/937, generator loss: 1.7489167732615298, discriminator loss: 0.86058514609679\n",
            "Epoch 210/300, Batch 700/937, generator loss: 1.7488353173316737, discriminator loss: 0.8605905487219534\n",
            "Epoch 210/300, Batch 800/937, generator loss: 1.7487485984365962, discriminator loss: 0.8605871762355913\n",
            "Epoch 210/300, Batch 900/937, generator loss: 1.7486768691585874, discriminator loss: 0.8605924863477673\n",
            "Saving results...\n",
            "Epoch 211/300, Batch 0/937, generator loss: 1.748643093637007, discriminator loss: 0.8605976979277752\n",
            "Epoch 211/300, Batch 100/937, generator loss: 1.748600341411583, discriminator loss: 0.8605951919636745\n",
            "Epoch 211/300, Batch 200/937, generator loss: 1.7485417981490332, discriminator loss: 0.8606031578176693\n",
            "Epoch 211/300, Batch 300/937, generator loss: 1.7484794775479577, discriminator loss: 0.8605997747059487\n",
            "Epoch 211/300, Batch 400/937, generator loss: 1.7484157641738347, discriminator loss: 0.8605987193504013\n",
            "Epoch 211/300, Batch 500/937, generator loss: 1.7483354495843697, discriminator loss: 0.8606145027812906\n",
            "Epoch 211/300, Batch 600/937, generator loss: 1.7482579174586277, discriminator loss: 0.8606217458446236\n",
            "Epoch 211/300, Batch 700/937, generator loss: 1.748171684201324, discriminator loss: 0.860623954163717\n",
            "Epoch 211/300, Batch 800/937, generator loss: 1.7481091801968685, discriminator loss: 0.8606376989935477\n",
            "Epoch 211/300, Batch 900/937, generator loss: 1.7480343690249025, discriminator loss: 0.8606405796019936\n",
            "Saving results...\n",
            "Epoch 212/300, Batch 0/937, generator loss: 1.7480167355441756, discriminator loss: 0.8606458401455851\n",
            "Epoch 212/300, Batch 100/937, generator loss: 1.7479513753971703, discriminator loss: 0.8606470151879856\n",
            "Epoch 212/300, Batch 200/937, generator loss: 1.7478894728250256, discriminator loss: 0.8606478672047192\n",
            "Epoch 212/300, Batch 300/937, generator loss: 1.7478227859131938, discriminator loss: 0.8606506204087687\n",
            "Epoch 212/300, Batch 400/937, generator loss: 1.7477556812265822, discriminator loss: 0.8606479187721602\n",
            "Epoch 212/300, Batch 500/937, generator loss: 1.7476844998594119, discriminator loss: 0.8606521483514193\n",
            "Epoch 212/300, Batch 600/937, generator loss: 1.7476202079943395, discriminator loss: 0.8606503723105893\n",
            "Epoch 212/300, Batch 700/937, generator loss: 1.7475411721326455, discriminator loss: 0.8606558292439639\n",
            "Epoch 212/300, Batch 800/937, generator loss: 1.7474659510998152, discriminator loss: 0.8606666050471242\n",
            "Epoch 212/300, Batch 900/937, generator loss: 1.747398782120534, discriminator loss: 0.860672206872718\n",
            "Saving results...\n",
            "Epoch 213/300, Batch 0/937, generator loss: 1.7473666671241272, discriminator loss: 0.8606751976035367\n",
            "Epoch 213/300, Batch 100/937, generator loss: 1.747317858084436, discriminator loss: 0.8606762827916402\n",
            "Epoch 213/300, Batch 200/937, generator loss: 1.7472406080169853, discriminator loss: 0.8606786670033137\n",
            "Epoch 213/300, Batch 300/937, generator loss: 1.7471889257204398, discriminator loss: 0.8606848328824952\n",
            "Epoch 213/300, Batch 400/937, generator loss: 1.747134363735564, discriminator loss: 0.8606799265173414\n",
            "Epoch 213/300, Batch 500/937, generator loss: 1.7470646454383005, discriminator loss: 0.8606872965662177\n",
            "Epoch 213/300, Batch 600/937, generator loss: 1.7470024534334216, discriminator loss: 0.8606897284024075\n",
            "Epoch 213/300, Batch 700/937, generator loss: 1.746937336815364, discriminator loss: 0.8606875521126014\n",
            "Epoch 213/300, Batch 800/937, generator loss: 1.746882193472201, discriminator loss: 0.8606781625326544\n",
            "Epoch 213/300, Batch 900/937, generator loss: 1.746823409759454, discriminator loss: 0.8606724807923509\n",
            "Saving results...\n",
            "Epoch 214/300, Batch 0/937, generator loss: 1.7467919062559598, discriminator loss: 0.860678548139894\n",
            "Epoch 214/300, Batch 100/937, generator loss: 1.7467343540896714, discriminator loss: 0.860679812752867\n",
            "Epoch 214/300, Batch 200/937, generator loss: 1.746680046638304, discriminator loss: 0.8606784105491252\n",
            "Epoch 214/300, Batch 300/937, generator loss: 1.7466287958132187, discriminator loss: 0.8606865516364287\n",
            "Epoch 214/300, Batch 400/937, generator loss: 1.7465930719897116, discriminator loss: 0.8606864520101124\n",
            "Epoch 214/300, Batch 500/937, generator loss: 1.746538167397286, discriminator loss: 0.8606881542031259\n",
            "Epoch 214/300, Batch 600/937, generator loss: 1.7464826174548962, discriminator loss: 0.8606911066537569\n",
            "Epoch 214/300, Batch 700/937, generator loss: 1.746416591406692, discriminator loss: 0.8607030109777621\n",
            "Epoch 214/300, Batch 800/937, generator loss: 1.7463516393940024, discriminator loss: 0.8607039364366349\n",
            "Epoch 214/300, Batch 900/937, generator loss: 1.7462778955024638, discriminator loss: 0.8607041288061316\n",
            "Saving results...\n",
            "Epoch 215/300, Batch 0/937, generator loss: 1.7462579845090969, discriminator loss: 0.8607041257169604\n",
            "Epoch 215/300, Batch 100/937, generator loss: 1.7461911304921964, discriminator loss: 0.8607029899541967\n",
            "Epoch 215/300, Batch 200/937, generator loss: 1.7461350787947225, discriminator loss: 0.8607010105295597\n",
            "Epoch 215/300, Batch 300/937, generator loss: 1.746078571532861, discriminator loss: 0.8606979663928475\n",
            "Epoch 215/300, Batch 400/937, generator loss: 1.746001301981159, discriminator loss: 0.8606979512494677\n",
            "Epoch 215/300, Batch 500/937, generator loss: 1.7459465283200657, discriminator loss: 0.8606992837791042\n",
            "Epoch 215/300, Batch 600/937, generator loss: 1.7458668886298365, discriminator loss: 0.860695743069366\n",
            "Epoch 215/300, Batch 700/937, generator loss: 1.7458086566032354, discriminator loss: 0.8607019370853678\n",
            "Epoch 215/300, Batch 800/937, generator loss: 1.7457631744338868, discriminator loss: 0.8607037631000154\n",
            "Epoch 215/300, Batch 900/937, generator loss: 1.7456883364594078, discriminator loss: 0.8607073958815363\n",
            "Saving results...\n",
            "Epoch 216/300, Batch 0/937, generator loss: 1.7456644252012947, discriminator loss: 0.8607076478491381\n",
            "Epoch 216/300, Batch 100/937, generator loss: 1.7456021026607458, discriminator loss: 0.8607022127466568\n",
            "Epoch 216/300, Batch 200/937, generator loss: 1.7455426698343137, discriminator loss: 0.8606976379243122\n",
            "Epoch 216/300, Batch 300/937, generator loss: 1.7454904060597185, discriminator loss: 0.8606884524833504\n",
            "Epoch 216/300, Batch 400/937, generator loss: 1.7454233807394404, discriminator loss: 0.8606766054147994\n",
            "Epoch 216/300, Batch 500/937, generator loss: 1.7453685380264286, discriminator loss: 0.8606776834075666\n",
            "Epoch 216/300, Batch 600/937, generator loss: 1.7453040213290871, discriminator loss: 0.8606710774944053\n",
            "Epoch 216/300, Batch 700/937, generator loss: 1.7452371710485832, discriminator loss: 0.86067056234308\n",
            "Epoch 216/300, Batch 800/937, generator loss: 1.7451775770453073, discriminator loss: 0.8606696936191834\n",
            "Epoch 216/300, Batch 900/937, generator loss: 1.7451264985275843, discriminator loss: 0.8606758032175501\n",
            "Saving results...\n",
            "Epoch 217/300, Batch 0/937, generator loss: 1.7451095833010584, discriminator loss: 0.8606797840249278\n",
            "Epoch 217/300, Batch 100/937, generator loss: 1.7450549150541084, discriminator loss: 0.86067625887874\n",
            "Epoch 217/300, Batch 200/937, generator loss: 1.7450099964695458, discriminator loss: 0.8606688234131968\n",
            "Epoch 217/300, Batch 300/937, generator loss: 1.7449626028025507, discriminator loss: 0.8606677590350309\n",
            "Epoch 217/300, Batch 400/937, generator loss: 1.7449063166371093, discriminator loss: 0.860661891772769\n",
            "Epoch 217/300, Batch 500/937, generator loss: 1.7448387979923061, discriminator loss: 0.8606614236942377\n",
            "Epoch 217/300, Batch 600/937, generator loss: 1.744781047584836, discriminator loss: 0.8606647129912119\n",
            "Epoch 217/300, Batch 700/937, generator loss: 1.7447075770180909, discriminator loss: 0.8606656941633765\n",
            "Epoch 217/300, Batch 800/937, generator loss: 1.744660472322093, discriminator loss: 0.8606586622856333\n",
            "Epoch 217/300, Batch 900/937, generator loss: 1.7445983044788185, discriminator loss: 0.8606658870255521\n",
            "Saving results...\n",
            "Epoch 218/300, Batch 0/937, generator loss: 1.7445798150963179, discriminator loss: 0.8606669304028216\n",
            "Epoch 218/300, Batch 100/937, generator loss: 1.744517994633857, discriminator loss: 0.8606630136462936\n",
            "Epoch 218/300, Batch 200/937, generator loss: 1.744460167029623, discriminator loss: 0.8606498748709887\n",
            "Epoch 218/300, Batch 300/937, generator loss: 1.7444039284820196, discriminator loss: 0.8606439438252136\n",
            "Epoch 218/300, Batch 400/937, generator loss: 1.74435281547584, discriminator loss: 0.860645618494351\n",
            "Epoch 218/300, Batch 500/937, generator loss: 1.7442984320222057, discriminator loss: 0.860640557118409\n",
            "Epoch 218/300, Batch 600/937, generator loss: 1.7442706300349642, discriminator loss: 0.8606389064303397\n",
            "Epoch 218/300, Batch 700/937, generator loss: 1.7442250025206822, discriminator loss: 0.8606400469772382\n",
            "Epoch 218/300, Batch 800/937, generator loss: 1.7441881732307687, discriminator loss: 0.860639730231296\n",
            "Epoch 218/300, Batch 900/937, generator loss: 1.7441487894706105, discriminator loss: 0.8606451148225184\n",
            "Saving results...\n",
            "Epoch 219/300, Batch 0/937, generator loss: 1.7441242245608297, discriminator loss: 0.860650970219691\n",
            "Epoch 219/300, Batch 100/937, generator loss: 1.7440672312249028, discriminator loss: 0.8606472976464531\n",
            "Epoch 219/300, Batch 200/937, generator loss: 1.7440233840653083, discriminator loss: 0.8606393081809361\n",
            "Epoch 219/300, Batch 300/937, generator loss: 1.7439854758864426, discriminator loss: 0.8606356821055993\n",
            "Epoch 219/300, Batch 400/937, generator loss: 1.7439493074448145, discriminator loss: 0.8606245005501666\n",
            "Epoch 219/300, Batch 500/937, generator loss: 1.7439112851502472, discriminator loss: 0.8606127793821166\n",
            "Epoch 219/300, Batch 600/937, generator loss: 1.7438641597245914, discriminator loss: 0.8605995400193511\n",
            "Epoch 219/300, Batch 700/937, generator loss: 1.74380796919816, discriminator loss: 0.8606016273539444\n",
            "Epoch 219/300, Batch 800/937, generator loss: 1.7437430341897275, discriminator loss: 0.860598877733036\n",
            "Epoch 219/300, Batch 900/937, generator loss: 1.7436970792580933, discriminator loss: 0.860600859497214\n",
            "Saving results...\n",
            "Epoch 220/300, Batch 0/937, generator loss: 1.7436732566034956, discriminator loss: 0.8605997062087798\n",
            "Epoch 220/300, Batch 100/937, generator loss: 1.743625777476521, discriminator loss: 0.860583228545883\n",
            "Epoch 220/300, Batch 200/937, generator loss: 1.7435974330022703, discriminator loss: 0.8605685492651453\n",
            "Epoch 220/300, Batch 300/937, generator loss: 1.7435421240711957, discriminator loss: 0.8605633255258737\n",
            "Epoch 220/300, Batch 400/937, generator loss: 1.7434880405651603, discriminator loss: 0.8605591785121294\n",
            "Epoch 220/300, Batch 500/937, generator loss: 1.7434305639929146, discriminator loss: 0.8605550456169416\n",
            "Epoch 220/300, Batch 600/937, generator loss: 1.7433705741982137, discriminator loss: 0.8605568006625126\n",
            "Epoch 220/300, Batch 700/937, generator loss: 1.743321937689539, discriminator loss: 0.8605527165095526\n",
            "Epoch 220/300, Batch 800/937, generator loss: 1.7432558675099445, discriminator loss: 0.8605716964753296\n",
            "Epoch 220/300, Batch 900/937, generator loss: 1.743195844318087, discriminator loss: 0.8605809469101499\n",
            "Saving results...\n",
            "Epoch 221/300, Batch 0/937, generator loss: 1.7431779265591416, discriminator loss: 0.860576669587957\n",
            "Epoch 221/300, Batch 100/937, generator loss: 1.7431362933324441, discriminator loss: 0.8605701658413678\n",
            "Epoch 221/300, Batch 200/937, generator loss: 1.7430908175733284, discriminator loss: 0.8605676211900467\n",
            "Epoch 221/300, Batch 300/937, generator loss: 1.7430503257956558, discriminator loss: 0.8605586900718039\n",
            "Epoch 221/300, Batch 400/937, generator loss: 1.7430169106996072, discriminator loss: 0.8605524916563266\n",
            "Epoch 221/300, Batch 500/937, generator loss: 1.7429728355378908, discriminator loss: 0.8605433906189909\n",
            "Epoch 221/300, Batch 600/937, generator loss: 1.7429239802573884, discriminator loss: 0.8605509236133374\n",
            "Epoch 221/300, Batch 700/937, generator loss: 1.7428681678284483, discriminator loss: 0.8605444061153312\n",
            "Epoch 221/300, Batch 800/937, generator loss: 1.7428124300951102, discriminator loss: 0.8605444812440848\n",
            "Epoch 221/300, Batch 900/937, generator loss: 1.7427896357246158, discriminator loss: 0.860550543010796\n",
            "Saving results...\n",
            "Epoch 222/300, Batch 0/937, generator loss: 1.7427693478038695, discriminator loss: 0.8605543144360485\n",
            "Epoch 222/300, Batch 100/937, generator loss: 1.7427272035121126, discriminator loss: 0.8605612369116445\n",
            "Epoch 222/300, Batch 200/937, generator loss: 1.7426947709860985, discriminator loss: 0.8605481722138014\n",
            "Epoch 222/300, Batch 300/937, generator loss: 1.7426537966621198, discriminator loss: 0.8605498137633547\n",
            "Epoch 222/300, Batch 400/937, generator loss: 1.7426050346769315, discriminator loss: 0.8605436186329393\n",
            "Epoch 222/300, Batch 500/937, generator loss: 1.7425474146088602, discriminator loss: 0.8605394111090855\n",
            "Epoch 222/300, Batch 600/937, generator loss: 1.7425099169731117, discriminator loss: 0.8605340069951969\n",
            "Epoch 222/300, Batch 700/937, generator loss: 1.742455460784609, discriminator loss: 0.860531642543188\n",
            "Epoch 222/300, Batch 800/937, generator loss: 1.7424068658986664, discriminator loss: 0.8605157047720661\n",
            "Epoch 222/300, Batch 900/937, generator loss: 1.7423613901926662, discriminator loss: 0.8605114919945647\n",
            "Saving results...\n",
            "Epoch 223/300, Batch 0/937, generator loss: 1.7423417165158304, discriminator loss: 0.860517457114592\n",
            "Epoch 223/300, Batch 100/937, generator loss: 1.7422975666677445, discriminator loss: 0.860511613760366\n",
            "Epoch 223/300, Batch 200/937, generator loss: 1.742269513884621, discriminator loss: 0.8605022371227646\n",
            "Epoch 223/300, Batch 300/937, generator loss: 1.7422436166809556, discriminator loss: 0.8604969976132082\n",
            "Epoch 223/300, Batch 400/937, generator loss: 1.7421851728069226, discriminator loss: 0.8604859147800092\n",
            "Epoch 223/300, Batch 500/937, generator loss: 1.7421375450965608, discriminator loss: 0.8604971344640345\n",
            "Epoch 223/300, Batch 600/937, generator loss: 1.7420916804557285, discriminator loss: 0.8604944510469446\n",
            "Epoch 223/300, Batch 700/937, generator loss: 1.7420543314238695, discriminator loss: 0.860498253916589\n",
            "Epoch 223/300, Batch 800/937, generator loss: 1.7420050282105324, discriminator loss: 0.8604958957893994\n",
            "Epoch 223/300, Batch 900/937, generator loss: 1.741965842303702, discriminator loss: 0.8604906834871434\n",
            "Saving results...\n",
            "Epoch 224/300, Batch 0/937, generator loss: 1.7419421888908435, discriminator loss: 0.8604884796895891\n",
            "Epoch 224/300, Batch 100/937, generator loss: 1.7418992038968444, discriminator loss: 0.8604672023199162\n",
            "Epoch 224/300, Batch 200/937, generator loss: 1.7418532715016064, discriminator loss: 0.8604639292057514\n",
            "Epoch 224/300, Batch 300/937, generator loss: 1.7418191869552813, discriminator loss: 0.8604615458131096\n",
            "Epoch 224/300, Batch 400/937, generator loss: 1.7417839970936138, discriminator loss: 0.8604563417319173\n",
            "Epoch 224/300, Batch 500/937, generator loss: 1.7417275072075051, discriminator loss: 0.8604552401223055\n",
            "Epoch 224/300, Batch 600/937, generator loss: 1.7416818920445747, discriminator loss: 0.8604501137257945\n",
            "Epoch 224/300, Batch 700/937, generator loss: 1.7416434340500109, discriminator loss: 0.860441420659824\n",
            "Epoch 224/300, Batch 800/937, generator loss: 1.7416003896404653, discriminator loss: 0.8604417689216151\n",
            "Epoch 224/300, Batch 900/937, generator loss: 1.7415455640078024, discriminator loss: 0.8604345132537794\n",
            "Saving results...\n",
            "Epoch 225/300, Batch 0/937, generator loss: 1.7415295771219501, discriminator loss: 0.8604341671373315\n",
            "Epoch 225/300, Batch 100/937, generator loss: 1.74148454965328, discriminator loss: 0.8604234996331972\n",
            "Epoch 225/300, Batch 200/937, generator loss: 1.7414478001737759, discriminator loss: 0.8604107265254054\n",
            "Epoch 225/300, Batch 300/937, generator loss: 1.741402917368194, discriminator loss: 0.8604059437607601\n",
            "Epoch 225/300, Batch 400/937, generator loss: 1.7413465402351676, discriminator loss: 0.8603975089502008\n",
            "Epoch 225/300, Batch 500/937, generator loss: 1.7413075944894096, discriminator loss: 0.8603963857029597\n",
            "Epoch 225/300, Batch 600/937, generator loss: 1.741261441407561, discriminator loss: 0.8603955330574096\n",
            "Epoch 225/300, Batch 700/937, generator loss: 1.7412269324858605, discriminator loss: 0.8603932476866905\n",
            "Epoch 225/300, Batch 800/937, generator loss: 1.7412011060751178, discriminator loss: 0.8603812235433593\n",
            "Epoch 225/300, Batch 900/937, generator loss: 1.7411605729444675, discriminator loss: 0.8603789137081247\n",
            "Saving results...\n",
            "Epoch 226/300, Batch 0/937, generator loss: 1.7411540287281349, discriminator loss: 0.8603792403008513\n",
            "Epoch 226/300, Batch 100/937, generator loss: 1.741127455507367, discriminator loss: 0.8603702843912835\n",
            "Epoch 226/300, Batch 200/937, generator loss: 1.7410731218333406, discriminator loss: 0.8603624121407547\n",
            "Epoch 226/300, Batch 300/937, generator loss: 1.741033318983633, discriminator loss: 0.86036461280421\n",
            "Epoch 226/300, Batch 400/937, generator loss: 1.7409916553957694, discriminator loss: 0.8603613306319725\n",
            "Epoch 226/300, Batch 500/937, generator loss: 1.7409300219034731, discriminator loss: 0.8603497037261073\n",
            "Epoch 226/300, Batch 600/937, generator loss: 1.7408754520780731, discriminator loss: 0.8603436806643063\n",
            "Epoch 226/300, Batch 700/937, generator loss: 1.740806069845192, discriminator loss: 0.8603447722026532\n",
            "Epoch 226/300, Batch 800/937, generator loss: 1.7407454883982791, discriminator loss: 0.8603422806874577\n",
            "Epoch 226/300, Batch 900/937, generator loss: 1.7406930065555042, discriminator loss: 0.8603385575238477\n",
            "Saving results...\n",
            "Epoch 227/300, Batch 0/937, generator loss: 1.7406731103746669, discriminator loss: 0.860336579234739\n",
            "Epoch 227/300, Batch 100/937, generator loss: 1.7406360263119738, discriminator loss: 0.8603199798225852\n",
            "Epoch 227/300, Batch 200/937, generator loss: 1.7406092838285785, discriminator loss: 0.8603014436456562\n",
            "Epoch 227/300, Batch 300/937, generator loss: 1.7405611945329138, discriminator loss: 0.8603015051675794\n",
            "Epoch 227/300, Batch 400/937, generator loss: 1.7405238878307405, discriminator loss: 0.8603002053050299\n",
            "Epoch 227/300, Batch 500/937, generator loss: 1.740502624809742, discriminator loss: 0.8602980551946957\n",
            "Epoch 227/300, Batch 600/937, generator loss: 1.7404645474874745, discriminator loss: 0.8602940509388327\n",
            "Epoch 227/300, Batch 700/937, generator loss: 1.740409887749789, discriminator loss: 0.8603043640719861\n",
            "Epoch 227/300, Batch 800/937, generator loss: 1.7403554428422199, discriminator loss: 0.8603038372791064\n",
            "Epoch 227/300, Batch 900/937, generator loss: 1.7403176808228877, discriminator loss: 0.8603027748968103\n",
            "Saving results...\n",
            "Epoch 228/300, Batch 0/937, generator loss: 1.7403081461731493, discriminator loss: 0.8602961908386457\n",
            "Epoch 228/300, Batch 100/937, generator loss: 1.7402799854378421, discriminator loss: 0.860278580469511\n",
            "Epoch 228/300, Batch 200/937, generator loss: 1.7402466451272083, discriminator loss: 0.8602672022137098\n",
            "Epoch 228/300, Batch 300/937, generator loss: 1.740221893054031, discriminator loss: 0.8602545684947225\n",
            "Epoch 228/300, Batch 400/937, generator loss: 1.7401751465108068, discriminator loss: 0.8602520628768113\n",
            "Epoch 228/300, Batch 500/937, generator loss: 1.7401303422036447, discriminator loss: 0.8602411833025925\n",
            "Epoch 228/300, Batch 600/937, generator loss: 1.7400733560895871, discriminator loss: 0.8602403921835705\n",
            "Epoch 228/300, Batch 700/937, generator loss: 1.7400254961549273, discriminator loss: 0.8602409324334727\n",
            "Epoch 228/300, Batch 800/937, generator loss: 1.7399820579965235, discriminator loss: 0.8602386636518413\n",
            "Epoch 228/300, Batch 900/937, generator loss: 1.7399346828032836, discriminator loss: 0.8602326801983404\n",
            "Saving results...\n",
            "Epoch 229/300, Batch 0/937, generator loss: 1.739910664339808, discriminator loss: 0.8602296314932945\n",
            "Epoch 229/300, Batch 100/937, generator loss: 1.7398615004690097, discriminator loss: 0.8602230081669056\n",
            "Epoch 229/300, Batch 200/937, generator loss: 1.7398202969003376, discriminator loss: 0.8602201055625693\n",
            "Epoch 229/300, Batch 300/937, generator loss: 1.7397764541686347, discriminator loss: 0.8602077949782125\n",
            "Epoch 229/300, Batch 400/937, generator loss: 1.7397403424948479, discriminator loss: 0.8602025052812504\n",
            "Epoch 229/300, Batch 500/937, generator loss: 1.7397040896866576, discriminator loss: 0.8601879206242055\n",
            "Epoch 229/300, Batch 600/937, generator loss: 1.73965861128428, discriminator loss: 0.8601814969727712\n",
            "Epoch 229/300, Batch 700/937, generator loss: 1.7396215963655144, discriminator loss: 0.8601721667799683\n",
            "Epoch 229/300, Batch 800/937, generator loss: 1.7395741426274003, discriminator loss: 0.8601712148876625\n",
            "Epoch 229/300, Batch 900/937, generator loss: 1.7395204810305187, discriminator loss: 0.8601673940397512\n",
            "Saving results...\n",
            "Epoch 230/300, Batch 0/937, generator loss: 1.7395041944760348, discriminator loss: 0.8601694761886439\n",
            "Epoch 230/300, Batch 100/937, generator loss: 1.7394790282947132, discriminator loss: 0.8601695017313972\n",
            "Epoch 230/300, Batch 200/937, generator loss: 1.7394461372462326, discriminator loss: 0.860153437253605\n",
            "Epoch 230/300, Batch 300/937, generator loss: 1.7394133926138895, discriminator loss: 0.8601539107585433\n",
            "Epoch 230/300, Batch 400/937, generator loss: 1.7393817477343947, discriminator loss: 0.8601480533260587\n",
            "Epoch 230/300, Batch 500/937, generator loss: 1.7393462338899814, discriminator loss: 0.8601334823864061\n",
            "Epoch 230/300, Batch 600/937, generator loss: 1.7393172162225323, discriminator loss: 0.86012694195322\n",
            "Epoch 230/300, Batch 700/937, generator loss: 1.7392783708196276, discriminator loss: 0.8601211145356251\n",
            "Epoch 230/300, Batch 800/937, generator loss: 1.7392359804829032, discriminator loss: 0.8601146412742189\n",
            "Epoch 230/300, Batch 900/937, generator loss: 1.7392053249198909, discriminator loss: 0.8601124891876543\n",
            "Saving results...\n",
            "Epoch 231/300, Batch 0/937, generator loss: 1.7391964163860314, discriminator loss: 0.8601113930161456\n",
            "Epoch 231/300, Batch 100/937, generator loss: 1.739170174658522, discriminator loss: 0.860100344689229\n",
            "Epoch 231/300, Batch 200/937, generator loss: 1.7391386971728875, discriminator loss: 0.8600981000292669\n",
            "Epoch 231/300, Batch 300/937, generator loss: 1.7391227519608754, discriminator loss: 0.86008521002722\n",
            "Epoch 231/300, Batch 400/937, generator loss: 1.73909019670695, discriminator loss: 0.8600784728696201\n",
            "Epoch 231/300, Batch 500/937, generator loss: 1.739054301369517, discriminator loss: 0.8600758879722656\n",
            "Epoch 231/300, Batch 600/937, generator loss: 1.7390153803309454, discriminator loss: 0.8600691938891225\n",
            "Epoch 231/300, Batch 700/937, generator loss: 1.7389761105023054, discriminator loss: 0.8600684222330974\n",
            "Epoch 231/300, Batch 800/937, generator loss: 1.738952486890636, discriminator loss: 0.8600627674719398\n",
            "Epoch 231/300, Batch 900/937, generator loss: 1.738911115044026, discriminator loss: 0.860060557268225\n",
            "Saving results...\n",
            "Epoch 232/300, Batch 0/937, generator loss: 1.7388914349848124, discriminator loss: 0.8600647487533063\n",
            "Epoch 232/300, Batch 100/937, generator loss: 1.7388756837264272, discriminator loss: 0.8600473664207158\n",
            "Epoch 232/300, Batch 200/937, generator loss: 1.7388505037494324, discriminator loss: 0.8600305980734739\n",
            "Epoch 232/300, Batch 300/937, generator loss: 1.7388284209971834, discriminator loss: 0.8600206833705588\n",
            "Epoch 232/300, Batch 400/937, generator loss: 1.7387948450778452, discriminator loss: 0.8600090678770751\n",
            "Epoch 232/300, Batch 500/937, generator loss: 1.7387697797760782, discriminator loss: 0.860001704903181\n",
            "Epoch 232/300, Batch 600/937, generator loss: 1.7387439405081138, discriminator loss: 0.8599929520770038\n",
            "Epoch 232/300, Batch 700/937, generator loss: 1.7387186582903424, discriminator loss: 0.8599854403077855\n",
            "Epoch 232/300, Batch 800/937, generator loss: 1.7386689400796937, discriminator loss: 0.8599777573831583\n",
            "Epoch 232/300, Batch 900/937, generator loss: 1.7386126593417144, discriminator loss: 0.859979573917047\n",
            "Saving results...\n",
            "Epoch 233/300, Batch 0/937, generator loss: 1.7385927111978168, discriminator loss: 0.8599789498342624\n",
            "Epoch 233/300, Batch 100/937, generator loss: 1.7385609486981735, discriminator loss: 0.8599739429003762\n",
            "Epoch 233/300, Batch 200/937, generator loss: 1.7385267795559873, discriminator loss: 0.8599572077949559\n",
            "Epoch 233/300, Batch 300/937, generator loss: 1.7384860283225194, discriminator loss: 0.8599547100321401\n",
            "Epoch 233/300, Batch 400/937, generator loss: 1.738466047634374, discriminator loss: 0.859943828536867\n",
            "Epoch 233/300, Batch 500/937, generator loss: 1.7384416800670537, discriminator loss: 0.8599321576991859\n",
            "Epoch 233/300, Batch 600/937, generator loss: 1.738410095200085, discriminator loss: 0.8599333849235672\n",
            "Epoch 233/300, Batch 700/937, generator loss: 1.7383783576449545, discriminator loss: 0.8599272128584241\n",
            "Epoch 233/300, Batch 800/937, generator loss: 1.7383373870875374, discriminator loss: 0.8599245863309224\n",
            "Epoch 233/300, Batch 900/937, generator loss: 1.7382968290633602, discriminator loss: 0.8599179253415671\n",
            "Saving results...\n",
            "Epoch 234/300, Batch 0/937, generator loss: 1.7382806964925315, discriminator loss: 0.8599152019830582\n",
            "Epoch 234/300, Batch 100/937, generator loss: 1.738264988106603, discriminator loss: 0.859903038334836\n",
            "Epoch 234/300, Batch 200/937, generator loss: 1.7382576334549749, discriminator loss: 0.8598965038238121\n",
            "Epoch 234/300, Batch 300/937, generator loss: 1.7382185992730728, discriminator loss: 0.8598823034484369\n",
            "Epoch 234/300, Batch 400/937, generator loss: 1.7381928565790787, discriminator loss: 0.8598790850988312\n",
            "Epoch 234/300, Batch 500/937, generator loss: 1.7381710397242942, discriminator loss: 0.8598670884311497\n",
            "Epoch 234/300, Batch 600/937, generator loss: 1.7381335411802934, discriminator loss: 0.8598618113792463\n",
            "Epoch 234/300, Batch 700/937, generator loss: 1.7380924337895827, discriminator loss: 0.859871546848829\n",
            "Epoch 234/300, Batch 800/937, generator loss: 1.7380658425532414, discriminator loss: 0.8598677903034978\n",
            "Epoch 234/300, Batch 900/937, generator loss: 1.7380300634079797, discriminator loss: 0.8598611575142171\n",
            "Saving results...\n",
            "Epoch 235/300, Batch 0/937, generator loss: 1.7380223857311399, discriminator loss: 0.8598572437307731\n",
            "Epoch 235/300, Batch 100/937, generator loss: 1.7379979936439633, discriminator loss: 0.8598438123695009\n",
            "Epoch 235/300, Batch 200/937, generator loss: 1.737983088861886, discriminator loss: 0.8598259227206106\n",
            "Epoch 235/300, Batch 300/937, generator loss: 1.7379423825651588, discriminator loss: 0.8598176897550404\n",
            "Epoch 235/300, Batch 400/937, generator loss: 1.7379114492441812, discriminator loss: 0.859805942468922\n",
            "Epoch 235/300, Batch 500/937, generator loss: 1.7378947245369023, discriminator loss: 0.8597971213394837\n",
            "Epoch 235/300, Batch 600/937, generator loss: 1.7378543682100678, discriminator loss: 0.8597854647753909\n",
            "Epoch 235/300, Batch 700/937, generator loss: 1.7378265557784491, discriminator loss: 0.8597785320801343\n",
            "Epoch 235/300, Batch 800/937, generator loss: 1.7378012366302542, discriminator loss: 0.8597709252431364\n",
            "Epoch 235/300, Batch 900/937, generator loss: 1.7377835425753176, discriminator loss: 0.8597704289551743\n",
            "Saving results...\n",
            "Epoch 236/300, Batch 0/937, generator loss: 1.7377634465022997, discriminator loss: 0.859767868671816\n",
            "Epoch 236/300, Batch 100/937, generator loss: 1.7377438621699288, discriminator loss: 0.8597529161240334\n",
            "Epoch 236/300, Batch 200/937, generator loss: 1.7377199301221118, discriminator loss: 0.8597427374785305\n",
            "Epoch 236/300, Batch 300/937, generator loss: 1.7377081160979628, discriminator loss: 0.8597223580651806\n",
            "Epoch 236/300, Batch 400/937, generator loss: 1.7376859998636063, discriminator loss: 0.8597100023141822\n",
            "Epoch 236/300, Batch 500/937, generator loss: 1.7376709716757528, discriminator loss: 0.8597021188150622\n",
            "Epoch 236/300, Batch 600/937, generator loss: 1.7376444789040355, discriminator loss: 0.8596959922530224\n",
            "Epoch 236/300, Batch 700/937, generator loss: 1.7376207251577636, discriminator loss: 0.859693829268962\n",
            "Epoch 236/300, Batch 800/937, generator loss: 1.7375825868332893, discriminator loss: 0.85969484958373\n",
            "Epoch 236/300, Batch 900/937, generator loss: 1.7375584402809938, discriminator loss: 0.8596879464831614\n",
            "Saving results...\n",
            "Epoch 237/300, Batch 0/937, generator loss: 1.7375511060251907, discriminator loss: 0.8596872563984982\n",
            "Epoch 237/300, Batch 100/937, generator loss: 1.73752418696604, discriminator loss: 0.8596743201188116\n",
            "Epoch 237/300, Batch 200/937, generator loss: 1.7375007888568945, discriminator loss: 0.8596566262055909\n",
            "Epoch 237/300, Batch 300/937, generator loss: 1.7374901382363561, discriminator loss: 0.8596491208367089\n",
            "Epoch 237/300, Batch 400/937, generator loss: 1.7374724651843707, discriminator loss: 0.8596450037476282\n",
            "Epoch 237/300, Batch 500/937, generator loss: 1.7374323589448182, discriminator loss: 0.8596332853346408\n",
            "Epoch 237/300, Batch 600/937, generator loss: 1.7374088001394699, discriminator loss: 0.8596264389318841\n",
            "Epoch 237/300, Batch 700/937, generator loss: 1.7373878849223212, discriminator loss: 0.8596063950420919\n",
            "Epoch 237/300, Batch 800/937, generator loss: 1.7373541785339148, discriminator loss: 0.8596104230675395\n",
            "Epoch 237/300, Batch 900/937, generator loss: 1.7373238550818446, discriminator loss: 0.8595974138641206\n",
            "Saving results...\n",
            "Epoch 238/300, Batch 0/937, generator loss: 1.7373070406543953, discriminator loss: 0.8595935140148145\n",
            "Epoch 238/300, Batch 100/937, generator loss: 1.7372980395774933, discriminator loss: 0.8595782732752454\n",
            "Epoch 238/300, Batch 200/937, generator loss: 1.7372861299509985, discriminator loss: 0.8595655643687675\n",
            "Epoch 238/300, Batch 300/937, generator loss: 1.7372712005099558, discriminator loss: 0.8595547849504347\n",
            "Epoch 238/300, Batch 400/937, generator loss: 1.7372431189245068, discriminator loss: 0.8595403957210439\n",
            "Epoch 238/300, Batch 500/937, generator loss: 1.7372133413747215, discriminator loss: 0.8595376947676957\n",
            "Epoch 238/300, Batch 600/937, generator loss: 1.7371759481323472, discriminator loss: 0.8595276972661224\n",
            "Epoch 238/300, Batch 700/937, generator loss: 1.7371675671275317, discriminator loss: 0.8595207854854613\n",
            "Epoch 238/300, Batch 800/937, generator loss: 1.7371477623640394, discriminator loss: 0.8595121242745299\n",
            "Epoch 238/300, Batch 900/937, generator loss: 1.7371292361474924, discriminator loss: 0.8595062628168588\n",
            "Saving results...\n",
            "Epoch 239/300, Batch 0/937, generator loss: 1.7371248022582597, discriminator loss: 0.8595055329537167\n",
            "Epoch 239/300, Batch 100/937, generator loss: 1.7371034003346315, discriminator loss: 0.8594852998750039\n",
            "Epoch 239/300, Batch 200/937, generator loss: 1.7370813580538214, discriminator loss: 0.8594736726566999\n",
            "Epoch 239/300, Batch 300/937, generator loss: 1.737063787576804, discriminator loss: 0.8594604178936709\n",
            "Epoch 239/300, Batch 400/937, generator loss: 1.7370417665233073, discriminator loss: 0.8594497208322546\n",
            "Epoch 239/300, Batch 500/937, generator loss: 1.7370331015986433, discriminator loss: 0.8594343618259042\n",
            "Epoch 239/300, Batch 600/937, generator loss: 1.7369974304982845, discriminator loss: 0.8594260358770448\n",
            "Epoch 239/300, Batch 700/937, generator loss: 1.7369676331077613, discriminator loss: 0.8594105929971798\n",
            "Epoch 239/300, Batch 800/937, generator loss: 1.7369441729306418, discriminator loss: 0.8594088221617748\n",
            "Epoch 239/300, Batch 900/937, generator loss: 1.7369146884808926, discriminator loss: 0.8594008284351191\n",
            "Saving results...\n",
            "Epoch 240/300, Batch 0/937, generator loss: 1.7369109457723535, discriminator loss: 0.85939614371954\n",
            "Epoch 240/300, Batch 100/937, generator loss: 1.7368952349318374, discriminator loss: 0.8593825648816256\n",
            "Epoch 240/300, Batch 200/937, generator loss: 1.736872369200626, discriminator loss: 0.8593759528622636\n",
            "Epoch 240/300, Batch 300/937, generator loss: 1.7368611777616663, discriminator loss: 0.8593580015141158\n",
            "Epoch 240/300, Batch 400/937, generator loss: 1.7368262875783413, discriminator loss: 0.8593444340378976\n",
            "Epoch 240/300, Batch 500/937, generator loss: 1.7367999753941405, discriminator loss: 0.8593290590163729\n",
            "Epoch 240/300, Batch 600/937, generator loss: 1.7367843954129647, discriminator loss: 0.8593202346357441\n",
            "Epoch 240/300, Batch 700/937, generator loss: 1.7367516507594298, discriminator loss: 0.859313115954426\n",
            "Epoch 240/300, Batch 800/937, generator loss: 1.7367240567317228, discriminator loss: 0.8593060734415088\n",
            "Epoch 240/300, Batch 900/937, generator loss: 1.7366888990591494, discriminator loss: 0.8593051609985917\n",
            "Saving results...\n",
            "Epoch 241/300, Batch 0/937, generator loss: 1.7366839624387516, discriminator loss: 0.8593006695149837\n",
            "Epoch 241/300, Batch 100/937, generator loss: 1.7366883185162065, discriminator loss: 0.8592852016447537\n",
            "Epoch 241/300, Batch 200/937, generator loss: 1.7366689867106186, discriminator loss: 0.8592646173871546\n",
            "Epoch 241/300, Batch 300/937, generator loss: 1.7366697609813138, discriminator loss: 0.8592477234861167\n",
            "Epoch 241/300, Batch 400/937, generator loss: 1.7366608942095374, discriminator loss: 0.8592394271061188\n",
            "Epoch 241/300, Batch 500/937, generator loss: 1.736648788174265, discriminator loss: 0.8592286360450657\n",
            "Epoch 241/300, Batch 600/937, generator loss: 1.736624758156625, discriminator loss: 0.8592126100999011\n",
            "Epoch 241/300, Batch 700/937, generator loss: 1.736602531886231, discriminator loss: 0.8591998237292399\n",
            "Epoch 241/300, Batch 800/937, generator loss: 1.7365829647058175, discriminator loss: 0.8591966096346888\n",
            "Epoch 241/300, Batch 900/937, generator loss: 1.73655154811544, discriminator loss: 0.8591991497898936\n",
            "Saving results...\n",
            "Epoch 242/300, Batch 0/937, generator loss: 1.736555508968888, discriminator loss: 0.8591950427156854\n",
            "Epoch 242/300, Batch 100/937, generator loss: 1.7365383787489992, discriminator loss: 0.8591781611900633\n",
            "Epoch 242/300, Batch 200/937, generator loss: 1.7365185468211735, discriminator loss: 0.8591611978914016\n",
            "Epoch 242/300, Batch 300/937, generator loss: 1.7365042355833884, discriminator loss: 0.8591505368407549\n",
            "Epoch 242/300, Batch 400/937, generator loss: 1.7364858532499787, discriminator loss: 0.8591438586602307\n",
            "Epoch 242/300, Batch 500/937, generator loss: 1.7364631196416063, discriminator loss: 0.8591336084453163\n",
            "Epoch 242/300, Batch 600/937, generator loss: 1.7364501403007333, discriminator loss: 0.8591219697474741\n",
            "Epoch 242/300, Batch 700/937, generator loss: 1.7364417185328151, discriminator loss: 0.8591099431219357\n",
            "Epoch 242/300, Batch 800/937, generator loss: 1.7364104857491598, discriminator loss: 0.8591065545676004\n",
            "Epoch 242/300, Batch 900/937, generator loss: 1.7363798424679773, discriminator loss: 0.8590948868963213\n",
            "Saving results...\n",
            "Epoch 243/300, Batch 0/937, generator loss: 1.7363762313294322, discriminator loss: 0.8590878690337302\n",
            "Epoch 243/300, Batch 100/937, generator loss: 1.7363628125341661, discriminator loss: 0.859069230551731\n",
            "Epoch 243/300, Batch 200/937, generator loss: 1.736363558166055, discriminator loss: 0.8590548540663542\n",
            "Epoch 243/300, Batch 300/937, generator loss: 1.7363524149933791, discriminator loss: 0.8590405556510257\n",
            "Epoch 243/300, Batch 400/937, generator loss: 1.7363299125278582, discriminator loss: 0.8590312773984425\n",
            "Epoch 243/300, Batch 500/937, generator loss: 1.7363081656296087, discriminator loss: 0.8590191547607249\n",
            "Epoch 243/300, Batch 600/937, generator loss: 1.7363033350075705, discriminator loss: 0.8590086557443254\n",
            "Epoch 243/300, Batch 700/937, generator loss: 1.7362943140815246, discriminator loss: 0.8589910784268282\n",
            "Epoch 243/300, Batch 800/937, generator loss: 1.7362807993038616, discriminator loss: 0.858983463987591\n",
            "Epoch 243/300, Batch 900/937, generator loss: 1.7362517631812384, discriminator loss: 0.8589750025017223\n",
            "Saving results...\n",
            "Epoch 244/300, Batch 0/937, generator loss: 1.736247149057165, discriminator loss: 0.8589774341324351\n",
            "Epoch 244/300, Batch 100/937, generator loss: 1.7362290915703877, discriminator loss: 0.8589578653439739\n",
            "Epoch 244/300, Batch 200/937, generator loss: 1.7362168597885608, discriminator loss: 0.8589385360550595\n",
            "Epoch 244/300, Batch 300/937, generator loss: 1.7362056437898792, discriminator loss: 0.8589235514579306\n",
            "Epoch 244/300, Batch 400/937, generator loss: 1.7361945590585275, discriminator loss: 0.8589068344137658\n",
            "Epoch 244/300, Batch 500/937, generator loss: 1.7361731395624915, discriminator loss: 0.8588896278635954\n",
            "Epoch 244/300, Batch 600/937, generator loss: 1.7361384119935546, discriminator loss: 0.8588737591117758\n",
            "Epoch 244/300, Batch 700/937, generator loss: 1.7361260844552402, discriminator loss: 0.8588723391213258\n",
            "Epoch 244/300, Batch 800/937, generator loss: 1.7361011419150196, discriminator loss: 0.8588738835388859\n",
            "Epoch 244/300, Batch 900/937, generator loss: 1.736069394287889, discriminator loss: 0.858857871988911\n",
            "Saving results...\n",
            "Epoch 245/300, Batch 0/937, generator loss: 1.736062245516828, discriminator loss: 0.8588525321468542\n",
            "Epoch 245/300, Batch 100/937, generator loss: 1.7360448059315423, discriminator loss: 0.8588368346246187\n",
            "Epoch 245/300, Batch 200/937, generator loss: 1.7360279615276963, discriminator loss: 0.8588189955094483\n",
            "Epoch 245/300, Batch 300/937, generator loss: 1.7360195841324826, discriminator loss: 0.858794034076705\n",
            "Epoch 245/300, Batch 400/937, generator loss: 1.7359902358566857, discriminator loss: 0.8587856305825314\n",
            "Epoch 245/300, Batch 500/937, generator loss: 1.735969779147537, discriminator loss: 0.8587711176369967\n",
            "Epoch 245/300, Batch 600/937, generator loss: 1.7359485840901632, discriminator loss: 0.8587673721476531\n",
            "Epoch 245/300, Batch 700/937, generator loss: 1.7359181998064557, discriminator loss: 0.858759468667006\n",
            "Epoch 245/300, Batch 800/937, generator loss: 1.7359088580945536, discriminator loss: 0.8587364315311066\n",
            "Epoch 245/300, Batch 900/937, generator loss: 1.7358750157850218, discriminator loss: 0.8587174781665321\n",
            "Saving results...\n",
            "Epoch 246/300, Batch 0/937, generator loss: 1.7358634274034324, discriminator loss: 0.8587122860583165\n",
            "Epoch 246/300, Batch 100/937, generator loss: 1.735847047382151, discriminator loss: 0.858689428884888\n",
            "Epoch 246/300, Batch 200/937, generator loss: 1.7358380510686713, discriminator loss: 0.8586725494266835\n",
            "Epoch 246/300, Batch 300/937, generator loss: 1.735839217088133, discriminator loss: 0.8586530745029965\n",
            "Epoch 246/300, Batch 400/937, generator loss: 1.7358328825842944, discriminator loss: 0.8586356455909648\n",
            "Epoch 246/300, Batch 500/937, generator loss: 1.7358271081924959, discriminator loss: 0.8586216078101222\n",
            "Epoch 246/300, Batch 600/937, generator loss: 1.7358135606428784, discriminator loss: 0.8586085432537113\n",
            "Epoch 246/300, Batch 700/937, generator loss: 1.735798242761356, discriminator loss: 0.8585940038568325\n",
            "Epoch 246/300, Batch 800/937, generator loss: 1.735786712903602, discriminator loss: 0.858580857457097\n",
            "Epoch 246/300, Batch 900/937, generator loss: 1.7357675404729838, discriminator loss: 0.8585743421127934\n",
            "Saving results...\n",
            "Epoch 247/300, Batch 0/937, generator loss: 1.7357683281563905, discriminator loss: 0.8585683752989639\n",
            "Epoch 247/300, Batch 100/937, generator loss: 1.7357442131924528, discriminator loss: 0.8585500521419466\n",
            "Epoch 247/300, Batch 200/937, generator loss: 1.7357270755029428, discriminator loss: 0.858533596075033\n",
            "Epoch 247/300, Batch 300/937, generator loss: 1.735711766314525, discriminator loss: 0.8585242621287481\n",
            "Epoch 247/300, Batch 400/937, generator loss: 1.7357135767239262, discriminator loss: 0.8585068447086022\n",
            "Epoch 247/300, Batch 500/937, generator loss: 1.7357081960303686, discriminator loss: 0.8584918250923343\n",
            "Epoch 247/300, Batch 600/937, generator loss: 1.7356935118220218, discriminator loss: 0.8584774531878709\n",
            "Epoch 247/300, Batch 700/937, generator loss: 1.735687800262329, discriminator loss: 0.8584589059600958\n",
            "Epoch 247/300, Batch 800/937, generator loss: 1.7356674182370864, discriminator loss: 0.8584481286638889\n",
            "Epoch 247/300, Batch 900/937, generator loss: 1.7356520903262376, discriminator loss: 0.8584344923471875\n",
            "Saving results...\n",
            "Epoch 248/300, Batch 0/937, generator loss: 1.7356464809374066, discriminator loss: 0.8584301221693477\n",
            "Epoch 248/300, Batch 100/937, generator loss: 1.735652470630014, discriminator loss: 0.8584082540349204\n",
            "Epoch 248/300, Batch 200/937, generator loss: 1.7356535992015323, discriminator loss: 0.8583864661105697\n",
            "Epoch 248/300, Batch 300/937, generator loss: 1.7356415050363398, discriminator loss: 0.8583657542404658\n",
            "Epoch 248/300, Batch 400/937, generator loss: 1.7356299928090737, discriminator loss: 0.85834830654783\n",
            "Epoch 248/300, Batch 500/937, generator loss: 1.7356345036666034, discriminator loss: 0.8583307010300388\n",
            "Epoch 248/300, Batch 600/937, generator loss: 1.735629211148464, discriminator loss: 0.8583171690075545\n",
            "Epoch 248/300, Batch 700/937, generator loss: 1.7356297270306105, discriminator loss: 0.8583048292700014\n",
            "Epoch 248/300, Batch 800/937, generator loss: 1.7356098774171405, discriminator loss: 0.8583012624184558\n",
            "Epoch 248/300, Batch 900/937, generator loss: 1.7355914366929168, discriminator loss: 0.8582889223140199\n",
            "Saving results...\n",
            "Epoch 249/300, Batch 0/937, generator loss: 1.735592800516191, discriminator loss: 0.8582822182697111\n",
            "Epoch 249/300, Batch 100/937, generator loss: 1.735573671193765, discriminator loss: 0.8582638627024027\n",
            "Epoch 249/300, Batch 200/937, generator loss: 1.7355731166560677, discriminator loss: 0.8582454902786004\n",
            "Epoch 249/300, Batch 300/937, generator loss: 1.7355745127565483, discriminator loss: 0.8582191311854183\n",
            "Epoch 249/300, Batch 400/937, generator loss: 1.7355712974067523, discriminator loss: 0.8582022385893852\n",
            "Epoch 249/300, Batch 500/937, generator loss: 1.7355597932988647, discriminator loss: 0.8581928003636767\n",
            "Epoch 249/300, Batch 600/937, generator loss: 1.7355668456058218, discriminator loss: 0.8581728245407007\n",
            "Epoch 249/300, Batch 700/937, generator loss: 1.7355485156645731, discriminator loss: 0.8581637165686495\n",
            "Epoch 249/300, Batch 800/937, generator loss: 1.7355375775738358, discriminator loss: 0.858149768261812\n",
            "Epoch 249/300, Batch 900/937, generator loss: 1.7355060837046652, discriminator loss: 0.8581424798360495\n",
            "Saving results...\n",
            "Epoch 250/300, Batch 0/937, generator loss: 1.7354996141283352, discriminator loss: 0.8581420168218962\n",
            "Epoch 250/300, Batch 100/937, generator loss: 1.7355121308119055, discriminator loss: 0.8581282251527501\n",
            "Epoch 250/300, Batch 200/937, generator loss: 1.7355111845064852, discriminator loss: 0.858106492520478\n",
            "Epoch 250/300, Batch 300/937, generator loss: 1.7355191887006196, discriminator loss: 0.8580924104731368\n",
            "Epoch 250/300, Batch 400/937, generator loss: 1.7355138762543425, discriminator loss: 0.8580739868697896\n",
            "Epoch 250/300, Batch 500/937, generator loss: 1.7355042331212656, discriminator loss: 0.8580615369511243\n",
            "Epoch 250/300, Batch 600/937, generator loss: 1.7354815748741288, discriminator loss: 0.858049883333416\n",
            "Epoch 250/300, Batch 700/937, generator loss: 1.7354762204401661, discriminator loss: 0.8580348962817376\n",
            "Epoch 250/300, Batch 800/937, generator loss: 1.735465738853082, discriminator loss: 0.8580187233933299\n",
            "Epoch 250/300, Batch 900/937, generator loss: 1.7354421139855394, discriminator loss: 0.858009448998345\n",
            "Saving results...\n",
            "Epoch 251/300, Batch 0/937, generator loss: 1.7354357286519553, discriminator loss: 0.8580041240768272\n",
            "Epoch 251/300, Batch 100/937, generator loss: 1.7354134498657727, discriminator loss: 0.857979138306193\n",
            "Epoch 251/300, Batch 200/937, generator loss: 1.7354105686897112, discriminator loss: 0.8579576493479694\n",
            "Epoch 251/300, Batch 300/937, generator loss: 1.7353973576176636, discriminator loss: 0.8579388608756603\n",
            "Epoch 251/300, Batch 400/937, generator loss: 1.7353977359646513, discriminator loss: 0.8579117808326729\n",
            "Epoch 251/300, Batch 500/937, generator loss: 1.735388765425094, discriminator loss: 0.8578954522542936\n",
            "Epoch 251/300, Batch 600/937, generator loss: 1.7353750053322814, discriminator loss: 0.8578853682421684\n",
            "Epoch 251/300, Batch 700/937, generator loss: 1.73536855123878, discriminator loss: 0.8578728368579239\n",
            "Epoch 251/300, Batch 800/937, generator loss: 1.7353582371775857, discriminator loss: 0.8578560868981283\n",
            "Epoch 251/300, Batch 900/937, generator loss: 1.7353606936978954, discriminator loss: 0.8578405646871551\n",
            "Saving results...\n",
            "Epoch 252/300, Batch 0/937, generator loss: 1.73536355616568, discriminator loss: 0.8578353341330038\n",
            "Epoch 252/300, Batch 100/937, generator loss: 1.7353684816063744, discriminator loss: 0.857815886675359\n",
            "Epoch 252/300, Batch 200/937, generator loss: 1.7353726215694463, discriminator loss: 0.857790979315689\n",
            "Epoch 252/300, Batch 300/937, generator loss: 1.7353698793359622, discriminator loss: 0.8577664951075834\n",
            "Epoch 252/300, Batch 400/937, generator loss: 1.735374960538487, discriminator loss: 0.8577433522657818\n",
            "Epoch 252/300, Batch 500/937, generator loss: 1.735358917735584, discriminator loss: 0.8577269034968817\n",
            "Epoch 252/300, Batch 600/937, generator loss: 1.735351620794589, discriminator loss: 0.857719085747922\n",
            "Epoch 252/300, Batch 700/937, generator loss: 1.7353514475139382, discriminator loss: 0.8577014731722825\n",
            "Epoch 252/300, Batch 800/937, generator loss: 1.7353382844473095, discriminator loss: 0.8576917212307799\n",
            "Epoch 252/300, Batch 900/937, generator loss: 1.7353155557868627, discriminator loss: 0.8576837468706798\n",
            "Saving results...\n",
            "Epoch 253/300, Batch 0/937, generator loss: 1.7353055986525676, discriminator loss: 0.8576767660281492\n",
            "Epoch 253/300, Batch 100/937, generator loss: 1.735294725327618, discriminator loss: 0.8576638462029911\n",
            "Epoch 253/300, Batch 200/937, generator loss: 1.7352865905557167, discriminator loss: 0.857638286106501\n",
            "Epoch 253/300, Batch 300/937, generator loss: 1.7352970763730196, discriminator loss: 0.8576208601874078\n",
            "Epoch 253/300, Batch 400/937, generator loss: 1.7352923753377412, discriminator loss: 0.8576001369844268\n",
            "Epoch 253/300, Batch 500/937, generator loss: 1.7352905945803143, discriminator loss: 0.8575755725831011\n",
            "Epoch 253/300, Batch 600/937, generator loss: 1.7352857189436446, discriminator loss: 0.8575585887748071\n",
            "Epoch 253/300, Batch 700/937, generator loss: 1.73527842805812, discriminator loss: 0.8575403171713649\n",
            "Epoch 253/300, Batch 800/937, generator loss: 1.73528134766965, discriminator loss: 0.8575177269170283\n",
            "Epoch 253/300, Batch 900/937, generator loss: 1.7352619784943026, discriminator loss: 0.857505645804609\n",
            "Saving results...\n",
            "Epoch 254/300, Batch 0/937, generator loss: 1.7352617755190727, discriminator loss: 0.8575040707421693\n",
            "Epoch 254/300, Batch 100/937, generator loss: 1.7352776231623317, discriminator loss: 0.8574766994472167\n",
            "Epoch 254/300, Batch 200/937, generator loss: 1.7352767298435074, discriminator loss: 0.8574560405194365\n",
            "Epoch 254/300, Batch 300/937, generator loss: 1.7352695048403903, discriminator loss: 0.8574377378236417\n",
            "Epoch 254/300, Batch 400/937, generator loss: 1.7352626061238399, discriminator loss: 0.8574247932275219\n",
            "Epoch 254/300, Batch 500/937, generator loss: 1.7352546756868614, discriminator loss: 0.8574086716010095\n",
            "Epoch 254/300, Batch 600/937, generator loss: 1.7352532437342523, discriminator loss: 0.857395682322924\n",
            "Epoch 254/300, Batch 700/937, generator loss: 1.7352529679774684, discriminator loss: 0.8573697173102108\n",
            "Epoch 254/300, Batch 800/937, generator loss: 1.7352493817042691, discriminator loss: 0.8573562424794337\n",
            "Epoch 254/300, Batch 900/937, generator loss: 1.7352408395991106, discriminator loss: 0.8573506591877155\n",
            "Saving results...\n",
            "Epoch 255/300, Batch 0/937, generator loss: 1.7352385948117977, discriminator loss: 0.8573484783450535\n",
            "Epoch 255/300, Batch 100/937, generator loss: 1.7352265114979455, discriminator loss: 0.8573233246129778\n",
            "Epoch 255/300, Batch 200/937, generator loss: 1.7352296451557063, discriminator loss: 0.8573052682695123\n",
            "Epoch 255/300, Batch 300/937, generator loss: 1.7352376761805837, discriminator loss: 0.8572833578273781\n",
            "Epoch 255/300, Batch 400/937, generator loss: 1.735243216864238, discriminator loss: 0.8572573370625887\n",
            "Epoch 255/300, Batch 500/937, generator loss: 1.7352374345664103, discriminator loss: 0.8572395397682369\n",
            "Epoch 255/300, Batch 600/937, generator loss: 1.7352407628765016, discriminator loss: 0.8572232592221374\n",
            "Epoch 255/300, Batch 700/937, generator loss: 1.7352329272701663, discriminator loss: 0.8572121099305172\n",
            "Epoch 255/300, Batch 800/937, generator loss: 1.735226023153217, discriminator loss: 0.8571992940170426\n",
            "Epoch 255/300, Batch 900/937, generator loss: 1.735212881230702, discriminator loss: 0.8571840925568851\n",
            "Saving results...\n",
            "Epoch 256/300, Batch 0/937, generator loss: 1.7352112537178337, discriminator loss: 0.8571773634391886\n",
            "Epoch 256/300, Batch 100/937, generator loss: 1.7352062213174126, discriminator loss: 0.8571549881271509\n",
            "Epoch 256/300, Batch 200/937, generator loss: 1.7352070630619363, discriminator loss: 0.8571320103837433\n",
            "Epoch 256/300, Batch 300/937, generator loss: 1.735206478606155, discriminator loss: 0.8571175327309136\n",
            "Epoch 256/300, Batch 400/937, generator loss: 1.7352111320437895, discriminator loss: 0.8570969150648726\n",
            "Epoch 256/300, Batch 500/937, generator loss: 1.7352037907131752, discriminator loss: 0.8570868704720985\n",
            "Epoch 256/300, Batch 600/937, generator loss: 1.7352005721461152, discriminator loss: 0.8570641781882957\n",
            "Epoch 256/300, Batch 700/937, generator loss: 1.7352143662241848, discriminator loss: 0.857045019667283\n",
            "Epoch 256/300, Batch 800/937, generator loss: 1.7352232458942414, discriminator loss: 0.857033108474845\n",
            "Epoch 256/300, Batch 900/937, generator loss: 1.735218135615927, discriminator loss: 0.8570127889253355\n",
            "Saving results...\n",
            "Epoch 257/300, Batch 0/937, generator loss: 1.735224171965172, discriminator loss: 0.8570046101220744\n",
            "Epoch 257/300, Batch 100/937, generator loss: 1.7352172425316674, discriminator loss: 0.8569906849260868\n",
            "Epoch 257/300, Batch 200/937, generator loss: 1.735231713400674, discriminator loss: 0.8569690718572321\n",
            "Epoch 257/300, Batch 300/937, generator loss: 1.7352293510339314, discriminator loss: 0.8569526654580277\n",
            "Epoch 257/300, Batch 400/937, generator loss: 1.7352359949991403, discriminator loss: 0.8569259874516817\n",
            "Epoch 257/300, Batch 500/937, generator loss: 1.735238986389087, discriminator loss: 0.8569085946388021\n",
            "Epoch 257/300, Batch 600/937, generator loss: 1.7352498919012473, discriminator loss: 0.8568948652283722\n",
            "Epoch 257/300, Batch 700/937, generator loss: 1.735250158659109, discriminator loss: 0.8568761075653846\n",
            "Epoch 257/300, Batch 800/937, generator loss: 1.7352390291892033, discriminator loss: 0.8568689584535931\n",
            "Epoch 257/300, Batch 900/937, generator loss: 1.7352434678439896, discriminator loss: 0.8568531016754101\n",
            "Saving results...\n",
            "Epoch 258/300, Batch 0/937, generator loss: 1.735238324036278, discriminator loss: 0.856849622830759\n",
            "Epoch 258/300, Batch 100/937, generator loss: 1.7352614075807675, discriminator loss: 0.856828509051226\n",
            "Epoch 258/300, Batch 200/937, generator loss: 1.7352752515050103, discriminator loss: 0.856804064616973\n",
            "Epoch 258/300, Batch 300/937, generator loss: 1.735273744798772, discriminator loss: 0.8567838636383187\n",
            "Epoch 258/300, Batch 400/937, generator loss: 1.7352807118408669, discriminator loss: 0.856771936753365\n",
            "Epoch 258/300, Batch 500/937, generator loss: 1.7352937959833696, discriminator loss: 0.8567494965307197\n",
            "Epoch 258/300, Batch 600/937, generator loss: 1.735289397627082, discriminator loss: 0.856737979649248\n",
            "Epoch 258/300, Batch 700/937, generator loss: 1.7352915381354863, discriminator loss: 0.8567191805364889\n",
            "Epoch 258/300, Batch 800/937, generator loss: 1.7352827673073934, discriminator loss: 0.8567108336872178\n",
            "Epoch 258/300, Batch 900/937, generator loss: 1.735286898155922, discriminator loss: 0.8567044570031054\n",
            "Saving results...\n",
            "Epoch 259/300, Batch 0/937, generator loss: 1.7352799238256922, discriminator loss: 0.8567004567164314\n",
            "Epoch 259/300, Batch 100/937, generator loss: 1.7352717335816976, discriminator loss: 0.8566885892537728\n",
            "Epoch 259/300, Batch 200/937, generator loss: 1.7352749396959914, discriminator loss: 0.8566656038788418\n",
            "Epoch 259/300, Batch 300/937, generator loss: 1.7352791587497634, discriminator loss: 0.8566463754478609\n",
            "Epoch 259/300, Batch 400/937, generator loss: 1.7352712373819557, discriminator loss: 0.8566295834166646\n",
            "Epoch 259/300, Batch 500/937, generator loss: 1.735297728687009, discriminator loss: 0.8566048071784719\n",
            "Epoch 259/300, Batch 600/937, generator loss: 1.7352952249817952, discriminator loss: 0.856591117309701\n",
            "Epoch 259/300, Batch 700/937, generator loss: 1.7352868655799185, discriminator loss: 0.8565779944355691\n",
            "Epoch 259/300, Batch 800/937, generator loss: 1.735288261036521, discriminator loss: 0.856563556819106\n",
            "Epoch 259/300, Batch 900/937, generator loss: 1.7352948901676624, discriminator loss: 0.8565403355976309\n",
            "Saving results...\n",
            "Epoch 260/300, Batch 0/937, generator loss: 1.7353002893329257, discriminator loss: 0.8565304068569982\n",
            "Epoch 260/300, Batch 100/937, generator loss: 1.7353159483396854, discriminator loss: 0.8565027875977032\n",
            "Epoch 260/300, Batch 200/937, generator loss: 1.7353017458550644, discriminator loss: 0.8564782444720431\n",
            "Epoch 260/300, Batch 300/937, generator loss: 1.7352876438723839, discriminator loss: 0.8564569458625256\n",
            "Epoch 260/300, Batch 400/937, generator loss: 1.7352864077387837, discriminator loss: 0.856432608962848\n",
            "Epoch 260/300, Batch 500/937, generator loss: 1.7352882053236007, discriminator loss: 0.856413678877451\n",
            "Epoch 260/300, Batch 600/937, generator loss: 1.7352733295836529, discriminator loss: 0.8564012063235595\n",
            "Epoch 260/300, Batch 700/937, generator loss: 1.735277842674859, discriminator loss: 0.8563813057673476\n",
            "Epoch 260/300, Batch 800/937, generator loss: 1.7352744296645668, discriminator loss: 0.8563645208862637\n",
            "Epoch 260/300, Batch 900/937, generator loss: 1.73526172835805, discriminator loss: 0.8563471038569507\n",
            "Saving results...\n",
            "Epoch 261/300, Batch 0/937, generator loss: 1.7352522302444446, discriminator loss: 0.856347724250131\n",
            "Epoch 261/300, Batch 100/937, generator loss: 1.7352618384313907, discriminator loss: 0.8563226731781165\n",
            "Epoch 261/300, Batch 200/937, generator loss: 1.7352737383207784, discriminator loss: 0.8563032245972586\n",
            "Epoch 261/300, Batch 300/937, generator loss: 1.7352786557473767, discriminator loss: 0.8562827062100827\n",
            "Epoch 261/300, Batch 400/937, generator loss: 1.7352721505011552, discriminator loss: 0.8562564297324702\n",
            "Epoch 261/300, Batch 500/937, generator loss: 1.7352714164559682, discriminator loss: 0.8562403909936773\n",
            "Epoch 261/300, Batch 600/937, generator loss: 1.7352714452463331, discriminator loss: 0.85622001057325\n",
            "Epoch 261/300, Batch 700/937, generator loss: 1.7352573708035033, discriminator loss: 0.856208351748151\n",
            "Epoch 261/300, Batch 800/937, generator loss: 1.7352461040188583, discriminator loss: 0.8561954596428376\n",
            "Epoch 261/300, Batch 900/937, generator loss: 1.735254941385924, discriminator loss: 0.8561792924689003\n",
            "Saving results...\n",
            "Epoch 262/300, Batch 0/937, generator loss: 1.7352553841436407, discriminator loss: 0.8561728730021131\n",
            "Epoch 262/300, Batch 100/937, generator loss: 1.7352497634715092, discriminator loss: 0.8561444290927767\n",
            "Epoch 262/300, Batch 200/937, generator loss: 1.7352581772719546, discriminator loss: 0.8561167961367636\n",
            "Epoch 262/300, Batch 300/937, generator loss: 1.7352618791328625, discriminator loss: 0.8560910212090077\n",
            "Epoch 262/300, Batch 400/937, generator loss: 1.735273442631194, discriminator loss: 0.8560694045109719\n",
            "Epoch 262/300, Batch 500/937, generator loss: 1.7352676342128701, discriminator loss: 0.856045646357845\n",
            "Epoch 262/300, Batch 600/937, generator loss: 1.7352647887538795, discriminator loss: 0.8560291578705549\n",
            "Epoch 262/300, Batch 700/937, generator loss: 1.7352603619030729, discriminator loss: 0.856021481451219\n",
            "Epoch 262/300, Batch 800/937, generator loss: 1.7352749747876073, discriminator loss: 0.8559948759121911\n",
            "Epoch 262/300, Batch 900/937, generator loss: 1.7352692997574857, discriminator loss: 0.8559846659673954\n",
            "Saving results...\n",
            "Epoch 263/300, Batch 0/937, generator loss: 1.735258525945396, discriminator loss: 0.8559798592285053\n",
            "Epoch 263/300, Batch 100/937, generator loss: 1.735269856450911, discriminator loss: 0.8559621502749201\n",
            "Epoch 263/300, Batch 200/937, generator loss: 1.7352691044911142, discriminator loss: 0.8559350077423061\n",
            "Epoch 263/300, Batch 300/937, generator loss: 1.7352733784106475, discriminator loss: 0.855910944030308\n",
            "Epoch 263/300, Batch 400/937, generator loss: 1.735258842745935, discriminator loss: 0.8558948529086011\n",
            "Epoch 263/300, Batch 500/937, generator loss: 1.7352649214840943, discriminator loss: 0.8558746861168726\n",
            "Epoch 263/300, Batch 600/937, generator loss: 1.7352896691992006, discriminator loss: 0.855853978625786\n",
            "Epoch 263/300, Batch 700/937, generator loss: 1.7353030044786917, discriminator loss: 0.8558334840319524\n",
            "Epoch 263/300, Batch 800/937, generator loss: 1.7353132893178258, discriminator loss: 0.8558106214793386\n",
            "Epoch 263/300, Batch 900/937, generator loss: 1.7353186934746339, discriminator loss: 0.8557996276504045\n",
            "Saving results...\n",
            "Epoch 264/300, Batch 0/937, generator loss: 1.7353199010222657, discriminator loss: 0.8557956939598372\n",
            "Epoch 264/300, Batch 100/937, generator loss: 1.7353413666154844, discriminator loss: 0.8557709531060205\n",
            "Epoch 264/300, Batch 200/937, generator loss: 1.7353493576590184, discriminator loss: 0.8557435375692247\n",
            "Epoch 264/300, Batch 300/937, generator loss: 1.7353707125689914, discriminator loss: 0.8557239073190598\n",
            "Epoch 264/300, Batch 400/937, generator loss: 1.73537726910097, discriminator loss: 0.8557062640937908\n",
            "Epoch 264/300, Batch 500/937, generator loss: 1.7353841344091778, discriminator loss: 0.8556787179637105\n",
            "Epoch 264/300, Batch 600/937, generator loss: 1.7353865313042207, discriminator loss: 0.855652932663296\n",
            "Epoch 264/300, Batch 700/937, generator loss: 1.7353813547193273, discriminator loss: 0.8556362886659717\n",
            "Epoch 264/300, Batch 800/937, generator loss: 1.7353905887798093, discriminator loss: 0.8556197566298532\n",
            "Epoch 264/300, Batch 900/937, generator loss: 1.7353873914896534, discriminator loss: 0.8556047155960667\n",
            "Saving results...\n",
            "Epoch 265/300, Batch 0/937, generator loss: 1.7353844461093033, discriminator loss: 0.8555994230930132\n",
            "Epoch 265/300, Batch 100/937, generator loss: 1.7353933716984926, discriminator loss: 0.8555777339874384\n",
            "Epoch 265/300, Batch 200/937, generator loss: 1.735396596633864, discriminator loss: 0.8555547452577245\n",
            "Epoch 265/300, Batch 300/937, generator loss: 1.7354101041146393, discriminator loss: 0.8555294016773487\n",
            "Epoch 265/300, Batch 400/937, generator loss: 1.7354309967822619, discriminator loss: 0.8554984114008266\n",
            "Epoch 265/300, Batch 500/937, generator loss: 1.7354321685155327, discriminator loss: 0.8554805907261078\n",
            "Epoch 265/300, Batch 600/937, generator loss: 1.7354426695211547, discriminator loss: 0.8554553425075305\n",
            "Epoch 265/300, Batch 700/937, generator loss: 1.7354419403201296, discriminator loss: 0.8554340946071182\n",
            "Epoch 265/300, Batch 800/937, generator loss: 1.735458774445501, discriminator loss: 0.8554092386062518\n",
            "Epoch 265/300, Batch 900/937, generator loss: 1.7354649353619358, discriminator loss: 0.8553895956369787\n",
            "Saving results...\n",
            "Epoch 266/300, Batch 0/937, generator loss: 1.7354710215458855, discriminator loss: 0.8553835559915449\n",
            "Epoch 266/300, Batch 100/937, generator loss: 1.7354843221245626, discriminator loss: 0.8553596743994746\n",
            "Epoch 266/300, Batch 200/937, generator loss: 1.735516964320424, discriminator loss: 0.8553367481503931\n",
            "Epoch 266/300, Batch 300/937, generator loss: 1.735548038204218, discriminator loss: 0.8553098783222496\n",
            "Epoch 266/300, Batch 400/937, generator loss: 1.735562289524644, discriminator loss: 0.8552789977643287\n",
            "Epoch 266/300, Batch 500/937, generator loss: 1.7355627326903127, discriminator loss: 0.8552557586248176\n",
            "Epoch 266/300, Batch 600/937, generator loss: 1.7355774183873134, discriminator loss: 0.8552381777290954\n",
            "Epoch 266/300, Batch 700/937, generator loss: 1.7355893094949157, discriminator loss: 0.8552129906569731\n",
            "Epoch 266/300, Batch 800/937, generator loss: 1.7355843604633394, discriminator loss: 0.8551910030836488\n",
            "Epoch 266/300, Batch 900/937, generator loss: 1.735601202830178, discriminator loss: 0.8551630491467053\n",
            "Saving results...\n",
            "Epoch 267/300, Batch 0/937, generator loss: 1.7356119360766715, discriminator loss: 0.8551551221654049\n",
            "Epoch 267/300, Batch 100/937, generator loss: 1.7356288380481497, discriminator loss: 0.8551284279278364\n",
            "Epoch 267/300, Batch 200/937, generator loss: 1.7356458459821065, discriminator loss: 0.8551020137978177\n",
            "Epoch 267/300, Batch 300/937, generator loss: 1.7356429846084602, discriminator loss: 0.8550826974085813\n",
            "Epoch 267/300, Batch 400/937, generator loss: 1.7356588057988716, discriminator loss: 0.8550641857053716\n",
            "Epoch 267/300, Batch 500/937, generator loss: 1.7356630142318663, discriminator loss: 0.855031328313546\n",
            "Epoch 267/300, Batch 600/937, generator loss: 1.7356773948795878, discriminator loss: 0.8550115586359855\n",
            "Epoch 267/300, Batch 700/937, generator loss: 1.7356779310125288, discriminator loss: 0.8549956990936913\n",
            "Epoch 267/300, Batch 800/937, generator loss: 1.7356900977712197, discriminator loss: 0.8549789670702518\n",
            "Epoch 267/300, Batch 900/937, generator loss: 1.735711661017262, discriminator loss: 0.8549559935349339\n",
            "Saving results...\n",
            "Epoch 268/300, Batch 0/937, generator loss: 1.7357159884379292, discriminator loss: 0.8549477887018948\n",
            "Epoch 268/300, Batch 100/937, generator loss: 1.7357355856529397, discriminator loss: 0.854917366798653\n",
            "Epoch 268/300, Batch 200/937, generator loss: 1.7357624700161383, discriminator loss: 0.854891944935493\n",
            "Epoch 268/300, Batch 300/937, generator loss: 1.735757759153124, discriminator loss: 0.8548659121718706\n",
            "Epoch 268/300, Batch 400/937, generator loss: 1.7357629634516933, discriminator loss: 0.8548435232511552\n",
            "Epoch 268/300, Batch 500/937, generator loss: 1.7357780024290674, discriminator loss: 0.8548272910414998\n",
            "Epoch 268/300, Batch 600/937, generator loss: 1.7357651925942932, discriminator loss: 0.8548089774845093\n",
            "Epoch 268/300, Batch 700/937, generator loss: 1.7357753903703241, discriminator loss: 0.8547833233479631\n",
            "Epoch 268/300, Batch 800/937, generator loss: 1.7357798106427613, discriminator loss: 0.8547617448721015\n",
            "Epoch 268/300, Batch 900/937, generator loss: 1.7357804877585554, discriminator loss: 0.854745531970179\n",
            "Saving results...\n",
            "Epoch 269/300, Batch 0/937, generator loss: 1.7357841152402946, discriminator loss: 0.854733421418139\n",
            "Epoch 269/300, Batch 100/937, generator loss: 1.7357929934458887, discriminator loss: 0.8547069312660249\n",
            "Epoch 269/300, Batch 200/937, generator loss: 1.7358106687524748, discriminator loss: 0.854673452174825\n",
            "Epoch 269/300, Batch 300/937, generator loss: 1.7358312188931364, discriminator loss: 0.8546515991846413\n",
            "Epoch 269/300, Batch 400/937, generator loss: 1.7358610645873056, discriminator loss: 0.854624193307961\n",
            "Epoch 269/300, Batch 500/937, generator loss: 1.7358807376878882, discriminator loss: 0.8546055817032313\n",
            "Epoch 269/300, Batch 600/937, generator loss: 1.7358966389549193, discriminator loss: 0.8545861110668374\n",
            "Epoch 269/300, Batch 700/937, generator loss: 1.7358959958578537, discriminator loss: 0.8545706486320077\n",
            "Epoch 269/300, Batch 800/937, generator loss: 1.7359238495407276, discriminator loss: 0.8545525963132772\n",
            "Epoch 269/300, Batch 900/937, generator loss: 1.7359213073318835, discriminator loss: 0.8545357993980327\n",
            "Saving results...\n",
            "Epoch 270/300, Batch 0/937, generator loss: 1.7359234759788842, discriminator loss: 0.8545289826811034\n",
            "Epoch 270/300, Batch 100/937, generator loss: 1.7359429107515383, discriminator loss: 0.8544956811952926\n",
            "Epoch 270/300, Batch 200/937, generator loss: 1.7359661976733676, discriminator loss: 0.854470302483758\n",
            "Epoch 270/300, Batch 300/937, generator loss: 1.7359978340097841, discriminator loss: 0.8544425225691664\n",
            "Epoch 270/300, Batch 400/937, generator loss: 1.7360076745499193, discriminator loss: 0.8544241437375301\n",
            "Epoch 270/300, Batch 500/937, generator loss: 1.7360255363045447, discriminator loss: 0.8544066230478843\n",
            "Epoch 270/300, Batch 600/937, generator loss: 1.7360352030180846, discriminator loss: 0.8543867002877584\n",
            "Epoch 270/300, Batch 700/937, generator loss: 1.7360545742660285, discriminator loss: 0.8543579588785614\n",
            "Epoch 270/300, Batch 800/937, generator loss: 1.736065758070277, discriminator loss: 0.8543395128786057\n",
            "Epoch 270/300, Batch 900/937, generator loss: 1.7360709844655997, discriminator loss: 0.8543160806969573\n",
            "Saving results...\n",
            "Epoch 271/300, Batch 0/937, generator loss: 1.7360654470769659, discriminator loss: 0.8543074847992734\n",
            "Epoch 271/300, Batch 100/937, generator loss: 1.736079193770051, discriminator loss: 0.8542734075803463\n",
            "Epoch 271/300, Batch 200/937, generator loss: 1.7360968880178511, discriminator loss: 0.8542466938474761\n",
            "Epoch 271/300, Batch 300/937, generator loss: 1.7361104563758982, discriminator loss: 0.8542232488553391\n",
            "Epoch 271/300, Batch 400/937, generator loss: 1.736117623396025, discriminator loss: 0.8542013115897789\n",
            "Epoch 271/300, Batch 500/937, generator loss: 1.7361364277386857, discriminator loss: 0.8541800425999024\n",
            "Epoch 271/300, Batch 600/937, generator loss: 1.7361527430097348, discriminator loss: 0.8541599236069745\n",
            "Epoch 271/300, Batch 700/937, generator loss: 1.7361884428595542, discriminator loss: 0.8541323670675794\n",
            "Epoch 271/300, Batch 800/937, generator loss: 1.7362007824890369, discriminator loss: 0.8541035318831341\n",
            "Epoch 271/300, Batch 900/937, generator loss: 1.7362109095942069, discriminator loss: 0.8540835144342949\n",
            "Saving results...\n",
            "Epoch 272/300, Batch 0/937, generator loss: 1.7362206834557128, discriminator loss: 0.8540721689411983\n",
            "Epoch 272/300, Batch 100/937, generator loss: 1.7362159587153432, discriminator loss: 0.8540525399472975\n",
            "Epoch 272/300, Batch 200/937, generator loss: 1.7362299042532479, discriminator loss: 0.8540268285709905\n",
            "Epoch 272/300, Batch 300/937, generator loss: 1.7362620563679751, discriminator loss: 0.8540006993592826\n",
            "Epoch 272/300, Batch 400/937, generator loss: 1.7362609279836854, discriminator loss: 0.8539763435623619\n",
            "Epoch 272/300, Batch 500/937, generator loss: 1.7362787491644145, discriminator loss: 0.8539468516923803\n",
            "Epoch 272/300, Batch 600/937, generator loss: 1.7363051381092574, discriminator loss: 0.8539238134818118\n",
            "Epoch 272/300, Batch 700/937, generator loss: 1.7363330685506744, discriminator loss: 0.853901276265444\n",
            "Epoch 272/300, Batch 800/937, generator loss: 1.7363559754723417, discriminator loss: 0.8538820699751092\n",
            "Epoch 272/300, Batch 900/937, generator loss: 1.7363775419065164, discriminator loss: 0.853851917312005\n",
            "Saving results...\n",
            "Epoch 273/300, Batch 0/937, generator loss: 1.7363844999674174, discriminator loss: 0.8538471260259475\n",
            "Epoch 273/300, Batch 100/937, generator loss: 1.736403061798919, discriminator loss: 0.8538161503370834\n",
            "Epoch 273/300, Batch 200/937, generator loss: 1.7364352926896067, discriminator loss: 0.8537841410393819\n",
            "Epoch 273/300, Batch 300/937, generator loss: 1.7364523877585585, discriminator loss: 0.8537565652628436\n",
            "Epoch 273/300, Batch 400/937, generator loss: 1.7364733704834712, discriminator loss: 0.8537264062553469\n",
            "Epoch 273/300, Batch 500/937, generator loss: 1.736501315343928, discriminator loss: 0.8537012525614733\n",
            "Epoch 273/300, Batch 600/937, generator loss: 1.7365089990435436, discriminator loss: 0.8536765414446267\n",
            "Epoch 273/300, Batch 700/937, generator loss: 1.7365241070611512, discriminator loss: 0.8536459903482724\n",
            "Epoch 273/300, Batch 800/937, generator loss: 1.7365328762401406, discriminator loss: 0.8536312318548265\n",
            "Epoch 273/300, Batch 900/937, generator loss: 1.7365417047362066, discriminator loss: 0.8536115259212139\n",
            "Saving results...\n",
            "Epoch 274/300, Batch 0/937, generator loss: 1.736544493175786, discriminator loss: 0.8536061331461379\n",
            "Epoch 274/300, Batch 100/937, generator loss: 1.7365615543826365, discriminator loss: 0.8535838820541874\n",
            "Epoch 274/300, Batch 200/937, generator loss: 1.7365864594915377, discriminator loss: 0.8535580657652869\n",
            "Epoch 274/300, Batch 300/937, generator loss: 1.7366092510872753, discriminator loss: 0.8535299220214547\n",
            "Epoch 274/300, Batch 400/937, generator loss: 1.7366231920278365, discriminator loss: 0.8535062535546679\n",
            "Epoch 274/300, Batch 500/937, generator loss: 1.7366407357583171, discriminator loss: 0.853481217984835\n",
            "Epoch 274/300, Batch 600/937, generator loss: 1.7366485322054344, discriminator loss: 0.8534534060165319\n",
            "Epoch 274/300, Batch 700/937, generator loss: 1.7366695019269733, discriminator loss: 0.8534268889621938\n",
            "Epoch 274/300, Batch 800/937, generator loss: 1.736689654630141, discriminator loss: 0.8534033157942819\n",
            "Epoch 274/300, Batch 900/937, generator loss: 1.7367141568885007, discriminator loss: 0.8533814764338526\n",
            "Saving results...\n",
            "Epoch 275/300, Batch 0/937, generator loss: 1.736714512762531, discriminator loss: 0.8533766346132701\n",
            "Epoch 275/300, Batch 100/937, generator loss: 1.7367283410745495, discriminator loss: 0.8533450636236262\n",
            "Epoch 275/300, Batch 200/937, generator loss: 1.7367430656057858, discriminator loss: 0.853321663665039\n",
            "Epoch 275/300, Batch 300/937, generator loss: 1.736769241010673, discriminator loss: 0.85329176048709\n",
            "Epoch 275/300, Batch 400/937, generator loss: 1.7367953353840242, discriminator loss: 0.8532626268182131\n",
            "Epoch 275/300, Batch 500/937, generator loss: 1.7368120341065367, discriminator loss: 0.8532388564153035\n",
            "Epoch 275/300, Batch 600/937, generator loss: 1.736823348710439, discriminator loss: 0.8532218882034104\n",
            "Epoch 275/300, Batch 700/937, generator loss: 1.7368254172798097, discriminator loss: 0.8531947678899301\n",
            "Epoch 275/300, Batch 800/937, generator loss: 1.7368450700234996, discriminator loss: 0.8531690988146655\n",
            "Epoch 275/300, Batch 900/937, generator loss: 1.7368663261371906, discriminator loss: 0.8531488344074679\n",
            "Saving results...\n",
            "Epoch 276/300, Batch 0/937, generator loss: 1.7368699174656117, discriminator loss: 0.8531383766062923\n",
            "Epoch 276/300, Batch 100/937, generator loss: 1.736888319109122, discriminator loss: 0.8531073027516585\n",
            "Epoch 276/300, Batch 200/937, generator loss: 1.7369063418584734, discriminator loss: 0.8530833609085373\n",
            "Epoch 276/300, Batch 300/937, generator loss: 1.7369340096689196, discriminator loss: 0.8530611659073053\n",
            "Epoch 276/300, Batch 400/937, generator loss: 1.736948084010714, discriminator loss: 0.8530424899835266\n",
            "Epoch 276/300, Batch 500/937, generator loss: 1.7369742425804144, discriminator loss: 0.8530109207492405\n",
            "Epoch 276/300, Batch 600/937, generator loss: 1.736996178710015, discriminator loss: 0.8529922655739175\n",
            "Epoch 276/300, Batch 700/937, generator loss: 1.737026116580743, discriminator loss: 0.8529612887162787\n",
            "Epoch 276/300, Batch 800/937, generator loss: 1.7370341440956105, discriminator loss: 0.8529327494876535\n",
            "Epoch 276/300, Batch 900/937, generator loss: 1.7370552922255407, discriminator loss: 0.8529096150731472\n",
            "Saving results...\n",
            "Epoch 277/300, Batch 0/937, generator loss: 1.7370609398991403, discriminator loss: 0.8528991934527921\n",
            "Epoch 277/300, Batch 100/937, generator loss: 1.737077030534577, discriminator loss: 0.8528624455051433\n",
            "Epoch 277/300, Batch 200/937, generator loss: 1.7370881805098666, discriminator loss: 0.852836346758456\n",
            "Epoch 277/300, Batch 300/937, generator loss: 1.7371044097773405, discriminator loss: 0.8528041671360138\n",
            "Epoch 277/300, Batch 400/937, generator loss: 1.7371320865227182, discriminator loss: 0.8527717508314022\n",
            "Epoch 277/300, Batch 500/937, generator loss: 1.7371589782618029, discriminator loss: 0.8527451500469069\n",
            "Epoch 277/300, Batch 600/937, generator loss: 1.737182751041913, discriminator loss: 0.8527262987007711\n",
            "Epoch 277/300, Batch 700/937, generator loss: 1.7372027545758557, discriminator loss: 0.8527118414632411\n",
            "Epoch 277/300, Batch 800/937, generator loss: 1.737229408723323, discriminator loss: 0.8526954179034939\n",
            "Epoch 277/300, Batch 900/937, generator loss: 1.7372489461511584, discriminator loss: 0.8526754984360873\n",
            "Saving results...\n",
            "Epoch 278/300, Batch 0/937, generator loss: 1.7372616064163755, discriminator loss: 0.8526664254668459\n",
            "Epoch 278/300, Batch 100/937, generator loss: 1.737282376958144, discriminator loss: 0.8526343124151983\n",
            "Epoch 278/300, Batch 200/937, generator loss: 1.737301440375627, discriminator loss: 0.8526100274957584\n",
            "Epoch 278/300, Batch 300/937, generator loss: 1.737331403164956, discriminator loss: 0.8525816113108821\n",
            "Epoch 278/300, Batch 400/937, generator loss: 1.7373535688139097, discriminator loss: 0.8525527111923416\n",
            "Epoch 278/300, Batch 500/937, generator loss: 1.7373799637437404, discriminator loss: 0.8525274200169359\n",
            "Epoch 278/300, Batch 600/937, generator loss: 1.7373925429200454, discriminator loss: 0.8525003255647078\n",
            "Epoch 278/300, Batch 700/937, generator loss: 1.7374033314780626, discriminator loss: 0.8524753795058714\n",
            "Epoch 278/300, Batch 800/937, generator loss: 1.7374078657454584, discriminator loss: 0.852455410557735\n",
            "Epoch 278/300, Batch 900/937, generator loss: 1.737425406545397, discriminator loss: 0.852428662569757\n",
            "Saving results...\n",
            "Epoch 279/300, Batch 0/937, generator loss: 1.737427158576125, discriminator loss: 0.8524197466745339\n",
            "Epoch 279/300, Batch 100/937, generator loss: 1.7374638340266084, discriminator loss: 0.8523839757292453\n",
            "Epoch 279/300, Batch 200/937, generator loss: 1.7374883604722953, discriminator loss: 0.8523532536032422\n",
            "Epoch 279/300, Batch 300/937, generator loss: 1.7375069972070685, discriminator loss: 0.8523278403779273\n",
            "Epoch 279/300, Batch 400/937, generator loss: 1.737528681684494, discriminator loss: 0.8523023720934033\n",
            "Epoch 279/300, Batch 500/937, generator loss: 1.7375462737654495, discriminator loss: 0.852277734662542\n",
            "Epoch 279/300, Batch 600/937, generator loss: 1.7375756922175611, discriminator loss: 0.8522481404892529\n",
            "Epoch 279/300, Batch 700/937, generator loss: 1.7375943455074547, discriminator loss: 0.8522227800420337\n",
            "Epoch 279/300, Batch 800/937, generator loss: 1.7376151610368362, discriminator loss: 0.8521978755653327\n",
            "Epoch 279/300, Batch 900/937, generator loss: 1.7376211099154493, discriminator loss: 0.8521808331266774\n",
            "Saving results...\n",
            "Epoch 280/300, Batch 0/937, generator loss: 1.7376238569817706, discriminator loss: 0.8521706811913133\n",
            "Epoch 280/300, Batch 100/937, generator loss: 1.7376473611716539, discriminator loss: 0.8521340304288157\n",
            "Epoch 280/300, Batch 200/937, generator loss: 1.7376612543959413, discriminator loss: 0.8521149600282817\n",
            "Epoch 280/300, Batch 300/937, generator loss: 1.7376634197582286, discriminator loss: 0.8520919835449791\n",
            "Epoch 280/300, Batch 400/937, generator loss: 1.7376793170193863, discriminator loss: 0.8520596670662366\n",
            "Epoch 280/300, Batch 500/937, generator loss: 1.7376970081661929, discriminator loss: 0.8520247521897623\n",
            "Epoch 280/300, Batch 600/937, generator loss: 1.7377258290899464, discriminator loss: 0.8519980045477928\n",
            "Epoch 280/300, Batch 700/937, generator loss: 1.7377492232208513, discriminator loss: 0.8519658119338462\n",
            "Epoch 280/300, Batch 800/937, generator loss: 1.7377743631841758, discriminator loss: 0.851941890086711\n",
            "Epoch 280/300, Batch 900/937, generator loss: 1.7378106734964582, discriminator loss: 0.8519197879275107\n",
            "Saving results...\n",
            "Epoch 281/300, Batch 0/937, generator loss: 1.7378183500067828, discriminator loss: 0.8519158504138671\n",
            "Epoch 281/300, Batch 100/937, generator loss: 1.737852814135102, discriminator loss: 0.8518860716263006\n",
            "Epoch 281/300, Batch 200/937, generator loss: 1.7378852166557959, discriminator loss: 0.8518564295936446\n",
            "Epoch 281/300, Batch 300/937, generator loss: 1.7379195963651073, discriminator loss: 0.8518195175316376\n",
            "Epoch 281/300, Batch 400/937, generator loss: 1.7379552368917055, discriminator loss: 0.8517918530015701\n",
            "Epoch 281/300, Batch 500/937, generator loss: 1.7379751768181522, discriminator loss: 0.8517691041008334\n",
            "Epoch 281/300, Batch 600/937, generator loss: 1.7380012757486407, discriminator loss: 0.8517401460259897\n",
            "Epoch 281/300, Batch 700/937, generator loss: 1.7380131086667987, discriminator loss: 0.8517182783975645\n",
            "Epoch 281/300, Batch 800/937, generator loss: 1.7380346844167052, discriminator loss: 0.8516885798212699\n",
            "Epoch 281/300, Batch 900/937, generator loss: 1.738054375471479, discriminator loss: 0.8516682720573767\n",
            "Saving results...\n",
            "Epoch 282/300, Batch 0/937, generator loss: 1.7380685539947773, discriminator loss: 0.8516595425594631\n",
            "Epoch 282/300, Batch 100/937, generator loss: 1.7381095823102557, discriminator loss: 0.8516311143804851\n",
            "Epoch 282/300, Batch 200/937, generator loss: 1.7381410816328122, discriminator loss: 0.8515984053043693\n",
            "Epoch 282/300, Batch 300/937, generator loss: 1.7381608819876828, discriminator loss: 0.8515769308651461\n",
            "Epoch 282/300, Batch 400/937, generator loss: 1.7382008076394315, discriminator loss: 0.8515501716700826\n",
            "Epoch 282/300, Batch 500/937, generator loss: 1.7382311980902247, discriminator loss: 0.851521122658343\n",
            "Epoch 282/300, Batch 600/937, generator loss: 1.7382599611055665, discriminator loss: 0.8514956803399665\n",
            "Epoch 282/300, Batch 700/937, generator loss: 1.7382969349381436, discriminator loss: 0.8514710609054624\n",
            "Epoch 282/300, Batch 800/937, generator loss: 1.7383026372314316, discriminator loss: 0.8514444859249788\n",
            "Epoch 282/300, Batch 900/937, generator loss: 1.7383113682752265, discriminator loss: 0.8514232205377252\n",
            "Saving results...\n",
            "Epoch 283/300, Batch 0/937, generator loss: 1.7383106787820848, discriminator loss: 0.8514148960476648\n",
            "Epoch 283/300, Batch 100/937, generator loss: 1.7383424315632259, discriminator loss: 0.8513820527113055\n",
            "Epoch 283/300, Batch 200/937, generator loss: 1.7383609438843453, discriminator loss: 0.8513563097334343\n",
            "Epoch 283/300, Batch 300/937, generator loss: 1.73840106123198, discriminator loss: 0.8513252707655167\n",
            "Epoch 283/300, Batch 400/937, generator loss: 1.738424479742792, discriminator loss: 0.8512966366435357\n",
            "Epoch 283/300, Batch 500/937, generator loss: 1.7384424963505805, discriminator loss: 0.851270278110275\n",
            "Epoch 283/300, Batch 600/937, generator loss: 1.738457338594632, discriminator loss: 0.8512488936447613\n",
            "Epoch 283/300, Batch 700/937, generator loss: 1.738478485209464, discriminator loss: 0.8512231252842934\n",
            "Epoch 283/300, Batch 800/937, generator loss: 1.7385079964459558, discriminator loss: 0.8511892775778418\n",
            "Epoch 283/300, Batch 900/937, generator loss: 1.738535173265774, discriminator loss: 0.8511642386313051\n",
            "Saving results...\n",
            "Epoch 284/300, Batch 0/937, generator loss: 1.7385435344146947, discriminator loss: 0.8511570129973302\n",
            "Epoch 284/300, Batch 100/937, generator loss: 1.738576948148132, discriminator loss: 0.8511261322948418\n",
            "Epoch 284/300, Batch 200/937, generator loss: 1.7386026903981615, discriminator loss: 0.8510947433169771\n",
            "Epoch 284/300, Batch 300/937, generator loss: 1.7386276572137853, discriminator loss: 0.8510675893559526\n",
            "Epoch 284/300, Batch 400/937, generator loss: 1.738657978283293, discriminator loss: 0.8510429476562801\n",
            "Epoch 284/300, Batch 500/937, generator loss: 1.738690926645652, discriminator loss: 0.851010545308589\n",
            "Epoch 284/300, Batch 600/937, generator loss: 1.7387026816608644, discriminator loss: 0.8509822400906698\n",
            "Epoch 284/300, Batch 700/937, generator loss: 1.7387196960296558, discriminator loss: 0.8509546573204685\n",
            "Epoch 284/300, Batch 800/937, generator loss: 1.7387463105046947, discriminator loss: 0.8509226443242455\n",
            "Epoch 284/300, Batch 900/937, generator loss: 1.738775476203059, discriminator loss: 0.8508957368451033\n",
            "Saving results...\n",
            "Epoch 285/300, Batch 0/937, generator loss: 1.7387878042793796, discriminator loss: 0.8508843553563945\n",
            "Epoch 285/300, Batch 100/937, generator loss: 1.7388131614715703, discriminator loss: 0.850856177937828\n",
            "Epoch 285/300, Batch 200/937, generator loss: 1.738855775011873, discriminator loss: 0.8508237384289086\n",
            "Epoch 285/300, Batch 300/937, generator loss: 1.7388793538783789, discriminator loss: 0.8508008961281441\n",
            "Epoch 285/300, Batch 400/937, generator loss: 1.7389031400091142, discriminator loss: 0.850776371761831\n",
            "Epoch 285/300, Batch 500/937, generator loss: 1.7389403485501855, discriminator loss: 0.8507464198907746\n",
            "Epoch 285/300, Batch 600/937, generator loss: 1.7389735975321303, discriminator loss: 0.8507182958732514\n",
            "Epoch 285/300, Batch 700/937, generator loss: 1.7389931374896164, discriminator loss: 0.8506904918533132\n",
            "Epoch 285/300, Batch 800/937, generator loss: 1.7390103165820816, discriminator loss: 0.8506590114311445\n",
            "Epoch 285/300, Batch 900/937, generator loss: 1.7390351936626094, discriminator loss: 0.8506321015023665\n",
            "Saving results...\n",
            "Epoch 286/300, Batch 0/937, generator loss: 1.7390400468058693, discriminator loss: 0.8506210278245284\n",
            "Epoch 286/300, Batch 100/937, generator loss: 1.7390752958121845, discriminator loss: 0.8505847634288145\n",
            "Epoch 286/300, Batch 200/937, generator loss: 1.7391040110685598, discriminator loss: 0.8505553830331068\n",
            "Epoch 286/300, Batch 300/937, generator loss: 1.7391361649851693, discriminator loss: 0.8505223358859364\n",
            "Epoch 286/300, Batch 400/937, generator loss: 1.7391627221919916, discriminator loss: 0.8504943910067421\n",
            "Epoch 286/300, Batch 500/937, generator loss: 1.7391873181448116, discriminator loss: 0.8504593841062508\n",
            "Epoch 286/300, Batch 600/937, generator loss: 1.7392128160155256, discriminator loss: 0.8504292394068196\n",
            "Epoch 286/300, Batch 700/937, generator loss: 1.7392459728785217, discriminator loss: 0.850402903495241\n",
            "Epoch 286/300, Batch 800/937, generator loss: 1.7392648392237435, discriminator loss: 0.8503771526827532\n",
            "Epoch 286/300, Batch 900/937, generator loss: 1.7392735077813668, discriminator loss: 0.8503499385824514\n",
            "Saving results...\n",
            "Epoch 287/300, Batch 0/937, generator loss: 1.7392732963473532, discriminator loss: 0.8503416493960599\n",
            "Epoch 287/300, Batch 100/937, generator loss: 1.7393058629108977, discriminator loss: 0.8503119069562509\n",
            "Epoch 287/300, Batch 200/937, generator loss: 1.7393442186200272, discriminator loss: 0.8502729621796684\n",
            "Epoch 287/300, Batch 300/937, generator loss: 1.739367048938405, discriminator loss: 0.8502484311792969\n",
            "Epoch 287/300, Batch 400/937, generator loss: 1.739396814334699, discriminator loss: 0.8502186573458678\n",
            "Epoch 287/300, Batch 500/937, generator loss: 1.7394246749959874, discriminator loss: 0.8501913822404739\n",
            "Epoch 287/300, Batch 600/937, generator loss: 1.7394525544275052, discriminator loss: 0.850160114704218\n",
            "Epoch 287/300, Batch 700/937, generator loss: 1.739488542212567, discriminator loss: 0.8501275875774692\n",
            "Epoch 287/300, Batch 800/937, generator loss: 1.7395114166294983, discriminator loss: 0.8501013035294158\n",
            "Epoch 287/300, Batch 900/937, generator loss: 1.7395446606880032, discriminator loss: 0.8500704716873102\n",
            "Saving results...\n",
            "Epoch 288/300, Batch 0/937, generator loss: 1.739543945610508, discriminator loss: 0.8500571611719135\n",
            "Epoch 288/300, Batch 100/937, generator loss: 1.7395726890237933, discriminator loss: 0.8500319634704449\n",
            "Epoch 288/300, Batch 200/937, generator loss: 1.7396068740535617, discriminator loss: 0.8499969153831675\n",
            "Epoch 288/300, Batch 300/937, generator loss: 1.7396517293135518, discriminator loss: 0.8499670324782548\n",
            "Epoch 288/300, Batch 400/937, generator loss: 1.7396818011264816, discriminator loss: 0.8499371007873251\n",
            "Epoch 288/300, Batch 500/937, generator loss: 1.7397297383705754, discriminator loss: 0.8499051669130415\n",
            "Epoch 288/300, Batch 600/937, generator loss: 1.7397561826922685, discriminator loss: 0.8498764173362611\n",
            "Epoch 288/300, Batch 700/937, generator loss: 1.7397826183885479, discriminator loss: 0.849846562325062\n",
            "Epoch 288/300, Batch 800/937, generator loss: 1.739819006351917, discriminator loss: 0.8498224345851454\n",
            "Epoch 288/300, Batch 900/937, generator loss: 1.739848446556215, discriminator loss: 0.8497946740062811\n",
            "Saving results...\n",
            "Epoch 289/300, Batch 0/937, generator loss: 1.7398585646718814, discriminator loss: 0.8497832096771466\n",
            "Epoch 289/300, Batch 100/937, generator loss: 1.7399001755760148, discriminator loss: 0.8497477845841038\n",
            "Epoch 289/300, Batch 200/937, generator loss: 1.7399402350281659, discriminator loss: 0.8497211249349998\n",
            "Epoch 289/300, Batch 300/937, generator loss: 1.7399738639621927, discriminator loss: 0.8496905559200105\n",
            "Epoch 289/300, Batch 400/937, generator loss: 1.7399975231047462, discriminator loss: 0.849657115998426\n",
            "Epoch 289/300, Batch 500/937, generator loss: 1.7400323280212682, discriminator loss: 0.8496274859231874\n",
            "Epoch 289/300, Batch 600/937, generator loss: 1.7400592609237289, discriminator loss: 0.8495981778112979\n",
            "Epoch 289/300, Batch 700/937, generator loss: 1.7400923006056692, discriminator loss: 0.8495712600248779\n",
            "Epoch 289/300, Batch 800/937, generator loss: 1.7401131563294023, discriminator loss: 0.8495476029327894\n",
            "Epoch 289/300, Batch 900/937, generator loss: 1.7401211900465423, discriminator loss: 0.8495218953875294\n",
            "Saving results...\n",
            "Epoch 290/300, Batch 0/937, generator loss: 1.7401279713811468, discriminator loss: 0.8495123480366467\n",
            "Epoch 290/300, Batch 100/937, generator loss: 1.7401479202764532, discriminator loss: 0.8494834279670309\n",
            "Epoch 290/300, Batch 200/937, generator loss: 1.7401865543529138, discriminator loss: 0.8494573623141105\n",
            "Epoch 290/300, Batch 300/937, generator loss: 1.7402154187325685, discriminator loss: 0.8494291106798493\n",
            "Epoch 290/300, Batch 400/937, generator loss: 1.740253224191192, discriminator loss: 0.8493982508677436\n",
            "Epoch 290/300, Batch 500/937, generator loss: 1.7402898635544453, discriminator loss: 0.8493636750244307\n",
            "Epoch 290/300, Batch 600/937, generator loss: 1.74031071490811, discriminator loss: 0.8493372170146433\n",
            "Epoch 290/300, Batch 700/937, generator loss: 1.7403382870266113, discriminator loss: 0.849311939184819\n",
            "Epoch 290/300, Batch 800/937, generator loss: 1.7403688862768953, discriminator loss: 0.8492828318055741\n",
            "Epoch 290/300, Batch 900/937, generator loss: 1.7403780623273661, discriminator loss: 0.8492609251416995\n",
            "Saving results...\n",
            "Epoch 291/300, Batch 0/937, generator loss: 1.7403956477989753, discriminator loss: 0.8492489293853123\n",
            "Epoch 291/300, Batch 100/937, generator loss: 1.7404251337140546, discriminator loss: 0.8492154113783146\n",
            "Epoch 291/300, Batch 200/937, generator loss: 1.740447847091605, discriminator loss: 0.8491776658665972\n",
            "Epoch 291/300, Batch 300/937, generator loss: 1.7404674708660683, discriminator loss: 0.849138367691511\n",
            "Epoch 291/300, Batch 400/937, generator loss: 1.7404920704094078, discriminator loss: 0.8491036536738381\n",
            "Epoch 291/300, Batch 500/937, generator loss: 1.7405124338845486, discriminator loss: 0.8490766469647328\n",
            "Epoch 291/300, Batch 600/937, generator loss: 1.7405467764962421, discriminator loss: 0.8490502031109592\n",
            "Epoch 291/300, Batch 700/937, generator loss: 1.7405787390163159, discriminator loss: 0.8490264852964087\n",
            "Epoch 291/300, Batch 800/937, generator loss: 1.7406057766871572, discriminator loss: 0.8489974288299492\n",
            "Epoch 291/300, Batch 900/937, generator loss: 1.7406285799728638, discriminator loss: 0.8489725068507871\n",
            "Saving results...\n",
            "Epoch 292/300, Batch 0/937, generator loss: 1.7406445420369365, discriminator loss: 0.8489617941676737\n",
            "Epoch 292/300, Batch 100/937, generator loss: 1.7406731675730012, discriminator loss: 0.8489284632277435\n",
            "Epoch 292/300, Batch 200/937, generator loss: 1.7406952061591052, discriminator loss: 0.848893293098193\n",
            "Epoch 292/300, Batch 300/937, generator loss: 1.7407336128830628, discriminator loss: 0.8488614817430638\n",
            "Epoch 292/300, Batch 400/937, generator loss: 1.7407633308106902, discriminator loss: 0.8488359954649248\n",
            "Epoch 292/300, Batch 500/937, generator loss: 1.7407988935582335, discriminator loss: 0.8488018470406434\n",
            "Epoch 292/300, Batch 600/937, generator loss: 1.740824226169169, discriminator loss: 0.8487714608927328\n",
            "Epoch 292/300, Batch 700/937, generator loss: 1.740858188869894, discriminator loss: 0.8487485658339994\n",
            "Epoch 292/300, Batch 800/937, generator loss: 1.7408919033934964, discriminator loss: 0.8487233966824108\n",
            "Epoch 292/300, Batch 900/937, generator loss: 1.740925366757652, discriminator loss: 0.8486978629304573\n",
            "Saving results...\n",
            "Epoch 293/300, Batch 0/937, generator loss: 1.74093677636742, discriminator loss: 0.848688845566211\n",
            "Epoch 293/300, Batch 100/937, generator loss: 1.7409748362125521, discriminator loss: 0.8486499761791084\n",
            "Epoch 293/300, Batch 200/937, generator loss: 1.7410173692698283, discriminator loss: 0.8486152566794372\n",
            "Epoch 293/300, Batch 300/937, generator loss: 1.7410518326644207, discriminator loss: 0.8485856031710461\n",
            "Epoch 293/300, Batch 400/937, generator loss: 1.7410764388266973, discriminator loss: 0.8485557218183633\n",
            "Epoch 293/300, Batch 500/937, generator loss: 1.7411013002220388, discriminator loss: 0.8485257749160402\n",
            "Epoch 293/300, Batch 600/937, generator loss: 1.7411277470839908, discriminator loss: 0.8484961899047211\n",
            "Epoch 293/300, Batch 700/937, generator loss: 1.7411551778535355, discriminator loss: 0.848465002287938\n",
            "Epoch 293/300, Batch 800/937, generator loss: 1.7411826931053747, discriminator loss: 0.8484323862702627\n",
            "Epoch 293/300, Batch 900/937, generator loss: 1.7412309118235794, discriminator loss: 0.8484055722359907\n",
            "Saving results...\n",
            "Epoch 294/300, Batch 0/937, generator loss: 1.7412375725163933, discriminator loss: 0.8483969836121255\n",
            "Epoch 294/300, Batch 100/937, generator loss: 1.7412791895612645, discriminator loss: 0.8483606967158581\n",
            "Epoch 294/300, Batch 200/937, generator loss: 1.7413154533848534, discriminator loss: 0.8483341361162058\n",
            "Epoch 294/300, Batch 300/937, generator loss: 1.7413401211090775, discriminator loss: 0.848302542520091\n",
            "Epoch 294/300, Batch 400/937, generator loss: 1.7413720161838542, discriminator loss: 0.8482651253497524\n",
            "Epoch 294/300, Batch 500/937, generator loss: 1.7414049646927736, discriminator loss: 0.8482402507134815\n",
            "Epoch 294/300, Batch 600/937, generator loss: 1.7414290494155307, discriminator loss: 0.8482084435151495\n",
            "Epoch 294/300, Batch 700/937, generator loss: 1.7414571413207685, discriminator loss: 0.8481860495479643\n",
            "Epoch 294/300, Batch 800/937, generator loss: 1.7414965938647866, discriminator loss: 0.8481556167837618\n",
            "Epoch 294/300, Batch 900/937, generator loss: 1.7415222920848141, discriminator loss: 0.8481262810589877\n",
            "Saving results...\n",
            "Epoch 295/300, Batch 0/937, generator loss: 1.741534768096787, discriminator loss: 0.8481145424022882\n",
            "Epoch 295/300, Batch 100/937, generator loss: 1.741575125774672, discriminator loss: 0.8480832396959798\n",
            "Epoch 295/300, Batch 200/937, generator loss: 1.7416044074242478, discriminator loss: 0.8480458618541826\n",
            "Epoch 295/300, Batch 300/937, generator loss: 1.741653441978722, discriminator loss: 0.848015527356373\n",
            "Epoch 295/300, Batch 400/937, generator loss: 1.7416862039017469, discriminator loss: 0.8479863956841286\n",
            "Epoch 295/300, Batch 500/937, generator loss: 1.741719855591109, discriminator loss: 0.8479536938582319\n",
            "Epoch 295/300, Batch 600/937, generator loss: 1.741745599273702, discriminator loss: 0.8479257778045892\n",
            "Epoch 295/300, Batch 700/937, generator loss: 1.74177083011763, discriminator loss: 0.8478938972698413\n",
            "Epoch 295/300, Batch 800/937, generator loss: 1.741812951895494, discriminator loss: 0.8478623456112558\n",
            "Epoch 295/300, Batch 900/937, generator loss: 1.741839799014591, discriminator loss: 0.8478327596579975\n",
            "Saving results...\n",
            "Epoch 296/300, Batch 0/937, generator loss: 1.7418498023812823, discriminator loss: 0.8478253148743427\n",
            "Epoch 296/300, Batch 100/937, generator loss: 1.741887116214956, discriminator loss: 0.8477927441082739\n",
            "Epoch 296/300, Batch 200/937, generator loss: 1.7419151029387916, discriminator loss: 0.8477556856252252\n",
            "Epoch 296/300, Batch 300/937, generator loss: 1.7419442592024204, discriminator loss: 0.8477190939103341\n",
            "Epoch 296/300, Batch 400/937, generator loss: 1.7419770279036693, discriminator loss: 0.8476862597695262\n",
            "Epoch 296/300, Batch 500/937, generator loss: 1.742017110047506, discriminator loss: 0.847655407544476\n",
            "Epoch 296/300, Batch 600/937, generator loss: 1.7420449548494459, discriminator loss: 0.8476237696852669\n",
            "Epoch 296/300, Batch 700/937, generator loss: 1.7420729097198264, discriminator loss: 0.8475956742369037\n",
            "Epoch 296/300, Batch 800/937, generator loss: 1.7421240672654337, discriminator loss: 0.8475615943668932\n",
            "Epoch 296/300, Batch 900/937, generator loss: 1.7421505025699435, discriminator loss: 0.8475390472539581\n",
            "Saving results...\n",
            "Epoch 297/300, Batch 0/937, generator loss: 1.742164330984268, discriminator loss: 0.8475283043995784\n",
            "Epoch 297/300, Batch 100/937, generator loss: 1.7422013948564836, discriminator loss: 0.8474894098415898\n",
            "Epoch 297/300, Batch 200/937, generator loss: 1.7422372982533574, discriminator loss: 0.8474581845927365\n",
            "Epoch 297/300, Batch 300/937, generator loss: 1.7422790612954155, discriminator loss: 0.847422439422524\n",
            "Epoch 297/300, Batch 400/937, generator loss: 1.74232816192279, discriminator loss: 0.8473881471016559\n",
            "Epoch 297/300, Batch 500/937, generator loss: 1.7423623356395321, discriminator loss: 0.8473516823376024\n",
            "Epoch 297/300, Batch 600/937, generator loss: 1.742394813768382, discriminator loss: 0.8473233817205137\n",
            "Epoch 297/300, Batch 700/937, generator loss: 1.7424232080267417, discriminator loss: 0.8473019246701643\n",
            "Epoch 297/300, Batch 800/937, generator loss: 1.7424719011169807, discriminator loss: 0.8472682062855095\n",
            "Epoch 297/300, Batch 900/937, generator loss: 1.7424993048706883, discriminator loss: 0.8472450941459485\n",
            "Saving results...\n",
            "Epoch 298/300, Batch 0/937, generator loss: 1.7425092218813862, discriminator loss: 0.8472348042485778\n",
            "Epoch 298/300, Batch 100/937, generator loss: 1.7425542878453018, discriminator loss: 0.8472007574997735\n",
            "Epoch 298/300, Batch 200/937, generator loss: 1.7425983271869994, discriminator loss: 0.8471683566415811\n",
            "Epoch 298/300, Batch 300/937, generator loss: 1.7426408043253223, discriminator loss: 0.8471349479155031\n",
            "Epoch 298/300, Batch 400/937, generator loss: 1.742687763744011, discriminator loss: 0.8471014898820195\n",
            "Epoch 298/300, Batch 500/937, generator loss: 1.742711595425727, discriminator loss: 0.8470700615228882\n",
            "Epoch 298/300, Batch 600/937, generator loss: 1.7427393725478424, discriminator loss: 0.8470404800770398\n",
            "Epoch 298/300, Batch 700/937, generator loss: 1.7427771935193315, discriminator loss: 0.8470096534213628\n",
            "Epoch 298/300, Batch 800/937, generator loss: 1.7428049373222798, discriminator loss: 0.8469763658986955\n",
            "Epoch 298/300, Batch 900/937, generator loss: 1.7428458200576515, discriminator loss: 0.8469389331451774\n",
            "Saving results...\n",
            "Epoch 299/300, Batch 0/937, generator loss: 1.7428562022954424, discriminator loss: 0.8469297075417425\n",
            "Epoch 299/300, Batch 100/937, generator loss: 1.7429093901112953, discriminator loss: 0.846891269107889\n",
            "Epoch 299/300, Batch 200/937, generator loss: 1.7429505918509876, discriminator loss: 0.8468493078145762\n",
            "Epoch 299/300, Batch 300/937, generator loss: 1.7429910902918095, discriminator loss: 0.8468142412556147\n",
            "Epoch 299/300, Batch 400/937, generator loss: 1.743023846430332, discriminator loss: 0.8467875290742514\n",
            "Epoch 299/300, Batch 500/937, generator loss: 1.743050403481633, discriminator loss: 0.8467555466174045\n",
            "Epoch 299/300, Batch 600/937, generator loss: 1.7430852831982682, discriminator loss: 0.8467266749144764\n",
            "Epoch 299/300, Batch 700/937, generator loss: 1.7431138848307173, discriminator loss: 0.8466926087828313\n",
            "Epoch 299/300, Batch 800/937, generator loss: 1.7431496233925428, discriminator loss: 0.8466584965845689\n",
            "Epoch 299/300, Batch 900/937, generator loss: 1.7431765972712905, discriminator loss: 0.8466282166461946\n",
            "Saving results...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(discriminator, 'vanilla-gan-mnist-discriminator.pt')\n",
        "torch.save(discriminator, 'vanilla-gan-mnist-generator.pt')"
      ],
      "metadata": {
        "id": "4PnhrJuf-x6j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90Ipb5zVZxkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}