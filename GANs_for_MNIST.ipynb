{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GANs for MNIST"
      ],
      "metadata": {
        "id": "s3ekG60FKt1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we fit vanilla GANs to the MNIST dataset."
      ],
      "metadata": {
        "id": "FezESETeKvpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment"
      ],
      "metadata": {
        "id": "vYXrulR9LE7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's start by installing a couple of dependencies.\n",
        "\n",
        "!pip -q install torch torchvision lightning pandas seaborn git+https://github.com/oelin/valkyrie"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "pjqCGMLNMB0r",
        "outputId": "25385716-8b5b-4ddf-8fc2-c7280e5b7b83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's import everything we need.\n",
        "\n",
        "from typing import Any, Callable, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, Lambda, ToPILImage, PILToTensor, ToTensor, Normalize\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "import valkyrie as vk"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JOz0GwklM7pb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's initailize the MNIST dataset.\n",
        "\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(mean=0.5, std=0.5),\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='.', train=True, transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "IUFkBKOoSwWd",
        "outputId": "d9ae9ff2-3212-4801-9b4c-9e36a8081805"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 31142458.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 39873500.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 39820192.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20752209.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Models"
      ],
      "metadata": {
        "id": "FlNS4i8KS7UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "# Quick error fix\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# On Windows platform, the torch.distributed package only\n",
        "# supports Gloo backend, FileStore and TcpStore.\n",
        "# For FileStore, set init_method parameter in init_process_group\n",
        "# to a local file. Example as follow:\n",
        "# init_method=\"file:///f:/libtmp/some_file\"\n",
        "# dist.init_process_group(\n",
        "#    \"gloo\",\n",
        "#    rank=rank,\n",
        "#    init_method=init_method,\n",
        "#    world_size=world_size)\n",
        "# For TcpStore, same way as on Linux.\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "setup(0, 1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4PYlQGGYyjkq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "\n",
        "#fabric = L.Fabric(accelerator=\"cuda\", devices=8, strategy=\"ddp\")\n",
        "\n",
        "fabric = L.Fabric(accelerator=\"cpu\", devices=10, strategy=\"ddp_notebook\")\n",
        "fabric.launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t7Q-fHK-ylkV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown The generator.\n",
        "\n",
        "latent_dimension = 64\n",
        "input_dimension = 28 * 28\n",
        "\n",
        "generator = nn.Sequential(\n",
        "    nn.Linear(latent_dimension, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, input_dimension),\n",
        "    nn.Tanh(),\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "_ew-eRrQ298g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown The discriminator.\n",
        "\n",
        "discriminator = nn.Sequential(\n",
        "    nn.Linear(input_dimension, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 1),\n",
        "    nn.Sigmoid(),\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "j02XWLmxBUQF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unnormalize_image(x):\n",
        "    return ((x + 1) / 2).clamp(0, 1)"
      ],
      "metadata": {
        "id": "P5rx1-xi-NAZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Train them.\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 300\n",
        "batches = len(train_dataset) // batch_size\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "discriminator_optimizer = Adam(discriminator.parameters(), lr=2e-4)\n",
        "generator_optimizer = Adam(generator.parameters(), lr=2e-4)\n",
        "\n",
        "\n",
        "def generate_fake(batch_size) -> torch.Tensor:\n",
        "    z = torch.randn(batch_size, latent_dimension).cuda()\n",
        "    x = generator(z)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "discriminator_losses = []\n",
        "generator_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (inputs_real, _) in enumerate(train_dataloader):\n",
        "\n",
        "        # Train the discriminator.\n",
        "\n",
        "        if inputs_real.size(0) != batch_size:\n",
        "            continue\n",
        "\n",
        "        inputs_real = inputs_real.view(batch_size, -1).cuda()\n",
        "        inputs_fake = generate_fake(batch_size).detach().cuda()\n",
        "        labels_real = torch.ones(batch_size, 1).cuda()\n",
        "        labels_fake = torch.zeros(batch_size, 1).cuda()\n",
        "\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        discriminator_loss_real = criterion(discriminator(inputs_real), labels_real)\n",
        "        discriminator_loss_fake = criterion(discriminator(inputs_fake), labels_fake)\n",
        "        discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
        "        discriminator_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator.\n",
        "\n",
        "        inputs_fake = generate_fake(batch_size).cuda()\n",
        "\n",
        "        generator_optimizer.zero_grad()\n",
        "        generator_loss = criterion(discriminator(inputs_fake), labels_real)\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # Record losses.\n",
        "\n",
        "        discriminator_losses.append(discriminator_loss.item())\n",
        "        generator_losses.append(generator_loss.item())\n",
        "\n",
        "        if (i % 100) == 0:\n",
        "            mean_discriminator_loss = sum(discriminator_losses) / len(discriminator_losses)\n",
        "            mean_generator_loss = sum(generator_losses) / len(generator_losses)\n",
        "\n",
        "            print(f'Epoch {epoch}/{epochs}, Batch {i}/{batches}, generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}')\n",
        "\n",
        "    # Save results.\n",
        "\n",
        "    print('Saving results...')\n",
        "\n",
        "    images_to_save = inputs_fake.detach().cpu().view(inputs_fake.size(0), 1, 28, 28)\n",
        "    save_image(unnormalize_image(images_to_save.data), os.path.join('samples', f'epoch-{epoch}.png'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpvm762X4lV5",
        "outputId": "3bf6280e-052e-41fd-d878-230f30f8c35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/300, Batch 0/937, generator loss: 0.7021292448043823, discriminator loss: 1.4290435314178467\n",
            "Epoch 0/300, Batch 100/937, generator loss: 1.5424737511294904, discriminator loss: 0.7002204756925602\n",
            "Epoch 0/300, Batch 200/937, generator loss: 2.4499487085128897, discriminator loss: 0.4083510832166049\n",
            "Epoch 0/300, Batch 300/937, generator loss: 2.780550401670196, discriminator loss: 0.3540844147548426\n",
            "Epoch 0/300, Batch 400/937, generator loss: 3.31599023588875, discriminator loss: 0.33587620733886436\n",
            "Epoch 0/300, Batch 500/937, generator loss: 3.5412213979604954, discriminator loss: 0.34543886923504447\n",
            "Epoch 0/300, Batch 600/937, generator loss: 3.851242894638398, discriminator loss: 0.3241525048734047\n",
            "Epoch 0/300, Batch 700/937, generator loss: 3.9208222960099346, discriminator loss: 0.3076201768355516\n",
            "Epoch 0/300, Batch 800/937, generator loss: 3.948273557923111, discriminator loss: 0.3095491712868735\n",
            "Epoch 0/300, Batch 900/937, generator loss: 3.9190756919381355, discriminator loss: 0.3274981129902118\n",
            "Saving results...\n",
            "Epoch 1/300, Batch 0/937, generator loss: 3.9049999375206066, discriminator loss: 0.3384900661602394\n",
            "Epoch 1/300, Batch 100/937, generator loss: 3.8875442917291827, discriminator loss: 0.3557411942142914\n",
            "Epoch 1/300, Batch 200/937, generator loss: 3.8198037261179336, discriminator loss: 0.3799253703178494\n",
            "Epoch 1/300, Batch 300/937, generator loss: 3.7379460252448315, discriminator loss: 0.40545962666355445\n",
            "Epoch 1/300, Batch 400/937, generator loss: 3.668810379567524, discriminator loss: 0.4242031272634396\n",
            "Epoch 1/300, Batch 500/937, generator loss: 3.6276136770616487, discriminator loss: 0.4220844491285185\n",
            "Epoch 1/300, Batch 600/937, generator loss: 3.574057287489329, discriminator loss: 0.4226407373084242\n",
            "Epoch 1/300, Batch 700/937, generator loss: 3.518694528329649, discriminator loss: 0.43117577306271254\n",
            "Epoch 1/300, Batch 800/937, generator loss: 3.4944864042677732, discriminator loss: 0.4280522981352067\n",
            "Epoch 1/300, Batch 900/937, generator loss: 3.496578907713667, discriminator loss: 0.42315321935541417\n",
            "Saving results...\n",
            "Epoch 2/300, Batch 0/937, generator loss: 3.488588630835215, discriminator loss: 0.4231348902960618\n",
            "Epoch 2/300, Batch 100/937, generator loss: 3.4829501026793372, discriminator loss: 0.4209161956570571\n",
            "Epoch 2/300, Batch 200/937, generator loss: 3.473139339096575, discriminator loss: 0.41477276867233126\n",
            "Epoch 2/300, Batch 300/937, generator loss: 3.448025151093801, discriminator loss: 0.4090533563751599\n",
            "Epoch 2/300, Batch 400/937, generator loss: 3.443293256471445, discriminator loss: 0.4013208312172811\n",
            "Epoch 2/300, Batch 500/937, generator loss: 3.4442723172087417, discriminator loss: 0.39703456836782003\n",
            "Epoch 2/300, Batch 600/937, generator loss: 3.446308720882493, discriminator loss: 0.39206710913717147\n",
            "Epoch 2/300, Batch 700/937, generator loss: 3.458269681444446, discriminator loss: 0.38688221347708146\n",
            "Epoch 2/300, Batch 800/937, generator loss: 3.4821621540773693, discriminator loss: 0.37953444977508527\n",
            "Epoch 2/300, Batch 900/937, generator loss: 3.50132200668524, discriminator loss: 0.37175341211863466\n",
            "Saving results...\n",
            "Epoch 3/300, Batch 0/937, generator loss: 3.5084810265224653, discriminator loss: 0.36838191528678743\n",
            "Epoch 3/300, Batch 100/937, generator loss: 3.517015261043395, discriminator loss: 0.3638428599115686\n",
            "Epoch 3/300, Batch 200/937, generator loss: 3.525345724555917, discriminator loss: 0.3568781469361102\n",
            "Epoch 3/300, Batch 300/937, generator loss: 3.542159060720153, discriminator loss: 0.35189304922786385\n",
            "Epoch 3/300, Batch 400/937, generator loss: 3.5914621986866293, discriminator loss: 0.3442618431990843\n",
            "Epoch 3/300, Batch 500/937, generator loss: 3.6370362088552133, discriminator loss: 0.3383006646279875\n",
            "Epoch 3/300, Batch 600/937, generator loss: 3.6698039534467886, discriminator loss: 0.3336282960321549\n",
            "Epoch 3/300, Batch 700/937, generator loss: 3.68773871386988, discriminator loss: 0.3317202323674307\n",
            "Epoch 3/300, Batch 800/937, generator loss: 3.711498345135982, discriminator loss: 0.33048761400661264\n",
            "Epoch 3/300, Batch 900/937, generator loss: 3.7410549654561125, discriminator loss: 0.3292508705986808\n",
            "Saving results...\n",
            "Epoch 4/300, Batch 0/937, generator loss: 3.7521205917139886, discriminator loss: 0.3279936314801688\n",
            "Epoch 4/300, Batch 100/937, generator loss: 3.7723276052235875, discriminator loss: 0.3273767896013992\n",
            "Epoch 4/300, Batch 200/937, generator loss: 3.785125574980485, discriminator loss: 0.32446107049098827\n",
            "Epoch 4/300, Batch 300/937, generator loss: 3.795543611667927, discriminator loss: 0.32266865665620714\n",
            "Epoch 4/300, Batch 400/937, generator loss: 3.799530959137953, discriminator loss: 0.32239214919469794\n",
            "Epoch 4/300, Batch 500/937, generator loss: 3.8076240987911256, discriminator loss: 0.3188110531028733\n",
            "Epoch 4/300, Batch 600/937, generator loss: 3.807795337779299, discriminator loss: 0.3181819299278341\n",
            "Epoch 4/300, Batch 700/937, generator loss: 3.82306596837062, discriminator loss: 0.31789927094736026\n",
            "Epoch 4/300, Batch 800/937, generator loss: 3.826229871184151, discriminator loss: 0.3196711768202728\n",
            "Epoch 4/300, Batch 900/937, generator loss: 3.8250156209837574, discriminator loss: 0.3190520425489588\n",
            "Saving results...\n",
            "Epoch 5/300, Batch 0/937, generator loss: 3.8242987350958613, discriminator loss: 0.31846575823827555\n",
            "Epoch 5/300, Batch 100/937, generator loss: 3.8192523715763076, discriminator loss: 0.3176658559540762\n",
            "Epoch 5/300, Batch 200/937, generator loss: 3.825975806838511, discriminator loss: 0.31663742162305536\n",
            "Epoch 5/300, Batch 300/937, generator loss: 3.835238278928264, discriminator loss: 0.3148915332932932\n",
            "Epoch 5/300, Batch 400/937, generator loss: 3.841975825992195, discriminator loss: 0.313781257891662\n",
            "Epoch 5/300, Batch 500/937, generator loss: 3.8600562462303754, discriminator loss: 0.3119284716774909\n",
            "Epoch 5/300, Batch 600/937, generator loss: 3.8772461478271767, discriminator loss: 0.3101170366088008\n",
            "Epoch 5/300, Batch 700/937, generator loss: 3.9039062939631286, discriminator loss: 0.307873866878757\n",
            "Epoch 5/300, Batch 800/937, generator loss: 3.919353569234065, discriminator loss: 0.30616039257310534\n",
            "Epoch 5/300, Batch 900/937, generator loss: 3.92958286590274, discriminator loss: 0.30461489122909136\n",
            "Saving results...\n",
            "Epoch 6/300, Batch 0/937, generator loss: 3.9358144844742236, discriminator loss: 0.30467392981420754\n",
            "Epoch 6/300, Batch 100/937, generator loss: 3.9487743899161094, discriminator loss: 0.3044464634153709\n",
            "Epoch 6/300, Batch 200/937, generator loss: 3.9608671127900252, discriminator loss: 0.3027295840207744\n",
            "Epoch 6/300, Batch 300/937, generator loss: 3.974931808927263, discriminator loss: 0.3013169321443973\n",
            "Epoch 6/300, Batch 400/937, generator loss: 3.9885039131506406, discriminator loss: 0.2997681468678198\n",
            "Epoch 6/300, Batch 500/937, generator loss: 3.999088702814341, discriminator loss: 0.29938908078953097\n",
            "Epoch 6/300, Batch 600/937, generator loss: 4.005503305826811, discriminator loss: 0.29858616286167067\n",
            "Epoch 6/300, Batch 700/937, generator loss: 4.0127032295484515, discriminator loss: 0.2982281306536113\n",
            "Epoch 6/300, Batch 800/937, generator loss: 4.008771828238734, discriminator loss: 0.29996253131704226\n",
            "Epoch 6/300, Batch 900/937, generator loss: 4.0111639645409225, discriminator loss: 0.2997154209088342\n",
            "Saving results...\n",
            "Epoch 7/300, Batch 0/937, generator loss: 4.011996989488238, discriminator loss: 0.29955817502298643\n",
            "Epoch 7/300, Batch 100/937, generator loss: 4.014707116957183, discriminator loss: 0.2989849359764754\n",
            "Epoch 7/300, Batch 200/937, generator loss: 4.01553888538709, discriminator loss: 0.297924712829887\n",
            "Epoch 7/300, Batch 300/937, generator loss: 4.016325744725872, discriminator loss: 0.2969477978201024\n",
            "Epoch 7/300, Batch 400/937, generator loss: 4.016579466535784, discriminator loss: 0.2960795427897367\n",
            "Epoch 7/300, Batch 500/937, generator loss: 4.015184435147064, discriminator loss: 0.29529023086414713\n",
            "Epoch 7/300, Batch 600/937, generator loss: 4.014511344144797, discriminator loss: 0.29410514250324843\n",
            "Epoch 7/300, Batch 700/937, generator loss: 4.020669249061382, discriminator loss: 0.29255728878788956\n",
            "Epoch 7/300, Batch 800/937, generator loss: 4.018383276454457, discriminator loss: 0.292403808532222\n",
            "Epoch 7/300, Batch 900/937, generator loss: 4.021187797652173, discriminator loss: 0.29110198576482704\n",
            "Saving results...\n",
            "Epoch 8/300, Batch 0/937, generator loss: 4.024086134289366, discriminator loss: 0.2908084122703817\n",
            "Epoch 8/300, Batch 100/937, generator loss: 4.039860877007485, discriminator loss: 0.2903935860475184\n",
            "Epoch 8/300, Batch 200/937, generator loss: 4.048921050840467, discriminator loss: 0.2915141726220267\n",
            "Epoch 8/300, Batch 300/937, generator loss: 4.060617186151071, discriminator loss: 0.2909449429362458\n",
            "Epoch 8/300, Batch 400/937, generator loss: 4.0739902259806255, discriminator loss: 0.2902572259870159\n",
            "Epoch 8/300, Batch 500/937, generator loss: 4.084800557425667, discriminator loss: 0.2897959551052502\n",
            "Epoch 8/300, Batch 600/937, generator loss: 4.088340016453446, discriminator loss: 0.2889886866098784\n",
            "Epoch 8/300, Batch 700/937, generator loss: 4.0938609003788, discriminator loss: 0.289245290402448\n",
            "Epoch 8/300, Batch 800/937, generator loss: 4.103966140964023, discriminator loss: 0.28994264035143663\n",
            "Epoch 8/300, Batch 900/937, generator loss: 4.112455808386656, discriminator loss: 0.2911520300516667\n",
            "Saving results...\n",
            "Epoch 9/300, Batch 0/937, generator loss: 4.1155575571154595, discriminator loss: 0.2909711187081277\n",
            "Epoch 9/300, Batch 100/937, generator loss: 4.116848760915914, discriminator loss: 0.29197604879466255\n",
            "Epoch 9/300, Batch 200/937, generator loss: 4.120483331604912, discriminator loss: 0.29203762276754397\n",
            "Epoch 9/300, Batch 300/937, generator loss: 4.118741471290534, discriminator loss: 0.2930085704081189\n",
            "Epoch 9/300, Batch 400/937, generator loss: 4.117730684205637, discriminator loss: 0.2928889588703154\n",
            "Epoch 9/300, Batch 500/937, generator loss: 4.115938422815736, discriminator loss: 0.2934735264822604\n",
            "Epoch 9/300, Batch 600/937, generator loss: 4.118091868983451, discriminator loss: 0.29376982092628084\n",
            "Epoch 9/300, Batch 700/937, generator loss: 4.117831488152367, discriminator loss: 0.293422963714522\n",
            "Epoch 9/300, Batch 800/937, generator loss: 4.117111607724412, discriminator loss: 0.2936269421088462\n",
            "Epoch 9/300, Batch 900/937, generator loss: 4.121332255751054, discriminator loss: 0.2936890186290555\n",
            "Saving results...\n",
            "Epoch 10/300, Batch 0/937, generator loss: 4.125906359912771, discriminator loss: 0.2938110169940386\n",
            "Epoch 10/300, Batch 100/937, generator loss: 4.124798026026749, discriminator loss: 0.2946898058385661\n",
            "Epoch 10/300, Batch 200/937, generator loss: 4.124942999170836, discriminator loss: 0.29513988238232425\n",
            "Epoch 10/300, Batch 300/937, generator loss: 4.130045329725689, discriminator loss: 0.2952731405196571\n",
            "Epoch 10/300, Batch 400/937, generator loss: 4.1330647802062535, discriminator loss: 0.2953805552221543\n",
            "Epoch 10/300, Batch 500/937, generator loss: 4.132439351996056, discriminator loss: 0.2957952201393597\n",
            "Epoch 10/300, Batch 600/937, generator loss: 4.132792884129936, discriminator loss: 0.29603787425581546\n",
            "Epoch 10/300, Batch 700/937, generator loss: 4.1323640593858855, discriminator loss: 0.2961010559187451\n",
            "Epoch 10/300, Batch 800/937, generator loss: 4.132085427570081, discriminator loss: 0.2961399065259752\n",
            "Epoch 10/300, Batch 900/937, generator loss: 4.131238073786653, discriminator loss: 0.2959736275204014\n",
            "Saving results...\n",
            "Epoch 11/300, Batch 0/937, generator loss: 4.131218460158271, discriminator loss: 0.2958454619789533\n",
            "Epoch 11/300, Batch 100/937, generator loss: 4.130450515123764, discriminator loss: 0.2959664941963659\n",
            "Epoch 11/300, Batch 200/937, generator loss: 4.127765559785984, discriminator loss: 0.2962299262382286\n",
            "Epoch 11/300, Batch 300/937, generator loss: 4.129089155117805, discriminator loss: 0.29615795640078224\n",
            "Epoch 11/300, Batch 400/937, generator loss: 4.1320682489241864, discriminator loss: 0.2960603161477357\n",
            "Epoch 11/300, Batch 500/937, generator loss: 4.131582601540491, discriminator loss: 0.29642517229391857\n",
            "Epoch 11/300, Batch 600/937, generator loss: 4.13207357692924, discriminator loss: 0.296158630381088\n",
            "Epoch 11/300, Batch 700/937, generator loss: 4.141337811335012, discriminator loss: 0.2958977450609889\n",
            "Epoch 11/300, Batch 800/937, generator loss: 4.140028571622124, discriminator loss: 0.2964039038756757\n",
            "Epoch 11/300, Batch 900/937, generator loss: 4.138646316982482, discriminator loss: 0.2960293835712375\n",
            "Saving results...\n",
            "Epoch 12/300, Batch 0/937, generator loss: 4.137939885156639, discriminator loss: 0.29621628157599017\n",
            "Epoch 12/300, Batch 100/937, generator loss: 4.137998788621574, discriminator loss: 0.29596936326751416\n",
            "Epoch 12/300, Batch 200/937, generator loss: 4.136835265414795, discriminator loss: 0.2959224925057083\n",
            "Epoch 12/300, Batch 300/937, generator loss: 4.135870006586267, discriminator loss: 0.296106771010891\n",
            "Epoch 12/300, Batch 400/937, generator loss: 4.131809724063103, discriminator loss: 0.29667299222656973\n",
            "Epoch 12/300, Batch 500/937, generator loss: 4.1278749055637105, discriminator loss: 0.2967526739330787\n",
            "Epoch 12/300, Batch 600/937, generator loss: 4.127286873547122, discriminator loss: 0.2969115879564358\n",
            "Epoch 12/300, Batch 700/937, generator loss: 4.124946701836017, discriminator loss: 0.29679018350551195\n",
            "Epoch 12/300, Batch 800/937, generator loss: 4.123571607605359, discriminator loss: 0.29669877828627317\n",
            "Epoch 12/300, Batch 900/937, generator loss: 4.122719967257295, discriminator loss: 0.295945453077567\n",
            "Saving results...\n",
            "Epoch 13/300, Batch 0/937, generator loss: 4.122433771343972, discriminator loss: 0.2958383790028201\n",
            "Epoch 13/300, Batch 100/937, generator loss: 4.120385937549221, discriminator loss: 0.29592402471374113\n",
            "Epoch 13/300, Batch 200/937, generator loss: 4.120719942955493, discriminator loss: 0.2957502037023202\n",
            "Epoch 13/300, Batch 300/937, generator loss: 4.122423802439629, discriminator loss: 0.29580276966825586\n",
            "Epoch 13/300, Batch 400/937, generator loss: 4.124542684698878, discriminator loss: 0.2954543442039749\n",
            "Epoch 13/300, Batch 500/937, generator loss: 4.125820355443732, discriminator loss: 0.2954499056767522\n",
            "Epoch 13/300, Batch 600/937, generator loss: 4.125742939129027, discriminator loss: 0.2954300184877121\n",
            "Epoch 13/300, Batch 700/937, generator loss: 4.128290266020663, discriminator loss: 0.2954031894765152\n",
            "Epoch 13/300, Batch 800/937, generator loss: 4.13013852275753, discriminator loss: 0.295158810048671\n",
            "Epoch 13/300, Batch 900/937, generator loss: 4.132370973062559, discriminator loss: 0.2947025201699863\n",
            "Saving results...\n",
            "Epoch 14/300, Batch 0/937, generator loss: 4.131811078428559, discriminator loss: 0.2947403914935115\n",
            "Epoch 14/300, Batch 100/937, generator loss: 4.1296214834221985, discriminator loss: 0.2948105872862142\n",
            "Epoch 14/300, Batch 200/937, generator loss: 4.1274459986717495, discriminator loss: 0.2949000001317281\n",
            "Epoch 14/300, Batch 300/937, generator loss: 4.125740521030886, discriminator loss: 0.2951563397060564\n",
            "Epoch 14/300, Batch 400/937, generator loss: 4.12353605239188, discriminator loss: 0.2956418119289923\n",
            "Epoch 14/300, Batch 500/937, generator loss: 4.119441177532688, discriminator loss: 0.29597925557363436\n",
            "Epoch 14/300, Batch 600/937, generator loss: 4.115066913584681, discriminator loss: 0.2962948289386289\n",
            "Epoch 14/300, Batch 700/937, generator loss: 4.112139293910091, discriminator loss: 0.29632758824175626\n",
            "Epoch 14/300, Batch 800/937, generator loss: 4.110762200754464, discriminator loss: 0.29632559520649376\n",
            "Epoch 14/300, Batch 900/937, generator loss: 4.109335943124325, discriminator loss: 0.29623416782794526\n",
            "Saving results...\n",
            "Epoch 15/300, Batch 0/937, generator loss: 4.107758813174564, discriminator loss: 0.29646520767685325\n",
            "Epoch 15/300, Batch 100/937, generator loss: 4.1048906638052, discriminator loss: 0.2963818403249846\n",
            "Epoch 15/300, Batch 200/937, generator loss: 4.101030531977998, discriminator loss: 0.2965744930903786\n",
            "Epoch 15/300, Batch 300/937, generator loss: 4.097203583251836, discriminator loss: 0.29686916047303946\n",
            "Epoch 15/300, Batch 400/937, generator loss: 4.09440238996062, discriminator loss: 0.29726468530181005\n",
            "Epoch 15/300, Batch 500/937, generator loss: 4.089940374931584, discriminator loss: 0.297825941567722\n",
            "Epoch 15/300, Batch 600/937, generator loss: 4.086286176227879, discriminator loss: 0.29767285392204845\n",
            "Epoch 15/300, Batch 700/937, generator loss: 4.082491550888431, discriminator loss: 0.2983023838101444\n",
            "Epoch 15/300, Batch 800/937, generator loss: 4.080311664187048, discriminator loss: 0.29850791436437\n",
            "Epoch 15/300, Batch 900/937, generator loss: 4.077376128283214, discriminator loss: 0.2990894969109387\n",
            "Saving results...\n",
            "Epoch 16/300, Batch 0/937, generator loss: 4.075960381567888, discriminator loss: 0.29916785657704015\n",
            "Epoch 16/300, Batch 100/937, generator loss: 4.071202874669401, discriminator loss: 0.29983155516540916\n",
            "Epoch 16/300, Batch 200/937, generator loss: 4.068997347868039, discriminator loss: 0.30029015009428967\n",
            "Epoch 16/300, Batch 300/937, generator loss: 4.067044040609247, discriminator loss: 0.30069358849442823\n",
            "Epoch 16/300, Batch 400/937, generator loss: 4.065620837667443, discriminator loss: 0.30101638373607303\n",
            "Epoch 16/300, Batch 500/937, generator loss: 4.066058690253402, discriminator loss: 0.3011554553003544\n",
            "Epoch 16/300, Batch 600/937, generator loss: 4.064028518532328, discriminator loss: 0.30152052934102647\n",
            "Epoch 16/300, Batch 700/937, generator loss: 4.061952744802356, discriminator loss: 0.30190561359698037\n",
            "Epoch 16/300, Batch 800/937, generator loss: 4.061195262474187, discriminator loss: 0.3024237755697131\n",
            "Epoch 16/300, Batch 900/937, generator loss: 4.059895244263825, discriminator loss: 0.3027891750896149\n",
            "Saving results...\n",
            "Epoch 17/300, Batch 0/937, generator loss: 4.058892011182232, discriminator loss: 0.30286993154249175\n",
            "Epoch 17/300, Batch 100/937, generator loss: 4.055567248839552, discriminator loss: 0.303327607273758\n",
            "Epoch 17/300, Batch 200/937, generator loss: 4.05261240995455, discriminator loss: 0.30378132943717573\n",
            "Epoch 17/300, Batch 300/937, generator loss: 4.048217103533442, discriminator loss: 0.30463196491100725\n",
            "Epoch 17/300, Batch 400/937, generator loss: 4.041941584160338, discriminator loss: 0.3054988540321205\n",
            "Epoch 17/300, Batch 500/937, generator loss: 4.036783635453052, discriminator loss: 0.30642384918394133\n",
            "Epoch 17/300, Batch 600/937, generator loss: 4.031727344259232, discriminator loss: 0.307049843179444\n",
            "Epoch 17/300, Batch 700/937, generator loss: 4.0264967906819145, discriminator loss: 0.30785542526789195\n",
            "Epoch 17/300, Batch 800/937, generator loss: 4.021271605781645, discriminator loss: 0.30895238201274744\n",
            "Epoch 17/300, Batch 900/937, generator loss: 4.015277001511392, discriminator loss: 0.3097135859997019\n",
            "Saving results...\n",
            "Epoch 18/300, Batch 0/937, generator loss: 4.012930641415604, discriminator loss: 0.31014212169799593\n",
            "Epoch 18/300, Batch 100/937, generator loss: 4.006894491410476, discriminator loss: 0.31102798610140525\n",
            "Epoch 18/300, Batch 200/937, generator loss: 4.001003353176088, discriminator loss: 0.311937447018948\n",
            "Epoch 18/300, Batch 300/937, generator loss: 3.9965175520688367, discriminator loss: 0.31265821185030473\n",
            "Epoch 18/300, Batch 400/937, generator loss: 3.991810881039371, discriminator loss: 0.3133151927976815\n",
            "Epoch 18/300, Batch 500/937, generator loss: 3.9879433409313934, discriminator loss: 0.3140928633041303\n",
            "Epoch 18/300, Batch 600/937, generator loss: 3.9803888982132767, discriminator loss: 0.3149306008333864\n",
            "Epoch 18/300, Batch 700/937, generator loss: 3.9742466269712318, discriminator loss: 0.31590303892059046\n",
            "Epoch 18/300, Batch 800/937, generator loss: 3.9684474044315223, discriminator loss: 0.31673095595902767\n",
            "Epoch 18/300, Batch 900/937, generator loss: 3.9634088993495213, discriminator loss: 0.3175975967840425\n",
            "Saving results...\n",
            "Epoch 19/300, Batch 0/937, generator loss: 3.9615782997235103, discriminator loss: 0.3179575876154757\n",
            "Epoch 19/300, Batch 100/937, generator loss: 3.9553469425971164, discriminator loss: 0.3188560434614605\n",
            "Epoch 19/300, Batch 200/937, generator loss: 3.9485743141451084, discriminator loss: 0.3199354414558098\n",
            "Epoch 19/300, Batch 300/937, generator loss: 3.941543120559975, discriminator loss: 0.32101640726842556\n",
            "Epoch 19/300, Batch 400/937, generator loss: 3.934693856671914, discriminator loss: 0.3220529660166711\n",
            "Epoch 19/300, Batch 500/937, generator loss: 3.9283458487117677, discriminator loss: 0.3229479007447347\n",
            "Epoch 19/300, Batch 600/937, generator loss: 3.920970171926411, discriminator loss: 0.32398333774474336\n",
            "Epoch 19/300, Batch 700/937, generator loss: 3.914098430555386, discriminator loss: 0.3251795987648929\n",
            "Epoch 19/300, Batch 800/937, generator loss: 3.906118696923539, discriminator loss: 0.32607372935402396\n",
            "Epoch 19/300, Batch 900/937, generator loss: 3.8976519364063766, discriminator loss: 0.3275777164900833\n",
            "Saving results...\n",
            "Epoch 20/300, Batch 0/937, generator loss: 3.8949516933689847, discriminator loss: 0.32791048731698813\n",
            "Epoch 20/300, Batch 100/937, generator loss: 3.887864222291055, discriminator loss: 0.3288911023762416\n",
            "Epoch 20/300, Batch 200/937, generator loss: 3.879372054244364, discriminator loss: 0.3302994066851613\n",
            "Epoch 20/300, Batch 300/937, generator loss: 3.8719109042584474, discriminator loss: 0.3314387964169948\n",
            "Epoch 20/300, Batch 400/937, generator loss: 3.8637893828850705, discriminator loss: 0.3328374716706314\n",
            "Epoch 20/300, Batch 500/937, generator loss: 3.857366497570162, discriminator loss: 0.3337369293530526\n",
            "Epoch 20/300, Batch 600/937, generator loss: 3.851101780827145, discriminator loss: 0.33508389427788743\n",
            "Epoch 20/300, Batch 700/937, generator loss: 3.8454110734649514, discriminator loss: 0.3359016925993295\n",
            "Epoch 20/300, Batch 800/937, generator loss: 3.8387780912023355, discriminator loss: 0.3369447795253749\n",
            "Epoch 20/300, Batch 900/937, generator loss: 3.832443245434663, discriminator loss: 0.3381586571792425\n",
            "Saving results...\n",
            "Epoch 21/300, Batch 0/937, generator loss: 3.8301059278406884, discriminator loss: 0.33836908906089397\n",
            "Epoch 21/300, Batch 100/937, generator loss: 3.824048879206488, discriminator loss: 0.3392070918967888\n",
            "Epoch 21/300, Batch 200/937, generator loss: 3.817620432455423, discriminator loss: 0.34035404097423405\n",
            "Epoch 21/300, Batch 300/937, generator loss: 3.8115683086182885, discriminator loss: 0.341407210819236\n",
            "Epoch 21/300, Batch 400/937, generator loss: 3.805525843363167, discriminator loss: 0.34252938445030734\n",
            "Epoch 21/300, Batch 500/937, generator loss: 3.7997643772934717, discriminator loss: 0.34370255604827943\n",
            "Epoch 21/300, Batch 600/937, generator loss: 3.7928908010719677, discriminator loss: 0.344786391486688\n",
            "Epoch 21/300, Batch 700/937, generator loss: 3.7859029897220564, discriminator loss: 0.3460563426309554\n",
            "Epoch 21/300, Batch 800/937, generator loss: 3.779634210490915, discriminator loss: 0.34714959536210926\n",
            "Epoch 21/300, Batch 900/937, generator loss: 3.7724180699686625, discriminator loss: 0.34831955275263077\n",
            "Saving results...\n",
            "Epoch 22/300, Batch 0/937, generator loss: 3.770183649824685, discriminator loss: 0.34863844846539954\n",
            "Epoch 22/300, Batch 100/937, generator loss: 3.7629086089341386, discriminator loss: 0.3500434744596884\n",
            "Epoch 22/300, Batch 200/937, generator loss: 3.755047284480101, discriminator loss: 0.3513218935295392\n",
            "Epoch 22/300, Batch 300/937, generator loss: 3.7482651141708385, discriminator loss: 0.3526194929791614\n",
            "Epoch 22/300, Batch 400/937, generator loss: 3.7418726057799803, discriminator loss: 0.3539395881981161\n",
            "Epoch 22/300, Batch 500/937, generator loss: 3.735204682854321, discriminator loss: 0.354953373613274\n",
            "Epoch 22/300, Batch 600/937, generator loss: 3.72840920008496, discriminator loss: 0.3561667913076256\n",
            "Epoch 22/300, Batch 700/937, generator loss: 3.7223315137248627, discriminator loss: 0.3573485982717272\n",
            "Epoch 22/300, Batch 800/937, generator loss: 3.716289295914739, discriminator loss: 0.3583815029840338\n",
            "Epoch 22/300, Batch 900/937, generator loss: 3.709599479406123, discriminator loss: 0.35941217538785747\n",
            "Saving results...\n",
            "Epoch 23/300, Batch 0/937, generator loss: 3.707871994829191, discriminator loss: 0.35980429518041385\n",
            "Epoch 23/300, Batch 100/937, generator loss: 3.701573004313302, discriminator loss: 0.3609629308210599\n",
            "Epoch 23/300, Batch 200/937, generator loss: 3.6948199746948736, discriminator loss: 0.3621772197268714\n",
            "Epoch 23/300, Batch 300/937, generator loss: 3.688960158252659, discriminator loss: 0.3632392880641321\n",
            "Epoch 23/300, Batch 400/937, generator loss: 3.683431733011352, discriminator loss: 0.36412416320326724\n",
            "Epoch 23/300, Batch 500/937, generator loss: 3.677048609406702, discriminator loss: 0.36525200444542083\n",
            "Epoch 23/300, Batch 600/937, generator loss: 3.671470913531527, discriminator loss: 0.3663677127205129\n",
            "Epoch 23/300, Batch 700/937, generator loss: 3.6648131892977056, discriminator loss: 0.3673873926607821\n",
            "Epoch 23/300, Batch 800/937, generator loss: 3.6598303214531542, discriminator loss: 0.36828122320921614\n",
            "Epoch 23/300, Batch 900/937, generator loss: 3.654229703286466, discriminator loss: 0.36935066934173894\n",
            "Saving results...\n",
            "Epoch 24/300, Batch 0/937, generator loss: 3.6519877344145932, discriminator loss: 0.3697725943089634\n",
            "Epoch 24/300, Batch 100/937, generator loss: 3.646365952571448, discriminator loss: 0.37064097263570367\n",
            "Epoch 24/300, Batch 200/937, generator loss: 3.6409459208411294, discriminator loss: 0.3716147612246553\n",
            "Epoch 24/300, Batch 300/937, generator loss: 3.6354603263204956, discriminator loss: 0.3725550064665944\n",
            "Epoch 24/300, Batch 400/937, generator loss: 3.629427076820892, discriminator loss: 0.37390180918605714\n",
            "Epoch 24/300, Batch 500/937, generator loss: 3.6236909832871023, discriminator loss: 0.37485124867386294\n",
            "Epoch 24/300, Batch 600/937, generator loss: 3.6176711719230954, discriminator loss: 0.3759139110637344\n",
            "Epoch 24/300, Batch 700/937, generator loss: 3.6114765937390336, discriminator loss: 0.37713054272334673\n",
            "Epoch 24/300, Batch 800/937, generator loss: 3.6063457855681156, discriminator loss: 0.37797812178543494\n",
            "Epoch 24/300, Batch 900/937, generator loss: 3.600368864904071, discriminator loss: 0.3791444185334246\n",
            "Saving results...\n",
            "Epoch 25/300, Batch 0/937, generator loss: 3.597960198827805, discriminator loss: 0.37961254781895265\n",
            "Epoch 25/300, Batch 100/937, generator loss: 3.5917294101960135, discriminator loss: 0.3806739963713343\n",
            "Epoch 25/300, Batch 200/937, generator loss: 3.5857451305551473, discriminator loss: 0.38169827719354554\n",
            "Epoch 25/300, Batch 300/937, generator loss: 3.5791001166715937, discriminator loss: 0.38277328196885685\n",
            "Epoch 25/300, Batch 400/937, generator loss: 3.5730823393778848, discriminator loss: 0.3839502005363245\n",
            "Epoch 25/300, Batch 500/937, generator loss: 3.5665482757138953, discriminator loss: 0.38515203632554473\n",
            "Epoch 25/300, Batch 600/937, generator loss: 3.560024352732283, discriminator loss: 0.38643054691672296\n",
            "Epoch 25/300, Batch 700/937, generator loss: 3.5533673947356306, discriminator loss: 0.3877194501205749\n",
            "Epoch 25/300, Batch 800/937, generator loss: 3.546489704561147, discriminator loss: 0.3890190598069219\n",
            "Epoch 25/300, Batch 900/937, generator loss: 3.53983001614944, discriminator loss: 0.39029790546576915\n",
            "Saving results...\n",
            "Epoch 26/300, Batch 0/937, generator loss: 3.5374282260018997, discriminator loss: 0.3906860625597564\n",
            "Epoch 26/300, Batch 100/937, generator loss: 3.530898658731643, discriminator loss: 0.3917590980610093\n",
            "Epoch 26/300, Batch 200/937, generator loss: 3.525705586549674, discriminator loss: 0.39292435539674864\n",
            "Epoch 26/300, Batch 300/937, generator loss: 3.519493696253428, discriminator loss: 0.39403375604773017\n",
            "Epoch 26/300, Batch 400/937, generator loss: 3.5138051140117263, discriminator loss: 0.3951373862001308\n",
            "Epoch 26/300, Batch 500/937, generator loss: 3.5076149880361065, discriminator loss: 0.39629178061707954\n",
            "Epoch 26/300, Batch 600/937, generator loss: 3.502449758619766, discriminator loss: 0.3973388525471972\n",
            "Epoch 26/300, Batch 700/937, generator loss: 3.4978656126330057, discriminator loss: 0.39838314852496504\n",
            "Epoch 26/300, Batch 800/937, generator loss: 3.4934103932990106, discriminator loss: 0.39913259123465367\n",
            "Epoch 26/300, Batch 900/937, generator loss: 3.4879326390756713, discriminator loss: 0.40018017867094396\n",
            "Saving results...\n",
            "Epoch 27/300, Batch 0/937, generator loss: 3.486330377742236, discriminator loss: 0.40041664296105917\n",
            "Epoch 27/300, Batch 100/937, generator loss: 3.4807639400672725, discriminator loss: 0.40144395487397677\n",
            "Epoch 27/300, Batch 200/937, generator loss: 3.4755996705340406, discriminator loss: 0.4024883278935563\n",
            "Epoch 27/300, Batch 300/937, generator loss: 3.470304184036795, discriminator loss: 0.403549547692528\n",
            "Epoch 27/300, Batch 400/937, generator loss: 3.4647432045969055, discriminator loss: 0.40453044378455977\n",
            "Epoch 27/300, Batch 500/937, generator loss: 3.4592398666480717, discriminator loss: 0.4055670836429263\n",
            "Epoch 27/300, Batch 600/937, generator loss: 3.453742050068259, discriminator loss: 0.406664407591562\n",
            "Epoch 27/300, Batch 700/937, generator loss: 3.4482641918361185, discriminator loss: 0.4075260644692641\n",
            "Epoch 27/300, Batch 800/937, generator loss: 3.442931420490203, discriminator loss: 0.40854614036750064\n",
            "Epoch 27/300, Batch 900/937, generator loss: 3.437199950857472, discriminator loss: 0.4095267914201467\n",
            "Saving results...\n",
            "Epoch 28/300, Batch 0/937, generator loss: 3.435375849764108, discriminator loss: 0.4099153364893276\n",
            "Epoch 28/300, Batch 100/937, generator loss: 3.430404949906557, discriminator loss: 0.41075572702129415\n",
            "Epoch 28/300, Batch 200/937, generator loss: 3.425735012611616, discriminator loss: 0.4114585054875791\n",
            "Epoch 28/300, Batch 300/937, generator loss: 3.4206908256097392, discriminator loss: 0.4124600154498592\n",
            "Epoch 28/300, Batch 400/937, generator loss: 3.415555130539434, discriminator loss: 0.41337663642918065\n",
            "Epoch 28/300, Batch 500/937, generator loss: 3.4101717210468716, discriminator loss: 0.4143978475969259\n",
            "Epoch 28/300, Batch 600/937, generator loss: 3.404885425656733, discriminator loss: 0.41548346580640655\n",
            "Epoch 28/300, Batch 700/937, generator loss: 3.400353325948058, discriminator loss: 0.41635400821673435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4PnhrJuf-x6j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}